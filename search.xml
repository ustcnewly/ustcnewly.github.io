<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Generative Composition</title>
    <url>/2025/01/06/deep_learning/Generative%20Composition/</url>
    <content><![CDATA[<h3 id="Object-Text-Guided"><a href="#Object-Text-Guided" class="headerlink" title="(Object+Text)-Guided"></a>(Object+Text)-Guided</h3><h4 id="Training-free"><a href="#Training-free" class="headerlink" title="Training-free"></a>Training-free</h4><ul>
<li>Pengzhi Li, Qiang Nie, Ying Chen, Xi Jiang, Kai Wu, Yuhuan Lin, Yong Liu, Jinlong Peng, Chengjie Wang, Feng Zheng: “<em>Tuning-Free Image Customization with Image and Text Guidance.</em>“  arXiv preprint arXiv:2403.12658 (2024) <a href="https://arxiv.org/pdf/2403.12658" target="_blank" rel="noopener">[arXiv]</a><h4 id="Training-based"><a href="#Training-based" class="headerlink" title="Training-based"></a>Training-based</h4></li>
<li>Yicheng Yang, Pengxiang Li, Lu Zhang, Liqian Ma, Ping Hu, Siyu Du, Yunzhi Zhuge, Xu Jia, Huchuan Lu: “<em>DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting.</em>“ arXiv preprint arXiv:2411.17223 (2024) <a href="https://arxiv.org/pdf/2411.17223" target="_blank" rel="noopener">[arXiv]</a> <a href="https://github.com/mycfhs/dreammix" target="_blank" rel="noopener">[code]</a></li>
<li>Shaoan Xie, Yang Zhao, Zhisheng Xiao, Kelvin C.K. Chan, Yandong Li, Yanwu Xu, Kun Zhang, Tingbo Hou: “<em>DreamInpainter: Text-Guided Subject-Driven Image Inpainting with Diffusion Models.</em>“ arXiv preprint arXiv:2312.03771 (2023) <a href="https://arxiv.org/pdf/2312.03771" target="_blank" rel="noopener">[arXiv]</a></li>
<li>Yulin Pan, Chaojie Mao, Zeyinzi Jiang, Zhen Han, Jingfeng Zhang: “<em>Locate, Assign, Refine: Taming Customized Image Inpainting with Text-Subject Guidance.</em>“ arXiv preprint arXiv:2403.19534 (2024) <a href="https://arxiv.org/pdf/2403.19534" target="_blank" rel="noopener">[arXiv]</a> <a href="https://github.com/modelscope/scepter" target="_blank" rel="noopener">[code]</a></li>
</ul>
<h4 id="Foreground-3D-Background-image"><a href="#Foreground-3D-Background-image" class="headerlink" title="Foreground: 3D;  Background: image"></a>Foreground: 3D;  Background: image</h4><ul>
<li>Jinghao Zhou, Tomas Jakab, Philip Torr, Christian Rupprecht: “<em>Scene-Conditional 3D Object Stylization and Composition.</em>“ arXiv preprint arXiv:2312.12419 (2023) <a href="https://arxiv.org/pdf/2312.12419.pdf" target="_blank" rel="noopener">[arXiv]</a> <a href="https://github.com/shallowtoil/scene-cond-3d" target="_blank" rel="noopener">[code]</a></li>
</ul>
<h4 id="Foreground-3D-Background-3D"><a href="#Foreground-3D-Background-3D" class="headerlink" title="Foreground: 3D;  Background: 3D"></a>Foreground: 3D;  Background: 3D</h4><ul>
<li>Mohamad Shahbazi, Liesbeth Claessens, Michael Niemeyer, Edo Collins, Alessio Tonioni, Luc Van Gool, Federico Tombari: “<em>InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes.</em>“ arXiv preprint arXiv:2401.05335 (2024) <a href="https://arxiv.org/pdf/2401.05335.pdf" target="_blank" rel="noopener">[arXiv]</a></li>
<li>Rahul Goel, Dhawal Sirikonda, Saurabh Saini, PJ Narayanan: “<em>Interactive Segmentation of Radiance Fields.</em>“ CVPR (2023) <a href="https://arxiv.org/abs/2212.13545" target="_blank" rel="noopener">[arXiv]</a> <a href="https://github.com/rahul-goel/isrf_code" target="_blank" rel="noopener">[code]</a></li>
<li>Rahul Goel, Dhawal Sirikonda, Rajvi Shah, PJ Narayanan: “<em>FusedRF: Fusing Multiple Radiance Fields.</em>“ CVPR Workshop (2023) <a href="https://arxiv.org/abs/2306.04180" target="_blank" rel="noopener">[arXiv]</a></li>
<li>Verica Lazova, Vladimir Guzov, Kyle Olszewski, Sergey Tulyakov, Gerard Pons-Moll: “<em>Control-NeRF: Editable Feature Volumes for Scene Rendering and Manipulation.</em>“ WACV (2023) <a href="https://arxiv.org/abs/2204.10850" target="_blank" rel="noopener">[arXiv]</a></li>
<li>Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, Gang Zeng: “<em>Compressible-composable NeRF via Rank-residual Decomposition.</em>“ NIPS (2022) <a href="https://arxiv.org/abs/2205.14870" target="_blank" rel="noopener">[arXiv]</a> <a href="https://github.com/ashawkey/CCNeRF" target="_blank" rel="noopener">[code]</a></li>
<li>Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, Zhaopeng Cui: “<em>Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering.</em>“ ICCV (2021) <a href="https://arxiv.org/abs/2109.01847" target="_blank" rel="noopener">[arXiv]</a> <a href="https://github.com/zju3dv/object_nerf" target="_blank" rel="noopener">[code]</a></li>
</ul>
<h4 id="Foreground-video-Background-image"><a href="#Foreground-video-Background-image" class="headerlink" title="Foreground: video;  Background: image"></a>Foreground: video;  Background: image</h4><ul>
<li>Boxiao Pan, Zhan Xu, Chun-Hao Paul Huang, Krishna Kumar Singh, Yang Zhou, Leonidas J. Guibas, Jimei Yang: “<em>ActAnywhere: Subject-Aware Video Background Generation.</em>“ arXiv preprint arXiv:2401.10822 (2024) <a href="https://arxiv.org/pdf/2401.10822.pdf" target="_blank" rel="noopener">[arXiv]</a> </li>
</ul>
<h4 id="Foreground-video-Background-video"><a href="#Foreground-video-Background-video" class="headerlink" title="Foreground: video;  Background: video"></a>Foreground: video;  Background: video</h4><ul>
<li><p>Jiaqi Guo, Sitong Su, Junchen Zhu, Lianli Gao, Jingkuan Song: “<em>Training-Free Semantic Video Composition via Pre-trained Diffusion Model.</em>“ arXiv preprint arXiv:2401.09195 (2024) <a href="https://arxiv.org/pdf/2401.09195v1.pdf" target="_blank" rel="noopener">[arXiv]</a></p>
</li>
<li><p>Donghoon Lee, Tomas Pfister, Ming-Hsuan Yang: “<em>Inserting Videos into Videos.</em>“ CVPR (2019) <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Lee_Inserting_Videos_Into_Videos_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[pdf]</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Human Generation</title>
    <url>/2024/05/20/deep_learning/Human%20Generation/</url>
    <content><![CDATA[<p>Combine different components: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fruhstuck_InsetGAN_for_Full-Body_Image_Generation_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/2404.15267" target="_blank" rel="noopener">[2]</a></p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ol>
<li><p>Frühstück, Anna, et al. “Insetgan for full-body image generation.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</p>
</li>
<li><p>Huang, Zehuan, et al. “From Parts to Whole: A Unified Reference Framework for Controllable Human Image Generation.” arXiv preprint arXiv:2404.15267 (2024).</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Mixture-of-Experts</title>
    <url>/2024/05/08/deep_learning/Mixture-of-Experts/</url>
    <content><![CDATA[<p>The first paper: <a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf" target="_blank" rel="noopener">[1]</a></p>
<p>SwitchTransformer: <a href="https://arxiv.org/pdf/1701.06538" target="_blank" rel="noopener">[2]</a></p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4>]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>Remote Debugging for Eclipse+PyDev</title>
    <url>/2024/03/12/software/IDE/Remote%20Debugging%20for%20Eclipse+PyDev%20-%20%E5%89%AF%E6%9C%AC/</url>
    <content><![CDATA[<h2 id="When-server-is-Linux-client-is-Windows-using-Python"><a href="#When-server-is-Linux-client-is-Windows-using-Python" class="headerlink" title="When server is Linux, client is Windows, using Python."></a>When server is Linux, client is Windows, using Python.</h2><h3 id="In-the-python-file"><a href="#In-the-python-file" class="headerlink" title="In the python file:"></a>In the python file:</h3><p>add the following code<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pydevd</span><br><span class="line">pydevd.settrace(<span class="string">'202.120.38.30'</span>, port=<span class="number">5678</span>)</span><br></pre></td></tr></table></figure></p>
<p>in which the IP address is the Windows IP and the port is the default Eclipse debugger port.</p>
<h3 id="Under-Linux"><a href="#Under-Linux" class="headerlink" title="Under Linux:"></a>Under Linux:</h3><ol>
<li><code>pip install pydevd</code></li>
<li>In the file <code>lib/python2.7/site-packages/pydevd_file_utils.py</code>, modify <code>PATHS_FROM_ECLIPSE_TO_PYTHON = [(r&#39;L:\test&#39;, r&#39;/home/niuli/test&#39;)]</code>, in which the former is Windows path and the latter is Linux path.</li>
</ol>
<h3 id="Under-Windows"><a href="#Under-Windows" class="headerlink" title="Under Windows:"></a>Under Windows:</h3><ol>
<li>Install Eclipse IDE and PyDev plugin for Eclipse.</li>
<li>Pydev-&gt;Start Debug Server</li>
<li>Open the Debug perspective and watch Debug Server.</li>
</ol>
<p>After the above preparation, run python code under Linux and the debugging process will jump to the debug server under Windows. For interactive debugging, open PyDev Debug Console.</p>
]]></content>
      <categories>
        <category>software</category>
        <category>IDE</category>
      </categories>
      <tags>
        <tag>Eclipse</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Mirror Resources</title>
    <url>/2024/02/04/others/Mirror%20Resources/</url>
    <content><![CDATA[<p>arXiv:  <a href="http://xxx.itp.ac.cn/......pdf" target="_blank" rel="noopener">http://xxx.itp.ac.cn/......pdf</a></p>
<p>hugging face: <a href="https://hf-mirror.com" target="_blank" rel="noopener">https://hf-mirror.com</a></p>
<pre><code>export HF_ENDPOINT=https://hf-mirror.com

from huggingface_hub import snapshot_download

snapshot_download(repo_id=&#39;Qwen/Qwen-7B&#39;,
                  repo_type=&#39;model&#39;,
                  local_dir=&#39;./model_dir&#39;,
                  resume_download=True)
</code></pre><p>modelscope: <a href="https://modelscope.cn/home" target="_blank" rel="noopener">https://modelscope.cn/home</a></p>
<pre><code>from modelscope.hub.snapshot_download import snapshot_download

model_dir = snapshot_download(&#39;qwen/Qwen-7B&#39;, 
                              cache_dir=&#39;./model_dir&#39;, 
                              revision=&#39;master&#39;)
</code></pre>]]></content>
  </entry>
  <entry>
    <title>Line Detection</title>
    <url>/2024/01/03/classification_detection_segmentation/Line%20Detection/</url>
    <content><![CDATA[<p>[1] <a href="https://arxiv.org/abs/2106.00186" target="_blank" rel="noopener">MLSD</a></p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>[1] Gu, Geonmo, et al. “Towards light-weight and real-time line segment detection.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 1. 2022.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>Line detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Model Acceleration</title>
    <url>/2023/11/27/image_video_synthesis/Diffusion%20Model%20Acceleration/</url>
    <content><![CDATA[<p>Simplify model architecture: <a href="https://arxiv.org/pdf/2306.00980.pdf" target="_blank" rel="noopener">[3]</a></p>
<p>Reduce sampling times: DDIM <a href="https://arxiv.org/pdf/2010.02502.pdf" target="_blank" rel="noopener">[1]</a>, PLMS <a href="https://arxiv.org/pdf/2202.09778.pdf" target="_blank" rel="noopener">[2]</a></p>
<p>Step distillation: <a href="https://arxiv.org/pdf/2306.00980.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://arxiv.org/pdf/2202.00512.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://arxiv.org/pdf/2101.02388.pdf" target="_blank" rel="noopener">[5]</a> <a href="https://arxiv.org/pdf/2210.03142.pdf" target="_blank" rel="noopener">[6]</a></p>
<p>Adversarial distillation: <a href="https://static1.squarespace.com/static/6213c340453c3f502425776e/t/65663480a92fba51d0e1023f/1701197769659/adversarial_diffusion_distillation.pdf" target="_blank" rel="noopener">[7]</a></p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Song, Jiaming, Chenlin Meng, and Stefano Ermon. “Denoising diffusion implicit models.” arXiv preprint arXiv:2010.02502 (2020).</p>
<p>[2] Liu, Luping, et al. “Pseudo numerical methods for diffusion models on manifolds.” ICLR, (2022).</p>
<p>[3] Li, Yanyu, et al. “SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds.” NeurIPS(2023).</p>
<p>[4] Salimans, Tim, and Jonathan Ho. “Progressive distillation for fast sampling of diffusion models.” ICLR, 2022. </p>
<p>[5] Luhman, Eric, and Troy Luhman. “Knowledge distillation in iterative generative models for improved sampling speed.” arXiv preprint arXiv:2101.02388 (2021).</p>
<p>[6] Meng, Chenlin, et al. “On distillation of guided diffusion models.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.</p>
<p>[7] <a href="https://stability.ai/research/adversarial-diffusion-distillation" target="_blank" rel="noopener">https://stability.ai/research/adversarial-diffusion-distillation</a></p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>generative model</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Model for Sketches</title>
    <url>/2023/10/23/image_video_synthesis/Diffusion%20Model%20for%20Sketches/</url>
    <content><![CDATA[<h2 id="text-to-sketch"><a href="#text-to-sketch" class="headerlink" title="text-to-sketch"></a>text-to-sketch</h2><ul>
<li><a href="https://arxiv.org/pdf/2306.14685.pdf" target="_blank" rel="noopener">[1]</a>[SVG]: a) use text-to-image model to generate an image. Align image and sketch through CLIP. b) Align text and sketch through diffusion model.  </li>
<li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/21f76686538a5f06dc431efea5f475f5-Paper-Conference.pdf" target="_blank" rel="noopener">[2]</a>[SVG]: a) Align text and sketch through CLIP</li>
<li><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_VectorFusion_Text-to-SVG_by_Abstracting_Pixel-Based_Diffusion_Models_CVPR_2023_paper.pdf" target="_blank" rel="noopener">[3]</a>[SVG]: a) Align text and sketch through diffusion model</li>
</ul>
<h2 id="image-to-sketch"><a href="#image-to-sketch" class="headerlink" title="image-to-sketch"></a>image-to-sketch</h2><ul>
<li><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Vinker_CLIPascene_Scene_Sketching_with_Different_Types_and_Levels_of_Abstraction_ICCV_2023_paper.pdf" target="_blank" rel="noopener">[4]</a>[SVG]: Align image and sketch. Use MLP to predict offsets from initial points. </li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3528223.3530068" target="_blank" rel="noopener">[5]</a>[SVG]:  Align image and sketch. Use saliency for initialization. </li>
</ul>
<p>SVG:  Scalable Vector Graphics<br>SDS: score distillation sampling </p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Xing, Ximing, et al. “DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models.” arXiv preprint arXiv:2306.14685 (2023).</p>
<p>[2] Frans, Kevin, Lisa Soros, and Olaf Witkowski. “Clipdraw: Exploring text-to-drawing synthesis through language-image encoders.” Advances in Neural Information Processing Systems 35 (2022): 5207-5218.</p>
<p>[3] Jain, Ajay, Amber Xie, and Pieter Abbeel. “Vectorfusion: Text-to-svg by abstracting pixel-based diffusion models.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.</p>
<p>[4] Vinker, Yael, et al. “Clipascene: Scene sketching with different types and levels of abstraction.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.</p>
<p>[5] Vinker, Yael, et al. “Clipasso: Semantically-aware object sketching.” ACM Transactions on Graphics (TOG) 41.4 (2022): 1-11.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>generative model</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Model for Videos</title>
    <url>/2023/09/30/image_video_synthesis/Diffusion%20Model%20for%20Videos/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/pdf/2308.09592v1.pdf" target="_blank" rel="noopener">[1]</a> The edited previous frame serves as condition for editing the next frame. </p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Chai, Wenhao, et al. “StableVideo: Text-driven Consistency-aware Diffusion Video Editing.” arXiv preprint arXiv:2308.09592 (2023).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>generative model</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning Model Deployment</title>
    <url>/2023/09/04/software/Deep%20Learning%20Model%20Deployment/</url>
    <content><![CDATA[<ul>
<li>tutorial: <a href="https://mp.weixin.qq.com/s?__biz=MzI4MDcxNTY2MQ==&amp;mid=2247488952&amp;idx=1&amp;sn=880d3ad47a8fb3eab56514135f0e643b&amp;chksm=ebb51d5adcc2944c276af19e8cff5e73c934f8811706be0a94c5f47f9e767c902939903e6b95&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">[1]</a></li>
</ul>
]]></content>
      <categories>
        <category>software</category>
      </categories>
  </entry>
  <entry>
    <title>Docker</title>
    <url>/2023/09/04/software/Docker/</url>
    <content><![CDATA[<h2 id="Difference-between-VM-and-Docker"><a href="#Difference-between-VM-and-Docker" class="headerlink" title="Difference between VM and Docker"></a>Difference between VM and Docker</h2><p>VM: guest OS -&gt; BINS&amp;LIBS -&gt; App<br>Docker: BINS*LIBS -&gt; App</p>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p><a href="https://docs.docker.com/engine/install/ubuntu/" target="_blank" rel="noopener">https://docs.docker.com/engine/install/ubuntu/</a></p>
<h2 id="Uninstallation"><a href="#Uninstallation" class="headerlink" title="Uninstallation"></a>Uninstallation</h2><ul>
<li>Uninstall the Docker Engine, CLI, containerd, and Docker Compose packages: <code>sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras</code></li>
</ul>
<ul>
<li>Images, containers, volumes, or custom configuration files on your host aren’t automatically removed. To delete all images, containers, and volumes: <code>sudo rm -rf /var/lib/docker</code>, <code>sudo rm -rf /var/lib/containerd</code></li>
</ul>
<h2 id="Commands"><a href="#Commands" class="headerlink" title="Commands"></a>Commands</h2><p> <img src="https://files.mdnice.com/user/21592/15a1e052-91fa-4388-94b4-497016e19a1e.png" width="50%"></p>
<h3 id="image"><a href="#image" class="headerlink" title="image"></a>image</h3><p> <code>docker image pull $image_name:$version</code></p>
<p> <code>docker image tag $image_name:version $registryIP:port/username/image_name:version</code></p>
<p> <code>docker image push $registryIP:port/username/image_name:version</code></p>
<p> <code>docker image build -t $image_name .</code> </p>
<p> <code>docker image ls</code></p>
<p> <code>docker image rm $image_name</code></p>
<p> <code>docker image save $image_name &gt; $filename</code></p>
<p> <code>docker load &lt; $filename</code></p>
<h3 id="container"><a href="#container" class="headerlink" title="container"></a>container</h3><p> <img src="https://www.ustcnewly.com/github_images/077ev7.png" width="25%"></p>
<p>  <code>docker container create $image_name</code></p>
<p>  <code>docker container start $container_ID</code></p>
<p>  <code>docker container run $image_name</code> # <em>run</em> is equal to <em>create</em> and <em>start</em></p>
<p>  <code>docker container run -it  $image_name</code> /bin/bash</p>
<p>  <code>docker container ls</code>, <code>docker container ls --a</code> </p>
<p>  <code>docker container pause $container_ID</code> </p>
<p>  <code>docker container unpause $container_ID</code> </p>
<p>  <code>docker container stop $container_ID</code> </p>
<p>  <code>docker container kill $container_ID</code> # the difference between <em>stop</em> and <em>kill</em> is that <em>stop</em> may do some clean-up before killing the container</p>
<p>  <code>docker container rm $container_ID</code></p>
<p>  <code>docker container prune</code> # remove all the exit containers</p>
<p>  <code>docker container exec -it $containe_ID /bin/bash</code></p>
<p>  <code>docker container cp $container_ID:$file_path .</code></p>
<p>  <code>docker container commit $container_ID  $image_name:$version</code></p>
<h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2><pre><code>FROM python:3.7
WORKDIR ./docker_demo
ADD . .
RUN pip install -r requirements.txt
CMD [&quot;python&quot;, &quot;./src/main.py&quot;]
</code></pre><p>Tutorial: <a href="https://yeasy.gitbook.io/docker_practice/introduction/what" target="_blank" rel="noopener">[1]</a></p>
<h2 id="Use-GPU-in-docker"><a href="#Use-GPU-in-docker" class="headerlink" title="Use GPU in docker"></a>Use GPU in docker</h2><ol>
<li><p>Install nvidia-docker <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" target="_blank" rel="noopener">https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html</a></p>
</li>
<li><p><code>docker run --gpus all -it $image_name</code></p>
</li>
</ol>
]]></content>
      <categories>
        <category>software</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Segment Anything</title>
    <url>/2023/07/14/image_video_synthesis/Segment%20Anything/</url>
    <content><![CDATA[<ul>
<li><p>SAM <a href="https://arxiv.org/pdf/2304.02643.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>FastSAM <a href="https://arxiv.org/pdf/2306.12156.pdf" target="_blank" rel="noopener">[2]</a>: first generate proposals and then select target proposals </p>
</li>
<li><p>High-quality SAM <a href="https://arxiv.org/pdf/2306.01567.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
<li><p>Semantic-SAM <a href="https://arxiv.org/pdf/2306.01567.pdf" target="_blank" rel="noopener">[4]</a>: assign semantic labels </p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Kirillov, Alexander, et al. “Segment anything.” arXiv preprint arXiv:2304.02643 (2023).</p>
<p>[2] Zhao, Xu, et al. “Fast Segment Anything.” arXiv preprint arXiv:2306.12156 (2023).</p>
<p>[3] Ke, Lei, et al. “Segment Anything in High Quality.” arXiv preprint arXiv:2306.01567 (2023).</p>
<p>[4] Li, Feng, et al. “Semantic-SAM: Segment and Recognize Anything at Any Granularity.” arXiv preprint arXiv:2307.04767 (2023).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>Drag Image Editing</title>
    <url>/2023/07/14/image_video_synthesis/Drag%20Image%20Editing/</url>
    <content><![CDATA[<ul>
<li>DragGAN <a href="https://vcai.mpi-inf.mpg.de/projects/DragGAN/data/paper.pdf" target="_blank" rel="noopener">[1]</a>: motion supervision and point tracking</li>
<li>DragDiffusion <a href="https://arxiv.org/pdf/2306.14435.pdf" target="_blank" rel="noopener">[2]</a>: motion supervision and point tracking, similar to [1]</li>
<li>DragonDiffusion <a href="https://arxiv.org/pdf/2307.02421.pdf" target="_blank" rel="noopener">[3]</a>: no point tracking, two parallel feature branches</li>
<li>FreeDrag <a href="https://arxiv.org/pdf/2307.04684.pdf" target="_blank" rel="noopener">[4]</a>: no point tracking, adaptive template features, line search, fuzzy localization</li>
</ul>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Pan, Xingang, et al. “Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold.” SIGGRAPH (2023).</p>
<p>[2] Shi, Yujun, et al. “DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing.” arXiv preprint arXiv:2306.14435 (2023).</p>
<p>[3] Mou, Chong, et al. “DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models.” arXiv preprint arXiv:2307.02421 (2023).</p>
<p>[4] Ling, Pengyang, et al. “FreeDrag: Point Tracking is Not You Need for Interactive Point-based Image Editing.” arXiv preprint arXiv:2307.04684 (2023).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>image editing</tag>
      </tags>
  </entry>
  <entry>
    <title>Disentangled Generative Model</title>
    <url>/2023/06/20/image_video_synthesis/Disentangled%20Generative%20Model/</url>
    <content><![CDATA[<p>A survey on disentangled representation learning <a href="https://arxiv.org/pdf/2211.11695.pdf" target="_blank" rel="noopener">[1]</a></p>
<h3 id="Disentangled-Diffusion-Model"><a href="#Disentangled-Diffusion-Model" class="headerlink" title="Disentangled Diffusion Model"></a>Disentangled Diffusion Model</h3><ul>
<li><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf" target="_blank" rel="noopener">[2]</a>: make adjustment based on text embedding, learn optimal combination coefficients for different time steps. </li>
<li><a href="https://arxiv.org/pdf/2301.13721.pdf" target="_blank" rel="noopener">[3]</a>: disentangled gradient field, predict gradients conditioned on latent factors</li>
<li><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Preechakul_Diffusion_Autoencoders_Toward_a_Meaningful_and_Decodable_Representation_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[4]</a>: semantic subcode and stochastic details</li>
<li><a href="https://arxiv.org/pdf/2210.10960.pdf" target="_blank" rel="noopener">[5]</a>: predict the direction change in the latent h-space</li>
</ul>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Xin Wang, Hong Chen, Siao Tang, Zihao Wu, and Wenwu Zhu. “Disentangled Representation Learning.”</p>
<p>[2] Wu, Qiucheng, et al. “Uncovering the disentanglement capability in text-to-image diffusion models.” CVPR, 2023.</p>
<p>[3] Yang, Tao, et al. “DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models.” arXiv preprint arXiv:2301.13721 (2023).</p>
<p>[4] Preechakul, Konpat, et al. “Diffusion autoencoders: Toward a meaningful and decodable representation.” CVPR, 2022.</p>
<p>[5] Kwon, Mingi, Jaeseok Jeong, and Youngjung Uh. “Diffusion models already have a semantic latent space.” arXiv preprint arXiv:2210.10960 (2022).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>VAE</tag>
        <tag>diffusion model</tag>
      </tags>
  </entry>
  <entry>
    <title>Blender</title>
    <url>/2022/11/14/software/Blender/</url>
    <content><![CDATA[<ol>
<li><a href="https://github.com/yuki-koyama/blender-cli-rendering" target="_blank" rel="noopener">https://github.com/yuki-koyama/blender-cli-rendering</a></li>
<li><a href="https://github.com/FrozenBurning/Text2Light" target="_blank" rel="noopener">https://github.com/FrozenBurning/Text2Light</a> </li>
</ol>
]]></content>
      <categories>
        <category>software</category>
      </categories>
      <tags>
        <tag>rendering</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt for Vision</title>
    <url>/2022/10/17/deep_learning/Prompt%20for%20Vision/</url>
    <content><![CDATA[<ul>
<li><p>Prompt for image-to-image translation: <a href="https://arxiv.org/pdf/1811.05181.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>Prompt for visual grounding: <a href="https://arxiv.org/pdf/2109.11797.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
</ul>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Bar, Amir, et al. “Visual Prompting via Image Inpainting.” arXiv preprint arXiv:2209.00647 (2022).</p>
<p>[2] Yao, Yuan, et al. “Cpt: Colorful prompt tuning for pre-trained vision-language models.” arXiv preprint arXiv:2109.11797 (2021).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Smoothness Loss</title>
    <url>/2022/10/15/deep_learning/Smoothness%20Loss/</url>
    <content><![CDATA[<ol>
<li>Total Variation (TV) loss  </li>
<li>Poisson blending loss <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Fu_Auto-Exposure_Fusion_for_Single-Image_Shadow_Removal_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1910.11495.pdf" target="_blank" rel="noopener">[2]</a></li>
<li>Gradient loss <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_A_Late_Fusion_CNN_for_Digital_Matting_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>Laplacian loss <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Context-Aware_Image_Matting_for_Simultaneous_Foreground_and_Alpha_Estimation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://github.com/JizhiziLi/AIM/blob/48e3dc8d0f7308b42fe0593beb51f33e63ed2ddc/core/evaluate.py#L82" target="_blank" rel="noopener">[2]</a></li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Image Matting</title>
    <url>/2022/09/23/deep_learning/Image%20Matting/</url>
    <content><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>The target is separating foreground from background given some user annotation (e.g., trimask, scribble). The prevalent technique <a href="http://www.alphamatting.com/code.php" target="_blank" rel="noopener">alpha matting</a> is to solve $\mathbf{\alpha}$ (primary target), $\mathbf{F}$, $\mathbf{B}$ (subordinate target) in  $\mathbf{I}=\mathbf{\alpha}\circ\mathbf{F}+(1-\mathbf{\alpha})\circ \mathbf{B}$ <a href="https://dl.acm.org/ft_gateway.cfm?id=808606&amp;ftid=63750&amp;dwn=1&amp;CFID=4983534&amp;CFTOKEN=96dd1cd10963517e-788E0FD4-99DA-7734-E93FEF8C75E17956" target="_blank" rel="noopener">[1]</a> <a href="http://delivery.acm.org/10.1145/810000/808606/p253-porter.pdf?ip=202.120.14.211&amp;id=808606&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2E17676C47DFB149BF%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1544674017_517758b7f09ff19d5c0dc0db95a9091e" target="_blank" rel="noopener">[2]</a> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.248.3736&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[3]</a>.</p>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><ul>
<li><p><a href="http://alphamatting.com/" target="_blank" rel="noopener">Alphamatting.com</a> Dataset: 25 train images, 8 test images, each has 3 different trimaps: small, large, user. Input: image and trimap.</p>
</li>
<li><p><a href="https://sites.google.com/view/deepimagematting" target="_blank" rel="noopener">Composition-1k Dataset</a>: 1000 images and 50 unique foregrounds.</p>
</li>
<li><p><a href="https://github.com/lizhengwei1992/Semantic_Human_Matting" target="_blank" rel="noopener">Matting Human Dataset</a>: 34427 images, annotation is not very accurate.</p>
</li>
<li><p><a href="https://github.com/wukaoliu/CVPR2020-HAttMatting" target="_blank" rel="noopener">Dinstinctions-646</a>: composed of 646 foreground images</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/WACV2021/papers/Kang_ATM_Attentional_Text_Matting_WACV_2021_paper.pdf" target="_blank" rel="noopener">Text matting dataset</a></p>
</li>
</ul>
<h2 id="Evaluation-metrics"><a href="#Evaluation-metrics" class="headerlink" title="Evaluation metrics"></a>Evaluation metrics</h2><ul>
<li>quantitative: Sum of Absolute Differences (SAD), Mean Square Error (MSE), Gradient error, Connectivity error.</li>
</ul>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><ol>
<li><p>Affinity-based <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Aksoy_Designing_Effective_Inter-Pixel_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[1]</a>: pixel similarity metrics that rely on color similarity or spatial proximity. </p>
</li>
<li><p>Sampling-based <a href="https://link.springer.com/chapter/10.1007/978-3-319-46475-6_13" target="_blank" rel="noopener">[8]</a>: the foreground/background color of unknown pixels can be obtained by sampling the foreground/background color of known pixels.</p>
</li>
<li><p>Learning-based</p>
<ul>
<li>With trimap:<ul>
<li>Encoder-Decoder network <a href="https://arxiv.org/pdf/1703.03872.pdf" target="_blank" rel="noopener">[2]</a> is the first end-to-end method for image matting: input image and trimap, output alpha; alpha loss and compositional loss; refine alpha.</li>
<li>DeepMattePropNet <a href="https://www.ijcai.org/proceedings/2018/0139.pdf" target="_blank" rel="noopener">[4]</a>: use deep learning to approximate affinity-based matting method; compositional loss.</li>
<li>AlphaGAN <a href="https://arxiv.org/pdf/1807.10088.pdf" target="_blank" rel="noopener">[6]</a>: combine GAN with alpha loss and compositional loss.</li>
<li>Learning based sampling <a href="http://people.inf.ethz.ch/aksoyy/papers/CVPR19-samplenet.pdf" target="_blank" rel="noopener">[7]</a></li>
</ul>
</li>
<li>Without trimap:<ul>
<li>Light Dense Network (LDN) + Feathering Block (FB) <a href="https://arxiv.org/pdf/1707.08289.pdf" target="_blank" rel="noopener">[3]</a>: generate segmentation mask and refine the mask with feathering block. </li>
<li>T-Net+M-net <a href="https://arxiv.org/pdf/1809.01354.pdf" target="_blank" rel="noopener">[5]</a>: use segmentation task as trimap</li>
<li><a href="https://arxiv.org/pdf/2004.00626.pdf" target="_blank" rel="noopener">[9]</a>: capture the background image without subject and a corresponding video with subject</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Losses"><a href="#Losses" class="headerlink" title="Losses"></a>Losses</h2><p>gradient loss <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_A_Late_Fusion_CNN_for_Digital_Matting_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[11]</a> Laplacian loss <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Context-Aware_Image_Matting_for_Simultaneous_Foreground_and_Alpha_Estimation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[12]</a></p>
<h2 id="Extension"><a href="#Extension" class="headerlink" title="Extension"></a>Extension</h2><p>Omnimatte <a href="https://arxiv.org/pdf/2105.06993.pdf" target="_blank" rel="noopener">[10]</a>: segment objects and scene effects related to the objects (shadows, reflections, smoke)</p>
<h2 id="User-guided-Image-Matting"><a href="#User-guided-Image-Matting" class="headerlink" title="User-guided Image Matting"></a>User-guided Image Matting</h2><p>unified interactive image matting: <a href="https://arxiv.org/pdf/2205.08324.pdf" target="_blank" rel="noopener">[13]</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h3><p>[1] Aksoy, Yagiz, Tunc Ozan Aydin, and Marc Pollefeys. “Designing effective inter-pixel information flow for natural image matting.” CVPR, 2017.</p>
<p>[2] Xu, Ning, et al. “Deep image matting.” CVPR, 2017.</p>
<p>[3] Zhu, Bingke, et al. “Fast deep matting for portrait animation on mobile phone.” ACM MM, 2017.</p>
<p>[4] Wang, Yu, et al. “Deep Propagation Based Image Matting.” IJCAI. 2018.</p>
<p>[5] Quan Chen, Tiezheng Ge, Yanyu Xu, Zhiqiang Zhang, Xinxin Yang, Kun Gai, “Semantic Human Matting.” ACM MM, 2018.</p>
<p>[6] Lutz, Sebastian, Konstantinos Amplianitis, and Aljosa Smolic. “AlphaGAN: Generative adversarial networks for natural image matting.” BMVC, 2018.</p>
<p>[7] Jingwei Tang, Yagız Aksoy, Cengiz Oztireli, Markus Gross, Tunc Ozan Aydın. “Learning-based Sampling for Natural Image Matting”, CVPR, 2019.</p>
<p>[8] Feng, Xiaoxue, Xiaohui Liang, and Zili Zhang. “A cluster sampling method for image matting via sparse coding.” ECCV, 2016.</p>
<p>[9] Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steve Seitz, Ira Kemelmacher-Shlizerman:<br>Background Matting: The World is Your Green Screen. CVPR, 2020.</p>
<p>[10] Lu, Erika, et al. “Omnimatte: Associating Objects and Their Effects in Video.” CVPR, 2021.</p>
<p>[11] Zhang, Yunke, et al. “A late fusion cnn for digital matting.” CVPR, 2019.</p>
<p>[12] Hou, Qiqi, and Feng Liu. “Context-aware image matting for simultaneous foreground and alpha estimation.” ICCV. 2019.</p>
<p>[13] Yang, Stephen, et al. “Unified interactive image matting.” arXiv preprint arXiv:2205.08324 (2022).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>matting</tag>
      </tags>
  </entry>
  <entry>
    <title>Consistent Video Editing</title>
    <url>/2022/09/20/image_video_synthesis/Consistent%20Video%20Editing/</url>
    <content><![CDATA[<ul>
<li>Template based: <a href="https://arxiv.org/pdf/2109.11418.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ye_Deformable_Sprites_for_Unsupervised_Video_Decomposition_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[2]</a></li>
</ul>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol>
<li><p>Kasten, Yoni, et al. “Layered neural atlases for consistent video editing.” ACM Transactions on Graphics (TOG) 40.6 (2021): 1-12.</p>
</li>
<li><p>Ye, Vickie, et al. “Deformable Sprites for Unsupervised Video Decomposition.” CVPR, 2022.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Artistic and Photorealistic Style Transfer</title>
    <url>/2022/09/19/deep_learning/Artistic%20and%20Photorealistic%20Style%20Transfer/</url>
    <content><![CDATA[<h3 id="Transfer-strategy"><a href="#Transfer-strategy" class="headerlink" title="Transfer strategy"></a>Transfer strategy</h3><ul>
<li><p>Feature transfer: AdaIN <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[1]</a>, WCT <a href="https://arxiv.org/pdf/1705.08086.pdf" target="_blank" rel="noopener">[2]</a>, SANet <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Park_Arbitrary_Style_Transfer_With_Style-Attentional_Networks_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[3]</a>. </p>
</li>
<li><p><a href="https://ustcnewly.github.io/2022/01/21/deep_learning/Color%20Mapping/">Color transfer</a>: Learn color transformation (explicit function or implicit function (e.g., look-up table) conditioned on color values, location, semantic information, or other guidance. </p>
</li>
</ul>
<h3 id="Compare-Different-Backbones"><a href="#Compare-Different-Backbones" class="headerlink" title="Compare Different Backbones"></a>Compare Different Backbones</h3><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Rethinking_and_Improving_the_Robustness_of_Image_Style_Transfer_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[6]</a> <a href="http://jcst.ict.ac.cn/fileup/1000-9000/PDF/2022-3-8-2140.pdf" target="_blank" rel="noopener">[7]</a></p>
<h3 id="Losses"><a href="#Losses" class="headerlink" title="Losses:"></a>Losses:</h3><ul>
<li>paired supervision: L2 loss</li>
<li>unpaired supervision: adversarial loss</li>
<li>smooth loss: variation loss, Poisson loss</li>
<li>content loss: perception loss</li>
<li>style loss: Gram loss, AdaIn loss</li>
</ul>
<h3 id="Multi-scale-stylization"><a href="#Multi-scale-stylization" class="headerlink" title="Multi-scale stylization"></a>Multi-scale stylization</h3><ul>
<li><p>parallel: <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_AdaAttN_Revisit_Attention_Mechanism_in_Arbitrary_Neural_Style_Transfer_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[4]</a></p>
</li>
<li><p>sequential: <a href="https://arxiv.org/pdf/2004.10955.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Huang, Xun, and Serge Belongie. “Arbitrary style transfer in real-time with adaptive instance normalization.” ICCV, 2017.</p>
<p>[2] Li, Yijun, et al. “Universal style transfer via feature transforms.” NeurIPS, 2017.</p>
<p>[3] Park, Dae Young, and Kwang Hee Lee. “Arbitrary style transfer with style-attentional networks.” CVPR, 2019.</p>
<p>[4] Liu, Songhua, et al. “Adaattn: Revisit attention mechanism in arbitrary neural style transfer.” ICCV, 2021.</p>
<p>[5] Xia, Xide, et al. “Joint bilateral learning for real-time universal photorealistic style transfer.” ECCV, 2020.</p>
<p>[6] Wang, Pei, Yijun Li, and Nuno Vasconcelos. “Rethinking and improving the robustness of image style transfer.” CVPR, 2021.</p>
<p>[7] Wei, Hua-Peng, et al. “A Comparative Study of CNN-and Transformer-Based Visual Style Transfer.” Journal of Computer Science and Technology 37.3 (2022): 601-614.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>style transfer</tag>
      </tags>
  </entry>
  <entry>
    <title>Dynamic Kernel</title>
    <url>/2022/09/19/deep_learning/Dynamic%20Kernel/</url>
    <content><![CDATA[<p>Dynamic kernels: <a href="https://proceedings.neurips.cc/paper/2016/file/8bf1211fd4b7b94528899de0a43b9fb3-Paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/2003.05664.pdf" target="_blank" rel="noopener">[2]</a></p>
<p>Survey: <a href="https://arxiv.org/pdf/2102.04906" target="_blank" rel="noopener">[Dynamic neural networks: A survey]</a></p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol>
<li><p>Jia, Xu, et al. “Dynamic filter networks.” Advances in neural information processing systems 29 (2016).</p>
</li>
<li><p>Tian, Zhi, Chunhua Shen, and Hao Chen. “Conditional convolutions for instance segmentation.” European conference on computer vision. Springer, Cham, 2020.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Virtual Try-on</title>
    <url>/2022/09/10/deep_learning/Virtual%20Try-on/</url>
    <content><![CDATA[<h4 id="warping"><a href="#warping" class="headerlink" title="warping"></a>warping</h4><ul>
<li>correspondence matrix <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_CT-Net_Complementary_Transfering_Network_for_Garment_Transfer_With_Arbitrary_Geometric_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Morelli_Dress_Code_High-Resolution_Multi-Category_Virtual_Try-On_CVPRW_2022_paper.pdf" target="_blank" rel="noopener">[4]</a></li>
<li>TPS <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_CT-Net_Complementary_Transfering_Network_for_Garment_Transfer_With_Arbitrary_Geometric_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Fenocchi_Dual-Branch_Collaborative_Transformer_for_Virtual_Try-On_CVPRW_2022_paper.pdf" target="_blank" rel="noopener">[3]</a> </li>
<li>offset and weight <a href="https://arxiv.org/pdf/2207.09161.pdf" target="_blank" rel="noopener">[2]</a></li>
</ul>
<h4 id="target-person"><a href="#target-person" class="headerlink" title="target person"></a>target person</h4><p>Garment Transfer: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_CT-Net_Complementary_Transfering_Network_for_Garment_Transfer_With_Arbitrary_Geometric_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[5]</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9525061" target="_blank" rel="noopener">[6]</a> <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Amit_Raj_SwapNet_Garment_Transfer_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[8]</a></p>
<p>Controllable person image synthesis: <a href="https://arxiv.org/pdf/2208.00712.pdf" target="_blank" rel="noopener">[7]</a></p>
<p>Recurrent Person Image Generation: <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Cui_Dressing_in_Order_Recurrent_Person_Image_Generation_for_Pose_Transfer_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[9]</a></p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ol>
<li><p>Yang, Fan, and Guosheng Lin. “CT-Net: Complementary Transfering Network for Garment Transfer with Arbitrary Geometric Changes.” CVPR, 2021.</p>
</li>
<li><p>Bai, Shuai, et al. “Single Stage Virtual Try-on via Deformable Attention Flows.” arXiv preprint arXiv:2207.09161 (2022).</p>
</li>
<li><p>Fenocchi, Emanuele, et al. “Dual-Branch Collaborative Transformer for Virtual Try-On.” CVPR, 2022.</p>
</li>
<li><p>Morelli, Davide, et al. “Dress Code: High-Resolution Multi-Category Virtual Try-On.” CVPR, 2022.</p>
</li>
<li><p>Fan Yang, Guosheng Lin. “CT-Net: Complementary Transfering Network for Garment Transfer with Arbitrary Geometric Changes.” CVPR, 2021.</p>
</li>
<li><p>Liu, Ting, et al. “Spatial-aware texture transformer for high-fidelity garment transfer.” IEEE Transactions on Image Processing 30 (2021): 7499-7510.</p>
</li>
<li><p>Zhou, Xinyue, et al. “Cross Attention Based Style Distribution for Controllable Person Image Synthesis.” arXiv preprint arXiv:2208.00712 (2022).</p>
</li>
<li><p>Raj, Amit, et al. “Swapnet: Image based garment transfer.” European Conference on Computer Vision. Springer, Cham, 2018.</p>
</li>
<li><p>Cui, Aiyu, Daniel McKee, and Svetlana Lazebnik. “Dressing in order: Recurrent person image generation for pose transfer, virtual try-on and outfit editing.” ICCV, 2021.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Diffusion Model</title>
    <url>/2022/09/09/image_video_synthesis/Diffusion%20Model/</url>
    <content><![CDATA[<ul>
<li><p>class-conditioned image generation:  <a href="https://arxiv.org/pdf/2105.05233.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>Image-to-image translation: <a href="https://arxiv.org/pdf/2205.12952.pdf" target="_blank" rel="noopener">[4]</a>, <a href="https://arxiv.org/pdf/2209.15264.pdf" target="_blank" rel="noopener">[7]</a>, <a href="https://arxiv.org/pdf/2108.02938.pdf" target="_blank" rel="noopener">[6]</a>, <a href="https://openreview.net/pdf?id=aBsCjcPu_tE" target="_blank" rel="noopener">[8]</a> </p>
</li>
<li><p>Image-to-image translation with guidance: GLIDE<a href="https://arxiv.org/pdf/2112.10741v1.pdf" target="_blank" rel="noopener">[2]</a>(global: text), <a href="https://arxiv.org/pdf/2211.13752.pdf" target="_blank" rel="noopener">[20]</a>(global: text, sketch), <a href="https://arxiv.org/pdf/2211.15518.pdf" target="_blank" rel="noopener">[21]</a>(local: text), <a href="https://arxiv.org/pdf/2301.07093.pdf" target="_blank" rel="noopener">[22]</a>(local: text, image), ControlNet<a href="https://arxiv.org/pdf/2302.05543.pdf" target="_blank" rel="noopener">[23]</a>(global: mixture), T2I-Adapter<a href="https://arxiv.org/pdf/2302.08453.pdf" target="_blank" rel="noopener">[24]</a>(global: mixture), <a href="https://arxiv.org/pdf/2302.09778.pdf" target="_blank" rel="noopener">[25]</a>(local: mixture), <a href="https://arxiv.org/pdf/2302.13848.pdf" target="_blank" rel="noopener">[26]</a>(global: text), <a href="https://arxiv.org/pdf/2305.16322.pdf" target="_blank" rel="noopener">[27]</a>, Ctrl-Adapter <a href="https://arxiv.org/pdf/2404.09967" target="_blank" rel="noopener">[36]</a></p>
</li>
<li><p>unpaired Image-to-image translation: <a href="https://arxiv.org/pdf/2209.15264.pdf" target="_blank" rel="noopener">[19]</a> <a href="https://arxiv.org/pdf/2104.05358.pdf" target="_blank" rel="noopener">[28]</a> <a href="https://arxiv.org/pdf/2203.08382.pdf" target="_blank" rel="noopener">[29]</a></p>
</li>
<li><p>Image composition: SDEdit <a href="https://openreview.net/pdf?id=aBsCjcPu_tE" target="_blank" rel="noopener">[17]</a>, ILVR <a href="https://arxiv.org/pdf/2108.02938.pdf" target="_blank" rel="noopener">[6]</a>, <a href="https://arxiv.org/pdf/2302.10167.pdf" target="_blank" rel="noopener">[5]</a>, <a href="https://arxiv.org/pdf/2211.13227.pdf" target="_blank" rel="noopener">[9]</a>, <a href="https://arxiv.org/pdf/2212.00932.pdf" target="_blank" rel="noopener">[15]</a></p>
</li>
<li><p>Image inpainting: <a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[10]</a>,  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[11]</a>, <a href="https://arxiv.org/pdf/2212.02963.pdf" target="_blank" rel="noopener">[12]</a>, <a href="https://arxiv.org/pdf/2212.06909.pdf" target="_blank" rel="noopener">[13]</a></p>
</li>
<li><p>Predict mask: <a href="https://arxiv.org/pdf/2303.11681.pdf" target="_blank" rel="noopener">[31]</a> cross-attention and post processing; <a href="https://arxiv.org/pdf/2212.05034.pdf" target="_blank" rel="noopener">[32]</a> add one output channel; <a href="https://arxiv.org/pdf/2303.17870.pdf" target="_blank" rel="noopener">[33]</a> predict masks using the feature maps in early steps. </p>
</li>
</ul>
<p>MileStone: DDPM <a href="https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf" target="_blank" rel="noopener">[3]</a>, Stable diffusion <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf" target="_blank" rel="noopener">v1</a>, <a href="https://github.com/Stability-AI/stablediffusion" target="_blank" rel="noopener">v2</a>, <a href="https://arxiv.org/pdf/2307.01952.pdf" target="_blank" rel="noopener">XL</a>, <a href="https://arxiv.org/pdf/2403.03206.pdf" target="_blank" rel="noopener">v3</a></p>
<p>Acceleration: DDIM <a href="https://arxiv.org/pdf/2010.02502.pdf" target="_blank" rel="noopener">[14]</a>, PLMS <a href="https://arxiv.org/pdf/2202.09778.pdf" target="_blank" rel="noopener">[16]</a></p>
<p>High-resolution: <a href="https://arxiv.org/pdf/2310.15111.pdf" target="_blank" rel="noopener">[34]</a> progressive training</p>
<p>Light-weight: <a href="https://arxiv.org/pdf/2306.00980.pdf" target="_blank" rel="noopener">[35]</a> </p>
<p>Failure case analyses: <a href="https://arxiv.org/pdf/2306.02583.pdf" target="_blank" rel="noopener">[30]</a></p>
<p>Surveys</p>
<ul>
<li><a href="https://arxiv.org/pdf/2209.00796.pdf" target="_blank" rel="noopener">Diffusion Models: A Comprehensive Survey of Methods and Applications</a></li>
<li><a href="https://arxiv.org/pdf/2209.04747.pdf" target="_blank" rel="noopener">Diffusion Models in Vision: A Survey</a></li>
</ul>
<p>Tutorial materials: <a href="https://mp.weixin.qq.com/s/WGC6bhSNasqk8b0D2r8S1g" target="_blank" rel="noopener">[a]</a> <a href="https://zhuanlan.zhihu.com/p/610012156?utm_campaign=shareopn&amp;utm_medium=social&amp;utm_oi=1016221980986503168&amp;utm_psn=1614159296316567552&amp;utm_source=wechat_session" target="_blank" rel="noopener">[b]</a></p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Dhariwal, Prafulla, and Alex Nichol. “Diffusion models beat gans on image synthesis.” arXiv preprint arXiv:2105.05233 (2021).</p>
<p>[2] Nichol, Alex, et al. “Glide: Towards photorealistic image generation and editing with text-guided diffusion models.” arXiv preprint arXiv:2112.10741 (2021).</p>
<p>[3] Ho, Jonathan, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic models.” Advances in Neural Information Processing Systems 33 (2020): 6840-6851.</p>
<p>[4] Wang, Tengfei, et al. “Pretraining is All You Need for Image-to-Image Translation.” arXiv preprint arXiv:2205.12952 (2022).</p>
<p>[5] Hachnochi, Roy, et al. “Cross-domain Compositing with Pretrained Diffusion Models.” arXiv preprint arXiv:2302.10167 (2023).</p>
<p>[6] Choi, Jooyoung, et al. “ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models.” ICCV, 2021.</p>
<p>[7] Kwon, Gihyun, and Jong Chul Ye. “Diffusion-based image translation using disentangled style and content representation.” ICLR, 2023.</p>
<p>[8] Meng, Chenlin, et al. “Sdedit: Guided image synthesis and editing with stochastic differential equations.” ICLR, 2021.</p>
<p>[9] Yang, Binxin, et al. “Paint by Example: Exemplar-based Image Editing with Diffusion Models.” arXiv preprint arXiv:2211.13227 (2022).</p>
<p>[10] Lugmayr, Andreas, et al. “Repaint: Inpainting using denoising diffusion probabilistic models.” CVPR, 2022.</p>
<p>[11] Rombach, Robin, et al. “High-resolution image synthesis with latent diffusion models.” CVPR, 2022.</p>
<p>[12] Li, Wenbo, et al. “SDM: Spatial Diffusion Model for Large Hole Image Inpainting.” arXiv preprint arXiv:2212.02963 (2022).</p>
<p>[13] Wang, Su, et al. “Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting.” arXiv preprint arXiv:2212.06909 (2022).</p>
<p>[14] Song, Jiaming, Chenlin Meng, and Stefano Ermon. “Denoising diffusion implicit models.” arXiv preprint arXiv:2010.02502 (2020).</p>
<p>[15] Song, Yizhi, et al. “ObjectStitch: Generative Object Compositing.” CVPR, 2023.</p>
<p>[16] Liu, Luping, et al. “Pseudo numerical methods for diffusion models on manifolds.” ICLR, (2022).</p>
<p>[17] Meng, Chenlin, et al. “Sdedit: Guided image synthesis and editing with stochastic differential equations.” ICLR, 2021.</p>
<p>[19] Kwon, Gihyun, and Jong Chul Ye. “Diffusion-based image translation using disentangled style and content representation.” ILCR, 2023.</p>
<p>[20] Voynov, Andrey, Kfir Aberman, and Daniel Cohen-Or. “Sketch-Guided Text-to-Image Diffusion Models.” arXiv preprint arXiv:2211.13752 (2022).</p>
<p>[21] Yang, Zhengyuan, et al. “ReCo: Region-Controlled Text-to-Image Generation.” arXiv preprint arXiv:2211.15518 (2022).</p>
<p>[22] Li, Yuheng, et al. “GLIGEN: Open-Set Grounded Text-to-Image Generation.” arXiv preprint arXiv:2301.07093 (2023).</p>
<p>[23] Zhang, Lvmin, and Maneesh Agrawala. “Adding conditional control to text-to-image diffusion models.” arXiv preprint arXiv:2302.05543 (2023).</p>
<p>[24] Mou, Chong, et al. “T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.” arXiv preprint arXiv:2302.08453 (2023).</p>
<p>[25] Huang, Lianghua, et al. “Composer: Creative and controllable image synthesis with composable conditions.” arXiv preprint arXiv:2302.09778 (2023).</p>
<p>[26] Wei, Yuxiang, et al. “Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation.” arXiv preprint arXiv:2302.13848 (2023).</p>
<p>[27] Zhao, Shihao, et al. “Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models.” arXiv preprint arXiv:2305.16322 (2023).</p>
<p>[28] Sasaki, Hiroshi, Chris G. Willcocks, and Toby P. Breckon. “Unit-ddpm: Unpaired image translation with denoising diffusion probabilistic models.” arXiv preprint arXiv:2104.05358 (2021).</p>
<p>[29] Su, Xuan, et al. “Dual diffusion implicit bridges for image-to-image translation.” arXiv preprint arXiv:2203.08382 (2022).</p>
<p>[30] Chengbin Du, Yanxi Li, Zhongwei Qiu, Chang Xu, “Stable Diffusion is Unstable”. </p>
<p>[31] Wu, Weijia, et al. “Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models.” arXiv preprint arXiv:2303.11681 (2023).</p>
<p>[32] Xie, Shaoan, et al. “Smartbrush: Text and shape guided object inpainting with diffusion model.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.</p>
<p>[33] Ma, Jian, et al. “GlyphDraw: Learning to Draw Chinese Characters in Image Synthesis Models Coherently.” arXiv preprint arXiv:2303.17870 (2023).</p>
<p>[34] Gu, Jiatao, et al. “Matryoshka Diffusion Models.” arXiv preprint arXiv:2310.15111 (2023).  </p>
<p>[35] Li, Yanyu, et al. “SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds.” NeurIPS(2023).</p>
<p>[36] Lin, Han, et al. “Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model.” arXiv preprint arXiv:2404.09967 (2024).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>generative model</tag>
      </tags>
  </entry>
  <entry>
    <title>Exemplar-guided Image Translation</title>
    <url>/2022/09/09/image_video_synthesis/Exemplar-guided%20Image%20Translation/</url>
    <content><![CDATA[<p>Task: Each exemplar represents one domain. Transfer the style of exemplar image to the input image. </p>
<p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Cross-Domain_Correspondence_Learning_for_Exemplar-Based_Image_Translation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[1]</a></p>
<ul>
<li><p>reconstruct the style code: <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Anokhin_High-Resolution_Daytime_Translation_Without_Domain_Labels_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>use pretrained network (prior knowledge) to extract the style code: <a href="https://arxiv.org/pdf/2201.00424.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
</ul>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ol>
<li><p>Zhang, Pan, et al. “Cross-domain correspondence learning for exemplar-based image translation.” CVPR, 2020.</p>
</li>
<li><p>Anokhin, Ivan, et al. “High-resolution daytime translation without domain labels.” CVPR, 2020.</p>
</li>
<li><p>Tumanyan, Narek, et al. “Splicing ViT Features for Semantic Appearance Transfer.” CVPR, 2022.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>image translation</tag>
      </tags>
  </entry>
  <entry>
    <title>NERF</title>
    <url>/2022/08/22/deep_learning/NERF/</url>
    <content><![CDATA[<ul>
<li><p>NERF <a href="https://arxiv.org/pdf/2003.08934.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>GIRAFFE <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Niemeyer_GIRAFFE_Representing_Scenes_As_Compositional_Generative_Neural_Feature_Fields_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Mildenhall, Ben, et al. “Nerf: Representing scenes as neural radiance fields for view synthesis.” ECCV, 2020.</p>
<p>[2] Niemeyer, Michael, and Andreas Geiger. “Giraffe: Representing scenes as compositional generative neural feature fields.” CVPR, 2021.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Mask Form</title>
    <url>/2022/07/25/machine_learning/Mask%20Form/</url>
    <content><![CDATA[<ol>
<li><p>binary map</p>
</li>
<li><p>frequency: DCT <a href="https://arxiv.org/pdf/2011.09876.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>PolarMask <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_PolarMask_Single_Shot_Instance_Segmentation_With_Polar_Representation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>Hyperbolic <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Atigh_Hyperbolic_Image_Segmentation_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
</ol>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>[1] Shen, Xing, et al. “Dct-mask: Discrete cosine transform mask representation for instance segmentation.” CVPR, 2021.</p>
<p>[2] Xie, Enze, et al. “Polarmask: Single shot instance segmentation with polar representation.” CVPR, 2020.</p>
<p>[3] GhadimiAtigh, Mina, et al. “Hyperbolic Image Segmentation.” arXiv preprint arXiv:2203.05898 (2022).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>To a Beginner on Paper Writing</title>
    <url>/2022/07/22/others/To%20a%20Beginner%20on%20Paper%20Writing/</url>
    <content><![CDATA[<ul>
<li><p>Carefully read the following instructions. These are the key points you should pay attention to when writing papers.</p>
<ul>
<li><a href="https://ustcnewly.github.io/2022/06/16/others/Paper%20Writing/">https://ustcnewly.github.io/2022/06/16/others/Paper%20Writing/</a></li>
<li><a href="https://ustcnewly.github.io/2022/06/16/others/Paper%20Proofread/">https://ustcnewly.github.io/2022/06/16/others/Paper%20Proofread/</a></li>
<li><a href="https://ustcnewly.github.io/2022/06/16/others/Reasons%20to%20Reject%20a%20Paper/">https://ustcnewly.github.io/2022/06/16/others/Reasons%20to%20Reject%20a%20Paper/</a> </li>
</ul>
</li>
</ul>
<ul>
<li><p>The commonly used words in academic papers are summarized in <a href="https://ustcnewly.github.io/2022/06/16/others/Dictionary%20for%20Paper%20Writing/">https://ustcnewly.github.io/2022/06/16/others/Dictionary%20for%20Paper%20Writing/</a>.</p>
</li>
<li><p>Before writing your own paper, carefully read 10 closely related papers and record the materials (words/phrases/sentences) which could be used in your paper. Organize your collected materials and think about when to use them. Do not copy them word-by-word, you need to incorporate them into your own paper coherently and seamlessly. </p>
</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>Network Architecture</title>
    <url>/2022/07/15/deep_learning/Network%20Architecture/</url>
    <content><![CDATA[<ol>
<li><p><a href="https://ustcnewly.github.io/2022/06/16/deep_learning/Transformer/">Transformer</a></p>
</li>
<li><p>Large kernel: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://arxiv.org/pdf/2207.03620.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
</ol>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>[1] Liu, Zhuang, et al. “A convnet for the 2020s.” CVPR, 2022.</p>
<p>[2] Ding, Xiaohan, et al. “Scaling up your kernels to 31x31: Revisiting large kernel design in cnns.” CVPR, 2022.</p>
<p>[3] More ConvNets in the 2020s: Scaling up Kernels Beyond 51 × 51 using Sparsity</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>network structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Install VirtualBox Guest Addition</title>
    <url>/2022/06/16/software/VirtualBox/Install%20VirtualBox%20Guest%20Addition/</url>
    <content><![CDATA[<ol>
<li><p>Before install guest addition from CD, do the following</p>
<p> <code>sudo apt-get install dkms build-essential linux-headers-generic linux-headers-$(uname -r)</code></p>
<p> For missing linux kernel headers or other common problems, refer to <a href="http://blog.ackx.net/virtualbox-guest-additions-common-errors.html" target="_blank" rel="noopener">this</a>.</p>
<p> use <code>uname -r</code> or <code>uname -a</code> to look up the kernel version, use <code>dpkg --get-selections | grep linux</code> to check the installed linux kernels.</p>
</li>
<li><p>If you click the sharefolder item in the menubar and get the follwing error: ‘The VirtualBox Guest Additions do not seem to be available on this virtual machine, and shared folders cannot be used without them’, the following commands may be helpful.  </p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install virtualbox-guest-additions-iso</span><br><span class="line">sudo apt-get update </span><br><span class="line">sudo apt-get dist-upgrade</span><br><span class="line">sudo apt-get install virtualbox-guest-x11</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>software</category>
      </categories>
      <tags>
        <tag>VirtualBox</tag>
      </tags>
  </entry>
  <entry>
    <title>Enlarge VirtualBox vdi</title>
    <url>/2022/06/16/software/VirtualBox/Enlarge%20VirtualBox%20vdi/</url>
    <content><![CDATA[<ol>
<li><p>Go to virtualbox installation directory and execute the following command:</p>
<p>  <code>D:\Program Files\Oracle\VirtualBox\VBoxManage.exe modifyhd &quot;F:\VirtualBox\my ubuntu.vdi&quot; --resize 15360</code></p>
<p> Note 15360 is the new size (M), this command can only enlarge the size.</p>
</li>
<li><p>Install gparted by <code>sudo apt-get install gparted</code> and make the extended disk space available to use.</p>
</li>
<li><p>Remount <code>/home</code> to the new disk. For concrete steps, refer to this <a href="https://help.ubuntu.com/community/Partitioning/Home/Moving" target="_blank" rel="noopener">link</a>. </p>
</li>
</ol>
]]></content>
      <categories>
        <category>software</category>
      </categories>
      <tags>
        <tag>VirtualBox</tag>
      </tags>
  </entry>
  <entry>
    <title>Git</title>
    <url>/2022/06/16/software/version_control/Git/</url>
    <content><![CDATA[<p><img src="https://www.ustcnewly.com/github_images/t0IXoZq.jpg" width="50%" height="50%"></p>
<ol>
<li><p><strong>Initialize center and local repository</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span>init from scratch</span><br><span class="line">git init &lt;local_dir&gt;</span><br><span class="line"><span class="meta">#</span>for central repository without working directory, use --bare</span><br><span class="line">git init --bare &lt;local_dir&gt;</span><br><span class="line"><span class="meta">#</span>clone from GitHub, can just clone a subdirectory. pull all the remote branches. set up "main" branch to track "origin/main".  </span><br><span class="line">git clone &lt;remote_repo&gt; &lt;remote_dir&gt;</span><br><span class="line"><span class="meta">#</span>pull from other local repository</span><br><span class="line">git remote add &lt;remote_repo&gt; &lt;remote_address&gt;</span><br><span class="line">   git pull # only pull the default branch</span><br><span class="line">git pull &lt;remote_repo&gt; &lt;remote_branch&gt; # pull is equal to fetch and merge</span><br><span class="line">   git fetch &lt;remote_repo&gt; &lt;remote_branch&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Configuration</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git config --global user.name $name</span><br><span class="line">git config --global user.email $email</span><br><span class="line">   git config --global credential.helper store</span><br><span class="line">git config --global alias.$alias-name $git-command</span><br><span class="line">git config --system core.editor $editor # e.g., vim</span><br><span class="line">git config --global --edit</span><br></pre></td></tr></table></figure>
<p><code>.git/config</code>  Repository-specific settings.<br><code>~/.gitconfig</code>  User-specific settings. This is where options set with the —global flag are stored.<br><code>/etc/gitconfig</code> System-wide settings.</p>
</li>
<li><p><strong>Staged snapshot in the staging area</strong><br>operations on tracked files:</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git add &lt;file&gt;</span><br><span class="line">git add &lt;dir&gt;</span><br><span class="line">git add . # add all the files in the current directory</span><br><span class="line">git add * # add  all the files and folders in the current directory </span><br><span class="line"><span class="meta">   #</span> before using `git add .` or `git add *`, we can exclude the untracked files in `.gitignore` (e.g., `*.&lt;ext&gt;`, `&lt;dir&gt;/`, use `!&lt;file&gt;` for exception).</span><br><span class="line"></span><br><span class="line">git rm &lt;file&gt; # delete the file</span><br><span class="line">   git rm --cached &lt;file&gt; # untrack the file</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">git mv &lt;old name&gt; &lt;new name&gt; # mv = rm+add</span><br><span class="line"></span><br><span class="line">git restore --staged &lt;file&gt; # move staged changes to unstaged changes</span><br><span class="line">   git restore &lt;file&gt; # remove unstaged changes</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>clear the entire staging area</span><br><span class="line">git reset HEAD # move all staged changes to unstaged changes</span><br><span class="line">   git reset --hard HEAD # remove all staged and unstaged changes</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p><strong>Commit the staged snapshot</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git commit -m "&lt;message&gt;"  #new commit node</span><br><span class="line">git commit --amend #old commit node</span><br><span class="line"><span class="meta">#</span>`-a` means staging all the changes of tracked files</span><br><span class="line">git commit -a -m "&lt;message&gt;" </span><br><span class="line">git commit -a --amend</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Branch</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git branch #list local branches		</span><br><span class="line">   git branch -r # list remote branches</span><br><span class="line">   git branch -a # list both remote branches and local branches</span><br><span class="line">   git branch -vv # list local branches and their tracked remote branches	</span><br><span class="line">git branch -d &lt;old_branch&gt; #use -D enforce delete</span><br><span class="line">git checkout &lt;old_branch&gt; #switch to branch $branch </span><br><span class="line">git branch &lt;new_branch&gt; #create a new branch </span><br><span class="line">git checkout -b &lt;new_branch&gt; #switch to a new branch $branch</span><br></pre></td></tr></table></figure>
<p> (a) Imaging there exists a hash table of <code>key:value</code> pairs with branch name as key and commit node as value. When checkout a new branch b1, <code>b1:current-commit-code</code> will be stored in the hash table. When checkout an existing branch b1, you will reach the hashed commit code. In both cases, b1 (HEAD) will be used as the hash key for the next commit node, that is, when you commit next time, <code>b1:new-commit-code</code> will be updated in the hash table.<br> (b) Another perspective is treating the branch name as a pointer to the commit node and each branch is a retrospective history dating back from that node, which is essentially the same as (a).</p>
<p> When checkout to another branch, git status (staged or unstaged modifications) will be transferred to that branch. If there exists file conflicts (operations on common yet different files), checkout will be forbidden. </p>
<p> Besides <code>branch</code>, users can also add <code>tag</code> to each commit node.</p>
<p> <img src="https://i.imgur.com/eZBhlc6.jpg" alt=""></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span>assume the current branch is master</span><br><span class="line"><span class="meta">#</span>merge: merge experiment into master</span><br><span class="line"><span class="meta">#</span>master moves one step forward, both experiment and original master are its parents</span><br><span class="line">git merge experiment</span><br><span class="line"><span class="meta">#</span>assume the current branch is experiment</span><br><span class="line"><span class="meta">#</span>rebase: base experiment on master</span><br><span class="line"><span class="meta">#</span>duplicate the nodes between C2 and experiment after master, move experiment to up-to-date</span><br><span class="line">git rebase master</span><br><span class="line"><span class="meta">#</span>in the simple fast-forward case, assume &lt;branch&gt; is ahead of &lt;branch1&gt;</span><br><span class="line"><span class="meta">#</span>then, `merge &lt;branch1&gt; &lt;branch2&gt;` and `rebase &lt;branch1&gt; &lt;branch2&gt;` work the same, just move &lt;branch1&gt; to &lt;branch2&gt;.</span><br><span class="line"><span class="meta">#</span>whereas, `merge &lt;branch2&gt; &lt;branch1&gt;` or `rebase &lt;branch2&gt; &lt;branch1&gt;` are not allowed.</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Tracking changes</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git status </span><br><span class="line">git log #commit history</span><br><span class="line">git log -n &lt;limit&gt; --stat</span><br><span class="line">   git log --all --graph --oneline # show the tree structure of all branches</span><br><span class="line">git log --grep="&lt;pattern&gt;"</span><br><span class="line">git log --author="John Smith" -p hello.py</span><br><span class="line">git diff #view unstaged modification</span><br><span class="line">git diff --cached #view staged modification</span><br><span class="line">git diff &lt;branch1&gt;..&lt;branch2&gt;</span><br><span class="line">git diff &lt;repo&gt;/&lt;branch1&gt;..&lt;branch2&gt;</span><br></pre></td></tr></table></figure>
<p>when using <code>git log</code>, it will only show the ancestral commit nodes of current node. </p>
</li>
<li><p><strong>Date back to history</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span>revert the operations in &lt;commit&gt; and add one commit node ahead of HEAD</span><br><span class="line"><span class="meta">#</span>revert does not work on file</span><br><span class="line">git revert &lt;commit&gt;</span><br><span class="line"><span class="meta">#</span>go back to &lt;commit&gt;</span><br><span class="line"><span class="meta">#</span>git checkout &lt;commit&gt; &lt;file&gt; is just copying the &lt;file&gt; in &lt;commit&gt;</span><br><span class="line">git checkout &lt;commit&gt;  		</span><br><span class="line"><span class="meta">#</span>git reset &lt;commit&gt; changes branch name, working directory, and staged index</span><br><span class="line"><span class="meta">#</span>git reset &lt;file&gt; is removing the staged file</span><br><span class="line">git reset --mixed &lt;commit&gt; #default, save the working directory</span><br><span class="line">git reset --soft &lt;commit&gt; #save both working directory and staged index</span><br><span class="line">git reset --hard &lt;commit&gt; #save none, dangerous!</span><br></pre></td></tr></table></figure>
<p>(a) For reset file, there are no options <code>--hard</code> and <code>--soft</code>.<br>(b) For reset commit, after <code>reset --mixed</code> or <code>reset --hard</code>, there are no staged snapshots. After <code>reset --soft</code>, staged snapshots enable the staged index to remain the same. More specifically, at the time before or after <code>reset --soft</code>, if you run <code>git commit</code>, the generated commit node should be the same. </p>
</li>
</ol>
<ol>
<li><p><strong>Communicate with remote repositories</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git remote -v #list remote repositories</span><br><span class="line">git remote add &lt;remote_repo&gt; &lt;remote_address&gt;</span><br><span class="line">git branch -r #list the fetched branches of remote repositories</span><br><span class="line"></span><br><span class="line">git fetch &lt;repo&gt;  # fetch all of the remote branches</span><br><span class="line">git fetch &lt;repo&gt; &lt;remote_branch&gt;:&lt;local_branch&gt;</span><br><span class="line">git checkout -b &lt;new_branch&gt; &lt;repo&gt;/&lt;remote_branch&gt; # Checking out a local copy of specific branch</span><br><span class="line">git pull &lt;repo&gt; &lt;remote_branch&gt;:&lt;local_branch&gt;</span><br><span class="line">git push &lt;origin&gt; &lt;local_branch&gt;:&lt;remote_branch&gt;</span><br></pre></td></tr></table></figure>
<p>after merging code to address the conflict, use <code>git commit -a -m &quot;$message&quot;</code>, then <code>git push</code>.</p>
<p> <code>git pull</code> is equal to a <code>git fetch</code> followed by a <code>git merge</code>.</p>
</li>
<li><p><strong>Clean working directory</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clean -f/d #clean untracked files/directory</span><br><span class="line">git clean -f $path #clean untracked files in $path</span><br><span class="line"><span class="meta">#</span>`git clean` is often used togther with `git reset —hard`</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Stashing</strong></p>
<p>If you are working in your repository, and your workflow is interrupted by another project, you can save the current state of your repository (the working directory and the staging index) in the Git stash. When you are ready to start working where you left off, you can restore the stashed repository state. Note that doing a stash gives you a clean working directory, and leaves you at the last commit. </p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git stash save # stash the current state</span><br><span class="line">git stash list # see what is in the Git stash:</span><br><span class="line">git stash apply # return your stashed state to the working directory</span><br><span class="line">git stash pop # return your stashed state to the working directory and delete it from the stash</span><br></pre></td></tr></table></figure>
<p>Note the commit-tree grows. “git log” prints the path from root to current node. “leave behind” prints the path from current node to the first common ancestor between current node and new node. Detached head will be off any branch, so create a new branch for the detached head. </p>
</li>
</ol>
<p><strong>Tips:</strong></p>
<ol>
<li><code>checkout</code> and <code>reset</code> work on both file and commit with different meanings and usages, while <code>revert</code> only works on commit.</li>
<li><code>reset</code> is often used for cancelling uncommited changes while <code>revert</code> is often used for cancelling commited changes.</li>
<li>Compared with <code>reset</code>, <code>checkout</code> and <code>revert</code> focus on the modification, so they are forbidden in many cases.</li>
</ol>
<p><strong>Github User:</strong></p>
<ol>
<li>create a repository on GitHub</li>
<li>create a local folder</li>
<li><code>git init</code></li>
<li><code>git remote add &lt;remote_repo&gt; &lt;remote_address&gt;</code></li>
<li><code>git pull &lt;remote_repo&gt; &lt;remote_branch&gt;</code></li>
<li><code>git push --set-upstream &lt;remote_repo&gt; &lt;remote_branch&gt;</code></li>
<li>add files</li>
<li><code>git add *</code></li>
<li><code>git commit -m &quot;msg&quot;</code></li>
<li><code>git push</code> </li>
</ol>
<p><strong>Resources:</strong><br>A good Chinese tutorial for git is <a href="http://git.oschina.net/progit/index.html" target="_blank" rel="noopener">here</a>.<br>A good English tutorial for git is <a href="https://www.atlassian.com/git/tutorials/what-is-version-control" target="_blank" rel="noopener">here</a>.</p>
]]></content>
      <categories>
        <category>software</category>
        <category>version control</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu JDK</title>
    <url>/2022/06/16/software/library/Ubuntu%20JDK/</url>
    <content><![CDATA[<ol>
<li><p>Remove default OpenJDK</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get purge openjdk-\*</span><br></pre></td></tr></table></figure>
</li>
<li><p>download jdk(containing jre) and tar.</p>
</li>
<li><p>add into /etc/profile</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JAVA_HOME=&quot;/home/newly/Program_Files/Java/jdk1.8.0_65&quot;</span><br><span class="line">PATH=&quot;$PATH:$JAVA_HOME/jre/bin:$JAVA_HOME/bin&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>set newly installed jdk as default jdk.</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --install "/usr/bin/java" "java" "$JAVA_HOME/bin/java" 1</span><br><span class="line">sudo update-alternatives --install "/usr/bin/javac" "javac" "$JAVA_HOME/bin/javac" 1</span><br><span class="line">sudo update-alternatives --install "/usr/bin/javaws" "javaws" "$JAVA_HOME/bin/javaws" 1</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>software</category>
        <category>library</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>JDK</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenCV</title>
    <url>/2022/06/16/software/library/OpenCV/</url>
    <content><![CDATA[<h3 id="Locate-OpenCV"><a href="#Locate-OpenCV" class="headerlink" title="Locate OpenCV:"></a>Locate OpenCV:</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pkg-config --modversion opencv //version</span><br><span class="line">pkg-config --cflags opencv //include file path</span><br><span class="line">pkg-config --libs opencv //library path</span><br></pre></td></tr></table></figure>
<h3 id="Uninstall-OpenCV"><a href="#Uninstall-OpenCV" class="headerlink" title="Uninstall OpenCV:"></a>Uninstall OpenCV:</h3><ol>
<li>go to the compile package and <code>make uninstall</code></li>
<li>if cannot find the compile package, <code>sudo find / -name &quot;*opencv*&quot; -exec rm -i {} \;</code></li>
</ol>
<h3 id="Install-OpenCV"><a href="#Install-OpenCV" class="headerlink" title="Install OpenCV:"></a>Install OpenCV:</h3><p><strong>For Windows: C++(Visual Studio)</strong></p>
<ol>
<li>Download latest OpenCV from <a href="http://sourceforge.net/projects/opencvlibrary/?source=typ_redirect" target="_blank" rel="noopener">http://sourceforge.net/projects/opencvlibrary/?source=typ_redirect</a>.</li>
<li>Make a dir named build. Use cmake to generate OpenCV.sln in build.</li>
<li>Click OpenCV.sln and CMakeTargets-&gt;INSTALL-&gt;build. You can build it in both debug and release modes.</li>
<li>In the environmental variables, add D:\Program Files\OpenCV_3.0.0\build\install\x64\vc11 as OPENCV_DIR, add Path with %OPENCV_DIR%\bin. </li>
<li>Create a OpenCV project. C/C++-&gt;General-&gt;Additional Include Directories: $(OPENCV_DIR)\..\..\include</li>
<li>Linker-&gt;General-&gt;Additional Library Directories: $(OPENCV_DIR)\lib</li>
<li>Linker-&gt;Input-&gt;Additional Dependencies: copy all the lib files. <em>*</em>d.lib means debug mode.</li>
</ol>
<p>Note that there may be some problems for visual studio to recognize the updated environmental variable. Try to restart the visual studio or recreate the project.</p>
<p><strong>For Linux: install for C++</strong></p>
<ol>
<li>pre-install packages<br>[compiler] sudo apt-get install build-essential<br>[required] sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev<br>[optional] sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev</li>
<li>download the required version on <a href="https://sourceforge.net/projects/opencvlibrary/" target="_blank" rel="noopener">https://sourceforge.net/projects/opencvlibrary/</a></li>
<li>make and install <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake ..     //cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local ..</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>Tips: if you want to install multiple versions of OpenCV, set CMAKE_INSTALL_PREFIX differently for different versions of OpenCV.</p>
<pre><code class="lang-shell">cmake -D CMAKE_BUILD_TYPE=RELEASE \
    -D CMAKE_INSTALL_PREFIX=/usr/local \
    -D INSTALL_PYTHON_EXAMPLES=ON \
    -D INSTALL_C_EXAMPLES=ON \
    -D OPENCV_EXTRA_MODULES_PATH=../opencv_contrib/modules \
    -D PYTHON_EXECUTABLE=/usr/bin/python \
    -D BUILD_EXAMPLES=ON ..
</code></pre>
<p>If want to turn off some modules, use <code>-D BUILD_opencv_xfeatures2d=OFF</code> </p>
<p><strong>Switch between different versions of OpenCV:</strong></p>
<p>switch from version 3.2 to version 2.4 <a href="\code\OpenCV\switch_to_2.4.13.sh">shell</a></p>
<p>switch from version 2.4 to version 3.2 <a href="\code\OpenCV\switch_to_3.2.0.sh">shell</a></p>
<p><strong>For Linux: Python(Anaconda)</strong></p>
<pre><code class="lang-shell">source activate tgt_env
conda install -c https://conda.binstar.org/menpo opencv
spyder
</code></pre>
<p>Then try <code>import cv2</code></p>
]]></content>
      <categories>
        <category>software</category>
        <category>library</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
      </tags>
  </entry>
  <entry>
    <title>GStreamer+FFmpeg for Ubuntu14.04</title>
    <url>/2022/06/16/software/library/GStreamer+FFmpeg%20for%20Ubuntu14.04/</url>
    <content><![CDATA[<ul>
<li><p>Install gst-plugin</p>
  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:ddalex/gstreamer</span><br><span class="line">sudo apt-get install gstreamer0.10-*</span><br></pre></td></tr></table></figure>
</li>
<li><p>Install gstffmpeg</p>
  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:mc3man/gstffmpeg-keep</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install gstreamer0.10-ffmpeg</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>software</category>
        <category>library</category>
      </categories>
      <tags>
        <tag>GStreamer</tag>
        <tag>FFmpeg</tag>
      </tags>
  </entry>
  <entry>
    <title>Remote Debugging for Visual Code</title>
    <url>/2022/06/16/software/IDE/Remote%20Debugging%20for%20Visual%20Code/</url>
    <content><![CDATA[<p>Remote-SSH: Open SSH Configuration File  ~/.ssh/config</p>
<p>Host jump-box<br>    HostName <ip><br>    User <username><br>    Port <portnumber></portnumber></username></ip></p>
<p>Host target-box<br>    HostName <innerip><br>    User <username><br>    Port 22<br>    ProxyCommand ssh -q -W %h:%p jump-box</username></innerip></p>
<p>Remote Explorer</p>
]]></content>
      <categories>
        <category>software</category>
        <category>IDE</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Install JDK and Eclipse</title>
    <url>/2022/06/16/software/IDE/Install%20JDK%20and%20Eclipse/</url>
    <content><![CDATA[<h3 id="For-Windows"><a href="#For-Windows" class="headerlink" title="For Windows"></a>For Windows</h3><ol>
<li><p>Download Java JDK from <a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/index.html</a> and install. Note that JDK contains JRE. Modify the following environment variables:</p>
<ul>
<li>add JAVAHOME D:\Program Files (x86)\Java\jdk1.7.051</li>
<li>add Path %JAVA_HOME%\bin;\%JAVA_HOME%\jre\bin;</li>
<li>add CLASSPATH %JAVA_HOME%\lib\tools.jar;%JAVA_HOME%\jre\lib\rt.jar</li>
</ul>
</li>
<li><p>Download Eclipse from <a href="http://www.eclipse.org/downloads/" target="_blank" rel="noopener">http://www.eclipse.org/downloads/</a> and click eclipse.exe</p>
</li>
</ol>
<h3 id="For-Linux"><a href="#For-Linux" class="headerlink" title="For Linux"></a>For Linux</h3><ol>
<li><p>Install JDK</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:webupd8team/java</span><br><span class="line">sudo apt-get update // depending on the needed java version</span><br><span class="line">sudo apt-get install openjdk-7-jdk</span><br><span class="line">or sudo apt-get install openjdk-8-jre openjdk-8-jdk</span><br></pre></td></tr></table></figure>
</li>
<li><p>Download Eclipse and unpack</p>
</li>
<li><p>Create a new file eclipse.desktop in /usr/share/applications/ and add the below code </p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">   [Desktop Entry]</span><br><span class="line">   Name=Eclipse</span><br><span class="line">Comment=Eclipse IDE</span><br><span class="line">Exec=/home/liniu/Program_Files/Eclipse/eclipse/eclipse</span><br><span class="line">Icon=/home/ivan/Eclipse/icon.xpm</span><br><span class="line">Categories=Application;Development;Java;IDE</span><br><span class="line">Type=Application</span><br><span class="line">Terminal=0</span><br></pre></td></tr></table></figure>
<p>and then run</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo desktop-file-install /usr/share/applications/eclipse.desktop</span><br></pre></td></tr></table></figure>
</li>
<li><p>Add to the path:  <code>sudo ln -s /opt/eclipse/eclipse /usr/local/bin/</code></p>
</li>
</ol>
]]></content>
      <categories>
        <category>software</category>
        <category>IDE</category>
      </categories>
      <tags>
        <tag>Eclipse</tag>
        <tag>Java</tag>
        <tag>JDK</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu MATLAB</title>
    <url>/2022/06/16/software/Ubuntu%20MATLAB/</url>
    <content><![CDATA[<ol>
<li>install by running the sh file</li>
<li>refer to the crack folder</li>
<li>After installation, create a shortcut<br>a) sudo ln -s /usr/local/MATLAB/R2012a/bin/matlab  /usr/bin/matlab<br>b) copy the matlab.desktop to /usr/share/applications and matlab.png to /usr/share/icons.</li>
</ol>
<p>Problem:</p>
<ol>
<li><p>/tmp/mathworks……. permission denied</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span>cd /Matlab.R2012a.UNIX/sys/java/jre/glnxa64/jre/bin</span><br><span class="line">chmod +x ./java</span><br></pre></td></tr></table></figure>
</li>
<li><p>warning: usr/local/MATLAB/R2012a/bin/util/oscheck.sh: /lib/libc.so.6: not found</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo ln -s /lib/x86_64-linux-gnu/libc.so.6 /lib64/libc.so.6</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>software</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>MATLAB</tag>
      </tags>
  </entry>
  <entry>
    <title>Medical Image Format</title>
    <url>/2022/06/16/software/Medical%20Image%20Format/</url>
    <content><![CDATA[<h3 id="Format"><a href="#Format" class="headerlink" title="Format"></a>Format</h3><ol>
<li><strong>DICOM:</strong> (Digital Imaging and COmmunications in Medicine) meta info + raw data</li>
<li><strong>nrrd:</strong> meta info (e.g., sizes, dimension, encoding, endian) + raw data</li>
</ol>
<p>The meta info in <code>nrrd</code> mainly consists of the format of raw data while the meta info in <code>DICOM</code> contains the detailed information of patient, study, and series.<br>There exist matlab and python functions to read DICOM and nrdd files.</p>
<ul>
<li>MATLAB<ul>
<li>DICOM: dicominfo, dicomread</li>
<li>nrrd: <a href="\code\medical_image\nrrdread.m">nrrdread</a></li>
<li>conversion from DICOM to nrrd: <a href="\code\medical_image\Dicom2nrrd.m">Dicom2nrrd</a></li>
</ul>
</li>
</ul>
<p>DICOM is generally a directory containing a sequence of slices while nrrd is a single file. The meta info of DICOM is more complicated and thus more error-prone than that of nrrd. Use <a href="https://github.com/Slicer/Slicer/tree/master/Modules/Scripted/DICOMPatcher" target="_blank" rel="noopener">DICOMPatcher</a> to fix up DICOM files if errors occur when loading them to 3D Slicer.</p>
<h3 id="Software"><a href="#Software" class="headerlink" title="Software"></a>Software</h3><ol>
<li><p><strong>3D Slicer</strong>: cross-platform, compatible with different formats (e.g., DICOM and nrrd), and convert between them.</p>
</li>
<li><p>Other softwares to view medical images: <a href="http://www.cabiatl.com/mricro/dicom/#links" target="_blank" rel="noopener">http://www.cabiatl.com/mricro/dicom/#links</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>software</category>
      </categories>
      <tags>
        <tag>medical</tag>
      </tags>
  </entry>
  <entry>
    <title>MATLAB</title>
    <url>/2022/06/16/software/MATLAB/</url>
    <content><![CDATA[<ol>
<li><p>run MATLAB using shellscript</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">matlab -nodisplay -r "vid2frames('../data/regular_videos/', 'RiceCam106.MP4', '../data/frames/', '.png');exit"</span><br></pre></td></tr></table></figure>
</li>
<li><p>VideoReader: NumberOfFrames is 0, cannot read frames.</p>
<p> something is wrong with gstreamer, first get the right version of gstreamer (0.10 for MATLAB R2015b)    </p>
<ul>
<li><p>install gst-plugins</p>
  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:ddalex/gstreamer</span><br><span class="line">sudo apt-get install gstreamer0.10-*</span><br></pre></td></tr></table></figure>
</li>
<li><p>install gstffmpeg</p>
  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:mc3man/gstffmpeg-keep</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install gstreamer0.10-ffmpeg</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>software</category>
      </categories>
      <tags>
        <tag>MATLAB</tag>
      </tags>
  </entry>
  <entry>
    <title>MarkdownPad</title>
    <url>/2022/06/16/software/MarkdownPad/</url>
    <content><![CDATA[<ol>
<li><p><strong>Problem</strong>: HTML cannot be rendered: the view has crashed awesomium: Awesomium.Windows.Controls.WebControl<br><strong>solution</strong>: Win+R to open regedit: change the following value from 1 to 0<br> <code>HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Lsa\FipsAlgorithmPolicy\Enabled</code></p>
</li>
<li><p><strong>Problem</strong>: How to suppport Latex: MathJax?<br><strong>solution</strong>: tools &gt; Options &gt; Advanced &gt; HTML Head Editor, add the following:</p>
 <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;script type=<span class="string">"text/javascript"</span></span><br><span class="line">   src=<span class="string">"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"</span>&gt;</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure>
<p>After adding the above, a simple example in the MarkDownPad: When $a \ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are:</p>
<script type="math/tex; mode=display">x = {-b \pm \sqrt{b^2-4ac} \over 2a}</script></li>
</ol>
<h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips:"></a>Tips:</h2><ol>
<li>insert image: <code>&lt;center&gt;&lt;img src=&quot;http://....jpg&quot; width=50% border=&quot;0&quot;&gt;&lt;/center&gt;</code></li>
</ol>
]]></content>
      <categories>
        <category>software</category>
      </categories>
      <tags>
        <tag>MarkdownPad</tag>
      </tags>
  </entry>
  <entry>
    <title>LaTeX</title>
    <url>/2022/06/16/software/LaTeX/</url>
    <content><![CDATA[<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation:"></a>Installation:</h2><ul>
<li><p>Ubuntu</p>
  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install texlive-full	</span><br><span class="line">sudo apt-get install texmaker</span><br></pre></td></tr></table></figure>
</li>
<li><p>Windows</p>
<ol>
<li>Install Miktex: <a href="https://miktex.org/download" target="_blank" rel="noopener">https://miktex.org/download</a></li>
<li>Install texmaker or texstudio</li>
</ol>
</li>
</ul>
<h2 id="Extension"><a href="#Extension" class="headerlink" title="Extension:"></a>Extension:</h2><ol>
<li>Latexdiff<ul>
<li>Install perl: <a href="http://www.perl.org/get.html" target="_blank" rel="noopener">http://www.perl.org/get.html</a> </li>
<li>Download latexdiff.zip package from <a href="https://ctan.org/pkg/latexdiff" target="_blank" rel="noopener">https://ctan.org/pkg/latexdiff</a></li>
<li>Unzip the latexdiff files and copy them to the Perl\perl\bin folder</li>
<li><code>latexdiff draft.tex revision.tex &gt; diff.tex</code></li>
</ul>
</li>
</ol>
<h2 id="Math-input"><a href="#Math-input" class="headerlink" title="Math input:"></a>Math input:</h2><ol>
<li><p>Include the <a href="\code\LaTeX\definition.py">definition.tex</a>  file with macro definition. <code>\input{definition}</code> in the main text.</p>
</li>
<li><p><a href="https://mathpix.com/" target="_blank" rel="noopener">Mathpix Snip</a> can convert images to LaTeX.</p>
</li>
<li><p><a href="http://www.jonathanleroux.org/software/iguanatex/" target="_blank" rel="noopener">IguanaTex</a> allows you to insert LaTeX formulations in PowerPoint.</p>
</li>
</ol>
<h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips:"></a>Tips:</h2><ol>
<li><p>Refer to <a href="http://bcmi.sjtu.edu.cn/~niuli/download/latex_symbols.pdf" target="_blank" rel="noopener">this</a> for commonly used symbols</p>
</li>
<li><p>Set color: <code>\usepackage{xcolor}</code> and <code>\textcolor{red}{}</code></p>
</li>
<li><p>Setlength: set the indentation in paragraphs <code>\setlength\parindent{0pt}</code>; set the separation between paragraphs <code>\setlength{\parskip}{10pt}</code>; set column width in table <code>\setlength{\tabcolsep}{12pt}</code>; set the space below table/figure <code>\setlength{\textfloatsep}{2pt}</code></p>
</li>
<li><p>Math annotation: expectation <code>\usepackage{amsfonts}</code> and <code>\mathbb{E}</code>; not imply: <code>\usepackage{amssymb}</code> and <code>\nRightarrow</code></p>
</li>
<li><p>Encoding error: try adding <code>\UseRawInputEncoding</code> or <code>\usepackage[utf8x]{inputenc}</code> or <code>\usepackage{newunicodechar}</code> <code>\newunicodechar{ﬁ}{fi}</code> </p>
</li>
</ol>
]]></content>
      <categories>
        <category>software</category>
      </categories>
      <tags>
        <tag>LaTeX</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow Learning Note</title>
    <url>/2022/06/16/programming_language/TensorFlow/TensorFlow%20Learning%20Note/</url>
    <content><![CDATA[<h3 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h3><hr>
<p>shape = [batch,height,width,channel]<br><strong>constant:</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.zeros(shape)</span><br><span class="line">tf.constant(<span class="number">0.4</span>, shape)</span><br><span class="line">tf.random_norm(shape, stddev=<span class="number">1e-1</span>)</span><br><span class="line">tf.truncated_norm(shape, stddev=<span class="number">1e-1</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>variable:</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.get_variable(varname, shape=[<span class="number">5</span>,<span class="number">5</span>,<span class="number">64</span>,<span class="number">192</span>], initializer=XXX)</span><br><span class="line">initializer=tf.zeros_initializer() <span class="comment"># bias</span></span><br><span class="line">initializer=tf.random_normal_initializer(<span class="number">0.0</span>, <span class="number">0.02</span>) </span><br><span class="line">initializer=tf.truncated_normal_initializer(<span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">initializer=tf.contrib.layers.xavier_initializer() <span class="comment"># weight</span></span><br></pre></td></tr></table></figure></p>
<p>Empirically, for weight initialization, xavier is better than truncated_norm, better than random_normal.</p>
<h3 id="Summary-for-tensorboard"><a href="#Summary-for-tensorboard" class="headerlink" title="Summary for tensorboard"></a>Summary for tensorboard</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.summary.scalar(<span class="string">'total_loss'</span>, total_loss) <span class="comment">#scalar, histogram, etc</span></span><br><span class="line">self.merged = tf.summary.merge_all()</span><br><span class="line">train_writer = tf.summary.FileWriter(summary_folder, model.sess.graph) <span class="comment">#include the sess graph</span></span><br><span class="line">train_writer.add_summary(summary, ibatch)</span><br></pre></td></tr></table></figure>
<p>after running the code, open tensorboard <code>tensorboard --logdir=&#39;./train&#39;</code></p>
<h3 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a>Checkpoint</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># save all the variables</span></span><br><span class="line">saver = tf.train.Saver() </span><br><span class="line"><span class="comment"># save partial variables </span></span><br><span class="line">saver = tf.train.Saver(&#123;<span class="string">"my_v2"</span>: v2&#125;)</span><br><span class="line"><span class="comment"># save and restore</span></span><br><span class="line">saver.save(sess, <span class="string">"/tmp/model.ckpt"</span>)</span><br><span class="line">saver.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)</span><br><span class="line"><span class="comment"># restore partial variables</span></span><br><span class="line">init_fn = slim.assign_from_checkpoint_fn(checkpoint_path, slim.get_variables_to_restore())</span><br><span class="line">init_fn(sess)</span><br></pre></td></tr></table></figure>
<p>inspect the checkpoint file<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python <span class="keyword">import</span> pywrap_tensorflow  </span><br><span class="line">checkpoint_path = os.path.join(model_dir, <span class="string">"model.ckpt"</span>)  </span><br><span class="line">reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)  </span><br><span class="line">var_to_shape_map = reader.get_variable_to_shape_map()  </span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> var_to_shape_map:  </span><br><span class="line">    print(<span class="string">"tensor_name: "</span>, key)  </span><br><span class="line">    print(reader.get_tensor(key)) <span class="comment"># Remove this is you want to print only variable names</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Layer"><a href="#Layer" class="headerlink" title="Layer"></a>Layer</h3><hr>
<ul>
<li><p>convolutional layer</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">kernel = tf.Variable(tf.zeros([<span class="number">5</span>,<span class="number">5</span>,<span class="number">64</span>,<span class="number">192</span>]))</span><br><span class="line">tf.nn.conv2d(x, kernel, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>) <span class="comment"># padding is 'SAME' or 'VALID' depending on stride is 1 or larger</span></span><br><span class="line"></span><br><span class="line">tf.nn.bias_add(prev_layer, bias)</span><br></pre></td></tr></table></figure>
</li>
<li><p>dropout layer</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"><span class="comment"># set keep_prob as 0.5 during training and 1 during testing</span></span><br><span class="line">tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure>
</li>
<li><p>pooling layer</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],  strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>loss layer</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))</span><br><span class="line">	tf.nn.l2_loss(prev_layer)</span><br></pre></td></tr></table></figure>
</li>
<li><p>metric layer</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># calculate top-k accuracy</span></span><br><span class="line">tf.nn.in_top_k(logits, label_holder, k) <span class="comment"># the accuracy of top k</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>activation layer<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.nn.relu(prev_layer)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>other layers<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.one_hot(self.input_class, self.n_class)</span><br><span class="line">tf.nn.embedding_lookup(codebook, indices)</span><br><span class="line">tf.nn.lrn() <span class="comment"># legacy</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Common-Operations"><a href="#Common-Operations" class="headerlink" title="Common Operations"></a>Common Operations</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.matmul</span><br><span class="line">tf.pow</span><br><span class="line">tf.diag_part</span><br><span class="line">tf.reduce_sum</span><br><span class="line">tf.reduce_mean</span><br><span class="line">tf.nn.softmax</span><br><span class="line">tf.tile(a,(<span class="number">2</span>,<span class="number">2</span>)) </span><br><span class="line">tf.nn.embedding_lookup</span><br><span class="line">sim.flatten</span><br><span class="line">tf.squeeze</span><br><span class="line">tf.reshape</span><br><span class="line">tf.concat(values=[a,b,c], axis=<span class="number">3</span>)</span><br><span class="line">tf.gather  <span class="comment">#get indexed value, only on 0-dim</span></span><br><span class="line">tf.gather_nd</span><br><span class="line">tf.map_fn</span><br><span class="line">tf.scan_fn</span><br><span class="line">tf.fold1/foldr</span><br></pre></td></tr></table></figure>
<h3 id="Training-and-testing"><a href="#Training-and-testing" class="headerlink" title="Training and testing"></a>Training and testing</h3><hr>
<ul>
<li><p>training stage</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>): <span class="comment">#note the iteration number here</span></span><br><span class="line">    sess.run(train, feed_dict=&#123;x:x_train, y:y_train&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>testing stage</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">acc.eval(feed_dict=&#123;x:x_test, y:y_test&#125;)</span><br><span class="line">sess.run(acc, feed_dict=&#123;x:x_test, y:y_test&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h3><hr>
<ul>
<li><p>For plain TensorFlow, reading data via queue is cumbersome. Refer to my another blog ‘TensorFlow Input Data’ for more details.<br>  ``python<br>  with tf.Session(config=config) as sess:      </p>
<pre><code>  coord = tf.train.Coordinator()
  threads = tf.train.start_queue_runners(sess=sess, coord=coord)
  sess.run(tf.initialize_local_variables())
  #main code
  coord.request_stop()
  coord.join(threads)
</code></pre>  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* With tfrecord and slim, using queue is much easier by using 	`slim.queues.QueueRunners`</span><br><span class="line">	```python</span><br><span class="line">	with tf.Session(config=config) as sess:        </span><br><span class="line">		with slim.queues.QueueRunners(sess):</span><br><span class="line">	``` </span><br><span class="line">  or `sv.managed_session`</span><br><span class="line">	```python</span><br><span class="line">	sv = tf.train.Supervisor(logdir=logdir, save_summaries_secs=0, saver=None)        </span><br><span class="line">	    with sv.managed_session() as sess:</span><br></pre></td></tr></table></figure>
<p>Sample code of generating tfrecord dataset and reading data via queue can downloaded <a href="\code\TensorFlow\generate_tfrecord.rar">here</a></p>
</li>
</ul>
<h3 id="Utils"><a href="#Utils" class="headerlink" title="Utils"></a>Utils</h3><hr>
<ul>
<li><p>check tensorflow version</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.__version__.split(<span class="string">'.'</span>)[<span class="number">0</span>] != <span class="string">"1"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>count parameter numbers</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parameter_count = tf.reduce_sum([tf.reduce_prod(tf.shape(v)) <span class="keyword">for</span> v <span class="keyword">in</span> tf.trainable_variables()])</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>programming language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow Input Data with Queue</title>
    <url>/2022/06/16/programming_language/TensorFlow/TensorFlow%20Input%20Data%20with%20Queue/</url>
    <content><![CDATA[<ol>
<li><p>Input raw data (e.g., text, image address). There are many different implementations of queue.</p>
<ul>
<li><p>feature-label pair: <a href="\code\TensorFlow\queue\feature_label_pair\customize_enqueue_op.py">[enqueue-op]</a> <a href="\code\TensorFlow\queue\feature_label_pair\customize_prefetch_func.py">[fetch-func]</a> <a href="\code\TensorFlow\queue\feature_label_pair\slice_input_producer.py">[slice-input-producer]</a> <a href="\code\TensorFlow\queue\feature_label_pair\string_input_producer.py">[string-input-producer]</a></p>
</li>
<li><p>image-image pair: <a href="\code\TensorFlow\queue\image_image_pair\slice_input_producer.py">[slice-input-producer]</a><a href="\code\TensorFlow\queue\image_image_pair\string_input_producer.py">[string-input-producer]</a></p>
</li>
</ul>
</li>
<li><p>A more elegant way is converting raw data to tfrecord format.</p>
</li>
</ol>
<p><strong>Tips:</strong></p>
<ol>
<li><p>setting large number_of_threading (e.g., 10) is helpful.</p>
</li>
<li><p>shuffle the training samples to avoid homogenuity when necessary.</p>
</li>
<li><p>place the training data in local disk instead of removable disk (consider I/O speed).</p>
</li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow GPU Usage</title>
    <url>/2022/06/16/programming_language/TensorFlow/TensorFlow%20GPU%20Usage/</url>
    <content><![CDATA[<p>All the GPU memory will be notoriously filled up even if you designate one GPU device.</p>
<h3 id="maximum-fraction"><a href="#maximum-fraction" class="headerlink" title="maximum fraction"></a>maximum fraction</h3><p>gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)<br>sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))</p>
<h3 id="automatic-growth"><a href="#automatic-growth" class="headerlink" title="automatic growth"></a>automatic growth</h3><p>config = tf.ConfigProto()<br>config.gpu_options.allow_growth=True<br>sess = tf.Session(config=config) </p>
<h3 id="visible-GPU"><a href="#visible-GPU" class="headerlink" title="visible GPU:"></a>visible GPU:</h3><p> os.environ[“CUDA_VISIBLE_DEVICES”] = “0,1” # use python to set environment variables</p>
<h3 id="use-multiple-GPUs"><a href="#use-multiple-GPUs" class="headerlink" title="use multiple GPUs"></a>use multiple GPUs</h3><p>One typical to use mulitple GPU is to average gradients, please refer to the <a href="\code\TensorFlow\multiGPU\multigpu_cnn.py">sample code</a>.</p>
]]></content>
      <categories>
        <category>programming language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>Slim Learning Note</title>
    <url>/2022/06/16/programming_language/TensorFlow/Slim%20Learning%20Note/</url>
    <content><![CDATA[<p>slim = tf.contrib.slim</p>
<h3 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h3><p>for certain functions, assign default values to certain parameters</p>
<pre><code>with slim.arg_scope([func1, func2, ....], arg1=val1, arg2=val2, ....)
</code></pre><p><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/contrib/layers</a></p>
<ul>
<li>slim.conv2d </li>
<li>slim.max_pool2d</li>
<li>slim.avg_pool2d</li>
<li>slim.dropout</li>
<li>slim.batch_norm</li>
<li>slim.softmax</li>
<li>tf.repeat(inputs, repetitions, layer, <em>args, *</em>kwargs)</li>
</ul>
<p>class Block(collections.namedtuple(‘Block’, [‘scope’, ‘unit_fn’, ‘args’])):<br> net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)</p>
<h3 id="Load-pretrained-model"><a href="#Load-pretrained-model" class="headerlink" title="Load pretrained model"></a>Load pretrained model</h3><p>Note that initialization function must be associated with sess.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">init_fn = slim.assign_from_checkpoint_fn(checkpoint_path, slim.get_variables_to_restore())</span><br><span class="line">init_fn(sess)</span><br></pre></td></tr></table></figure></p>
<h3 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h3><p><strong>With slim</strong>:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_norm_params = &#123;</span><br><span class="line">	    <span class="comment"># Decay for the moving averages.</span></span><br><span class="line">	    <span class="string">'decay'</span>: batch_norm_decay,</span><br><span class="line">	    <span class="comment"># epsilon to prevent 0s in variance.</span></span><br><span class="line">	    <span class="string">'epsilon'</span>: batch_norm_epsilon,</span><br><span class="line">	    <span class="comment"># collection containing update_ops.</span></span><br><span class="line">	    <span class="string">'updates_collections'</span>: tf.GraphKeys.UPDATE_OPS,</span><br><span class="line"> &#125;</span><br><span class="line">slim.arg_scope([slim.conv2d],  normalizer_fn=slim.batch_norm, normalizer_params=normalizer_params)</span><br></pre></td></tr></table></figure></p>
<p>For tf.contrib.layers or tf.slim, when is_training=True, mean and variance based on each batch are used and moving_mean and moving_variance are updated if applicable. When is_training=False, loaded  moving-mean an moving_variance are used.</p>
<p>To launch the update of moving_mean and moving_variance, special attention needs to be paid because this update operation is detached from gradient descent, which can be realized in the following ways.</p>
<p>The first method:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span><br><span class="line">variables_to_train = _get_variables_to_train()</span><br><span class="line">grads_and_vars = optimizer.compute_gradients(total_loss,  variables_to_train)</span><br><span class="line">grad_updates = optimizer.apply_gradients(grads_and_vars)</span><br><span class="line">update_ops.append(grad_updates)</span><br><span class="line">update_op = tf.group(*update_ops)</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([update_op]):</span><br><span class="line">    train_op = tf.identity(total_loss)</span><br></pre></td></tr></table></figure></p>
<p>The second method:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies(update_ops):</span><br><span class="line">	train_op = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure></p>
<p>The third method:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_op = slim.learning.create_train_op(total_loss, optimizer)</span><br></pre></td></tr></table></figure></p>
<p>Otherwise, one can set updates_collections=None in slim.batch_norm to force the updates in place, but that can have a speed penalty, especially in distributed settings.</p>
<p>However, when trained on small-scale  datasets, using moving_mean and moving_variance in the test stage often leads to extremely poor performance (close to random guess). This is due to the code start which renders moving_mean/variance unstable. There are two ways to fix the cold-start issue:</p>
<ul>
<li><p>in the testing stage, also set is_training=True, i.e., use the mean and variance based on each test batch.</p>
</li>
<li><p>decrease batch_norm running average decay from default 0.999 to something like 0.99, which can speed up the start-up. When tuning decay, there is a trade-off between warm-up speed and statistical accuracy. For small-scale datasets, warm-up may take exceedingly long time, e.g., 300 epochs.</p>
</li>
</ul>
<p><strong>without slim</strong>: tf.nn.batch_normalization, no moving_mean/variance<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm</span><span class="params">(bn_input)</span>:</span></span><br><span class="line">	<span class="keyword">with</span> tf.variable_scope(<span class="string">"batchnorm"</span>):</span><br><span class="line">		<span class="comment"># this block looks like it has 3 inputs on the graph unless we do this</span></span><br><span class="line">		bn_input = tf.identity(bn_input)</span><br><span class="line">		</span><br><span class="line">		channels = bn_input.get_shape()[<span class="number">3</span>]</span><br><span class="line">		offset = tf.get_variable(<span class="string">"offset"</span>, [channels], dtype=tf.float32, initializer=tf.zeros_initializer())</span><br><span class="line">		scale = tf.get_variable(<span class="string">"scale"</span>, [channels], dtype=tf.float32, initializer=tf.random_normal_initializer(<span class="number">1.0</span>, <span class="number">0.02</span>))</span><br><span class="line">		mean, variance = tf.nn.moments(bn_input, axes=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], keep_dims=<span class="keyword">False</span>)</span><br><span class="line">		normalized = tf.nn.batch_normalization(bn_input, mean, variance, offset, scale, variance_epsilon=<span class="number">1e-5</span>)</span><br><span class="line">		<span class="keyword">return</span> normalized	</span><br></pre></td></tr></table></figure></p>
<h3 id="Utils"><a href="#Utils" class="headerlink" title="Utils"></a>Utils</h3><p>print all model variables<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) <span class="comment"># all the global variables</span></span><br><span class="line">slim.get_model_variables() <span class="keyword">or</span> tf.get_collection(tf.GraphKeys.MODEL_VARIABLES) </span><br><span class="line"><span class="comment">#variables defined by slim (tf.contrib.framework.model_variable)</span></span><br><span class="line"><span class="comment">#excluding gradient variables</span></span><br><span class="line">tf.trainable_variables() <span class="comment">#excluding graident variables and batch_norm variables (moving_mean and moving_variance)</span></span><br></pre></td></tr></table></figure></p>
<p>print regularization losses(weight decay) and other losses<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">slim.losses.get_regularization_losses()</span><br><span class="line">slim.losses.get_losses() <span class="comment"># losses except weight decay</span></span><br><span class="line">slim.losses.get_total_loss(add_regularization_losses=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>programming language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch Learning Note</title>
    <url>/2022/06/16/programming_language/PyTorch/PyTorch%20Learning%20Note/</url>
    <content><![CDATA[<h3 id="Using-multiple-GPUs"><a href="#Using-multiple-GPUs" class="headerlink" title="Using multiple GPUs"></a>Using multiple GPUs</h3><p>device = torch.device(“cuda:gpu_id1”)</p>
<p>model = nn.DataParallel(model, [gpu_id1, gpu_id2, …])<br>model.to(device)</p>
<p>input = input.to(device)</p>
<p>Note that gpu_id1 must be the first gpu in the gpu_list in model.DataParallel(arg1, gpu_list)</p>
]]></content>
      <categories>
        <category>programming language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>wxPython</title>
    <url>/2022/06/16/programming_language/Python2/wxPython/</url>
    <content><![CDATA[<p><strong>Framework Code</strong></p>
<ol>
<li><p><a href="\code\wxPython\basic_frame\app.py">Simplest App</a>:  </p>
<ul>
<li><p><code>MyApp(False)</code>means noredirect while <code>MyApp(True, &#39;output.txt&#39;)</code> will redirect the output to the file output.txt.</p>
</li>
<li><p>Derivation from wx.App uses <code>def OnInit(self)</code> while others use <code>def __init__(self)</code></p>
</li>
<li><p>Donnot forget <code>return True</code></p>
</li>
</ul>
</li>
<li><p><a href="\code\wxPython\basic_frame\frame.py">Simplest Frame</a>: <code>frame.Show()</code>, <code>frame.Centre()</code> </p>
</li>
<li><p><a href="\code\wxPython\advanced_frame\texteditor.py">A frame with menubar, toolbar, and statusbar</a>: UpdateUIEvents are sent periodically by the framework during idle time to allow the application to check if the state of a control needs to be updated.</p>
<ul>
<li><p>Append a menu_item with icon,   </p>
   <pre><code>qmi = wx.MenuItem(fileMenu, APP_EXIT, '&Quit\tCtrl+Q')
   qmi.SetBitmap(wx.Bitmap('exit.png'))
   fileMenu.AppendItem(qmi)</code></pre>
</li>
<li><p>Menu check_item</p>
   <pre><code>self.shtl = viewMenu.Append(wx.ID_ANY, 'Show toolbar', 'Show Toolbar', kind=wx.ITEM_CHECK)
   self.Bind(wx.EVT_MENU, self.ToggleStatusBar, self.shst)
   def ToggleStatusBar(self, e):        
       if self.shst.IsChecked():
        self.statusbar.Show()
       else:
           self.statusbar.Hide()</code></pre>
</li>
</ul>
</li>
<li><p><a href="\code\wxPython\advanced_frame\validators.py">Pop a Dialogue box with validator</a>: derivation from <code>wx.PyValidator</code></p>
</li>
<li><p><a href="\code\wxPython\advanced_frame\login.py">A login dialogue before main frame</a>: <code>wx.PyValidator</code></p>
</li>
<li><p><a href="\code\wxPython\advanced_frame\notebook.py">Notebook with multiple pages</a></p>
</li>
<li><p><a href="\code\wxPython\advanced_frame\flatbook.py">Advanced notebook, user can add new pages</a></p>
</li>
<li><p><a href="\code\wxPython\advanced_frame\foldpannelbar.py">Foldable Pannel</a></p>
<p> <img src="\code\wxPython\md_img\foldable_pannel.jpg" alt=""></p>
</li>
<li><p><a href="\code\wxPython\advanced_frame\splash.py">Splash Screen</a>: splash before entering main frame <code>wx.SplashScreen</code></p>
</li>
<li><p><a href="\code\wxPython\advanced_frame\auiframe.py">Extendable Pannel</a></p>
<p> <img src="\code\wxPython\md_img\extendable_pannel.jpg" alt=""></p>
</li>
<li><p>SplitPanel (<a href="\code\wxPython\advanced_frame\columnpage.py">hide</a>, <a href="\code\wxPython\advanced_frame\columnpage2.py">show</a>): a horizontally or vertically splited panel, loading html file</p>
</li>
<li><p><a href="\code\wxPython\advanced_frame\FileDragDrop.py">File Drag Drop</a></p>
<p> <img src="\code\wxPython\md_img\filedragdrop.jpg" alt=""></p>
</li>
<li><p><a href="\code\wxPython\advanced_frame\filehunter.py">File Hunter</a></p>
<p> <img src="\code\wxPython\md_img\filehunter.png" alt=""></p>
</li>
<li><p><a href="\code\wxPython\advanced_frame\spreadsheet.py">SpreadSheet</a></p>
<p> <img src="\code\wxPython\md_img\spreadsheet.png" alt=""></p>
</li>
<li><p><a href="\code\wxPython\advanced_frame\mediaplayer.py">Media Player</a></p>
<p> <img src="\code\wxPython\md_img\player.png" alt=""></p>
</li>
<li><p><a href="\code\wxPython\advanced_frame\web_browser.py">Web Browser</a></p>
<p> <img src="\code\wxPython\md_img\browser.png" alt=""></p>
</li>
</ol>
<hr>
<p><strong>Component Code</strong></p>
<ol>
<li><p><a href="\code\wxPython\building_block\bitmaps.py">Bitmap</a>: use bitmap to beautify the appearance, or use the bitmap brush (transparent widgets may be a little troublesome <a href="\code\wxPython\building_block\transparent.py">here</a>)</p>
 <pre><code>self.Bind(wx.EVT_PAINT, self.OnPaint)
 def OnPaint(self, event):
     dc = wx.PaintDC(self)
     dc.SetBackgroundMode(wx.TRANSPARENT)
     brush1 = wx.BrushFromBitmap(wx.Bitmap('pattern1.jpg'))
     dc.SetBrush(brush1)
     w, h =  self.GetSize()
     dc.DrawRectangle(0, 0, w, h)</code></pre>
</li>
<li><p><a href="\code\wxPython\building_block\button_change_label.py">Simplest Button</a>: use lib to build advanced buttons</p>
<ul>
<li>Bind function with building blocks    <pre><code>self.Bind(wx.EVT_BUTTON, self.OnButton, button) 
   def OnButton(self, event):</code></pre></li>
<li>Note <code>GetChildren()</code>,  <code>GetParent()</code>, <code>GetId()</code>, <code>FindWindowById(self.btnId)</code></li>
</ul>
</li>
<li><p><a href="\code\wxPython\building_block\advanced_buttons.py">Advanced button</a>: bitmap button, toggle button, gradient button</p>
</li>
<li><p><a href="\code\wxPython\building_block\frame_icon.py">Frame Icon</a>: personalize the frame icon</p>
 <pre><code>con = wx.Icon(path, wx.BITMAP_TYPE_PNG)
 self.SetIcon(icon):</code></pre>
</li>
<li><p><a href="\code\wxPython\building_block\clipboard.py">Interaction with Clipboard</a>: paste to and copy from system clipboard</p>
</li>
<li><p><a href="\code\wxPython\building_block\drag_and_drop.py">Drop Files to Frame</a>:   : <code>wx.PyDropTarget</code></p>
</li>
<li><p><a href="\code\wxPython\building_block\two_stage_help.py">Help in Frame</a>: when initializing Frame</p>
 <pre><code>pre = wx.PreFrame()
 pre.SetExtraStyle(wx.FRAME_EX_CONTEXTHELP)
 pre.Create(parent, *args, **kwargs)
 self.PostCreate(pre)</code></pre>
</li>
<li><p><a href="\code\wxPython\building_block\checkbox.py">Checkbox</a>: use more general event detector to simplify code</p>
<pre><code>`self.Bind(wx.EVT_CHECKBOX, self.OnCheck)`
</code></pre><p> <code>e_obj = event.GetEventObject()</code></p>
</li>
<li><p><a href="\code\wxPython\building_block\choice_ctrl.py">dropdown menu</a></p>
</li>
<li><p>MessageBox</p>
 <pre><code>def ShowMessage(self,event):
     wx.MessageBox('Download completed', 'Info', wx.OK | wx.ICON_INFORMATION)</code></pre>
</li>
<li><p>Open file_dialogue</p>
 <pre><code>dlg = wx.FileDialog(self, "Open File", style=wx.FD_OPEN)
 if dlg.ShowModal() == wx.ID_OK:
     fname = dlg.GetPath()
     handle = open(fname, 'r')
     self.txtctrl.SetValue(handle.read())
     handle.close()</code></pre>
</li>
<li><p><a href="\code\wxPython\building_block\popupmenu.py">popup menu</a>: right click to pop up the menu</p>
</li>
<li><p><a href="\code\wxPython\building_block\staticbox.py">static box</a>: a static box containing components</p>
</li>
<li><p><a href="\code\wxPython\building_block\listbasics.py">list ctrl</a>: multi-column list</p>
<p> <img src="http://i.imgur.com/GjbWiwX.jpg" alt=""></p>
</li>
<li><p><a href="\code\wxPython\building_block\customtree.py">customtree</a>: a tree-structure file browser</p>
</li>
<li><p><a href="\code\wxPython\building_block\vlbox.py">virtual list box</a></p>
<p> <img src="http://i.imgur.com/9DswQ5f.jpg" alt=""></p>
</li>
<li><p><a href="\code\wxPython\building_block\stc_lexer.py">Styled Text</a>: i.e., python-style text</p>
</li>
<li><p><a href="\code\wxPython\building_block\downloader.py">Download Progressbar</a></p>
</li>
<li><p><a href="\code\wxPython\building_block\about.py">About info</a>: info including name, version, copyright, and description</p>
</li>
<li><p>Choose Color Dialogue</p>
 <pre><code>colour_data = wx.ColourData()
 colour = self.GetBackgroundColour()
 colour_data.SetColour(colour)
 colour_data.SetChooseFull(True)

 dlg = wx.ColourDialog(self, colour_data)
 if dlg.ShowModal() == wx.ID_OK:
     colour = dlg.GetColourData().GetColour()
     self.SetBackgroundColour(colour)
     self.Refresh()
 dlg.Destroy()</code></pre>
</li>
<li><p><a href="\code\wxPython\building_block\imagedlg.py">Image Browser with simple editing</a></p>
</li>
<li><p><a href="\code\wxPython\building_block\imageslide.py">Image Slide Show</a></p>
</li>
<li><p><a href="\code\wxPython\building_block\searchbar.py">Search Bar</a></p>
</li>
<li><p>Timer</p>
 <pre><code>self._timer = wx.Timer(self)
 self.Bind(wx.EVT_TIMER, self.OnTimer, self._timer)
 self._timer.Start(100)
 self._timer.Stop()</code></pre>
</li>
<li><p>Execute Command Line</p>
 <pre><code>import outputwin
 self.output = outputwin.OutputWindow(self)
 self.output.StartProcess("ping %s" % url, blocksize=64)</code></pre>
</li>
<li><p><a href="\code\wxPython\building_block\music_player.py">Music Player</a></p>
</li>
<li><p><a href="\code\wxPython\building_block\video_player.py">Video Player</a>: First, you need to install MplayerCtrl lib. Secondly, place the mplayer folder under the current working directory. </p>
</li>
</ol>
<hr>
<p><strong>Layout</strong></p>
<ol>
<li><p><a href="\code\wxPython\basic_frame\boxsizer.py"><code>wx.BoxSizer</code></a>: <code>proportion</code> is used to control main direction and <code>wx.EXPAND</code> is used to control the other direction. Note in BoxSizer, alignment is only valid in one direction. AddSpacer(50) is equal to Add((50,50)). AddStretchSpacer() is equal to Add((0,0),proportion=1).</p>
 <pre><code>sizer = wx.BoxSizer(wx.HORIZONTAL)
 sizer.AddSpacer(50)
 sizer.Add(sth,proportion=0, flag=wx.ALL, border=5) #use flag to mark which side has border
 sizer.Add((-1,10)) #add a black space, height=10
 # sizer.Add(sth,proportion=0, wx.EXPAND|wx.RIGHT|wx.ALIGN_RIGHT, border=5)
 sizer.AddSpacer((0,0)) #sizer.AddStretchSpacer()
 self.SetSizer(sizer)
 self.SetInitialSize()</code></pre>
</li>
<li><p><a href="\code\wxPython\basic_frame\gridsizer.py"><code>wx.GridSizer</code></a>:  <code>proportion</code> is usually set as 0, use <code>Add((20,20), 1, wx.EXPAND)</code> to take up space.</p>
 <pre><code>wx.GridSizer(2, 2, vgap=0, hgap=0)
 msizer.Add(sth, 0, wx.EXPAND)</code></pre>
</li>
<li><p><a href="\code\wxPython\basic_frame\flexgridsizer.py"><code>wx.FlexGridSizer</code></a>: make some rows and columns growable.</p>
 <pre><code>fgs.AddGrowableRow(2)
 fgs.AddGrowableCol(1)</code></pre>
</li>
<li><p><a href="\code\wxPython\basic_frame\gridbagsizer.py"><code>wx.GridBagSizer</code></a>: use <code>pos</code> and <code>span</code> to indicate the location and size.</p>
 <pre><code>sizer = wx.GridBagSizer(vgap=8, hgap=8)
 sizer.Add(sth, (1, 2), (1, 15), wx.EXPAND) </code></pre>







</li>
</ol>
<hr>
<p><strong>Notes</strong></p>
<ol>
<li><p>Event Propagation: When an event can intrigue multiple events, use <code>event.skip()</code> to guaranttee the occurrence of following events. Take <a href="\code\wxPython\building_block\keyevents.py">keyevents.py</a> for an example.</p>
</li>
<li><p>Virtual Ride: <code>wx.PyPannel</code></p>
</li>
<li><p>Bind function which will be checked in the idle time<br><code>self.Bind(wx.EVT_UPDATE_UI, self.OnUpdateEditMenu)</code></p>
</li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>wxPython</tag>
      </tags>
  </entry>
  <entry>
    <title>Python</title>
    <url>/2022/06/16/programming_language/Python2/Python/</url>
    <content><![CDATA[<ol>
<li><p>check type</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">isinstance(n,int)</span><br></pre></td></tr></table></figure>
</li>
<li><p>list</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1 = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>];</span><br><span class="line">t2 = [<span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>];</span><br><span class="line">t1.extend(t2);</span><br><span class="line">t1.append(<span class="string">'g'</span>);</span><br><span class="line">t1 = t1+[<span class="string">'g'</span>]</span><br><span class="line">t1.sort();</span><br><span class="line"><span class="keyword">del</span> t1[<span class="number">1</span>:<span class="number">3</span>];</span><br><span class="line">t1.remove(<span class="string">'b'</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>dictionary</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eng2sp = &#123;<span class="string">'one'</span>: <span class="string">'uno'</span>, <span class="string">'two'</span>: <span class="string">'dos'</span>, <span class="string">'three'</span>: <span class="string">'tres'</span>&#125;</span><br><span class="line">eng2sp.values() <span class="comment">#'uno','dos','tres'</span></span><br><span class="line">eng2sp.items()</span><br><span class="line"><span class="string">'one'</span> <span class="keyword">in</span> eng2sp <span class="comment">#true</span></span><br><span class="line">inverse = invert_dict(eng2sp)  </span><br><span class="line">d = dict(zip(<span class="string">'abc'</span>,range(<span class="number">3</span>)))</span><br><span class="line">d.get(word,<span class="number">0</span>) <span class="comment">#d[word] if word in d, 0 otherwise</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>tupe: similar with list but immutable</p>
</li>
<li><p>file operation</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fout = os.getcwd()</span><br><span class="line">os.path.abspath(<span class="string">'tmp.txt'</span>)</span><br><span class="line">os.path.exists(<span class="string">'tmp.txt'</span>)</span><br><span class="line">os.path.isfile/isdir</span><br><span class="line">os.listdir(cwd)</span><br><span class="line">fout = open(<span class="string">'output.txt'</span>,<span class="string">'w'</span>)</span><br><span class="line">fout.write(<span class="string">'hehe'</span>)</span><br><span class="line">fout.close()</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Send Email</title>
    <url>/2022/06/16/programming_language/Python2/Python%20Send%20Email/</url>
    <content><![CDATA[<ol>
<li><p>Open SMTP service for your email and obtain the SMTP password.</p>
</li>
<li><p>Create a Python script</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> win32serviceutil</span><br><span class="line"><span class="keyword">import</span> win32service</span><br><span class="line"><span class="keyword">import</span> win32event</span><br><span class="line"><span class="keyword">import</span> servicemanager</span><br><span class="line"><span class="keyword">import</span> smtplib</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Getmyip</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">visit</span><span class="params">(self,url)</span>:</span></span><br><span class="line">        opener = urllib2.urlopen(url)</span><br><span class="line">        <span class="keyword">if</span> url == opener.geturl():</span><br><span class="line">            IPstr = opener.read()</span><br><span class="line">        <span class="keyword">return</span> re.search(<span class="string">'\d+\.\d+\.\d+\.\d+'</span>,IPstr).group(<span class="number">0</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getip</span><span class="params">(self)</span>:</span></span><br><span class="line">        myip = self.visit(<span class="string">"http://www.net.cn/static/customercare/yourip.asp"</span>)</span><br><span class="line">        <span class="keyword">return</span> myip</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppServerSvc</span> <span class="params">(win32serviceutil.ServiceFramework)</span>:</span></span><br><span class="line">    _svc_name_ = <span class="string">"TestService"</span></span><br><span class="line">    _svc_display_name_ = <span class="string">"Test Service"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,args)</span>:</span></span><br><span class="line">        win32serviceutil.ServiceFramework.__init__(self,args)</span><br><span class="line">        self.hWaitStop = win32event.CreateEvent(<span class="keyword">None</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="keyword">None</span>)</span><br><span class="line">        self.run = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SvcStop</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)</span><br><span class="line">        win32event.SetEvent(self.hWaitStop)</span><br><span class="line">        self.run = <span class="keyword">False</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SvcDoRun</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">while</span>(self.run==<span class="keyword">True</span>):</span><br><span class="line">            self.main()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(self)</span>:</span></span><br><span class="line">        fromaddr = <span class="string">'XXX@qq.com'</span></span><br><span class="line">        toaddrs  = <span class="string">'XXX@qq.com'</span> </span><br><span class="line">        </span><br><span class="line">        server = smtplib.SMTP_SSL(<span class="string">'smtp.qq.com'</span>)</span><br><span class="line">        server.set_debuglevel(<span class="number">1</span>)    </span><br><span class="line">        print(<span class="string">"--- Need Authentication ---"</span>)</span><br><span class="line">        username = <span class="string">'XXX'</span> </span><br><span class="line">        password = <span class="string">'XXX'</span> </span><br><span class="line">        server.login(username, password)</span><br><span class="line">        </span><br><span class="line">        prev_msg = <span class="string">''</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">True</span>):</span><br><span class="line">            getmyip=Getmyip()</span><br><span class="line">            msg = getmyip.getip()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> msg==prev_msg:</span><br><span class="line">                server.sendmail(fromaddr, toaddrs, msg)</span><br><span class="line">                prev_msg = msg</span><br><span class="line">            time.sleep(<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">        server.quit()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) == <span class="number">1</span>:</span><br><span class="line">        servicemanager.Initialize()</span><br><span class="line">        servicemanager.PrepareToHostSingle(AppServerSvc)</span><br><span class="line">        servicemanager.StartServiceCtrlDispatcher()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        win32serviceutil.HandleCommandLine(AppServerSvc)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Make python file into an exe</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pyinstaller -F MyService.py</span><br></pre></td></tr></table></figure>
</li>
<li><p>Make exe into a Windows service</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sc create MyServer binPath=$exe_path</span><br><span class="line">sc start MyServer</span><br><span class="line">sc stop MyServer</span><br><span class="line">sc delete MyServer</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Draw Text on Image</title>
    <url>/2022/06/16/programming_language/Python2/Python%20Draw%20Text%20on%20Image/</url>
    <content><![CDATA[<h3 id="Use-PIL"><a href="#Use-PIL" class="headerlink" title="Use PIL"></a>Use PIL</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL.ImageDraw</span><br><span class="line"><span class="keyword">import</span> PIL.Image</span><br><span class="line"><span class="keyword">import</span> PIL.ImageFont</span><br><span class="line"></span><br><span class="line">pil_img = PIL.Image.fromarray(bb_img)</span><br><span class="line">draw = PIL.ImageDraw.Draw(pil_img)</span><br><span class="line">font = PIL.ImageFont.truetype(<span class="string">"/usr/share/fonts/truetype/ttf-dejavu/DejaVuSans.ttf"</span>,<span class="number">20</span>)</span><br><span class="line">draw.text((<span class="number">10</span>, <span class="number">10</span>),<span class="string">"Frame: %09d"</span>%(iframe),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),font=font)</span><br></pre></td></tr></table></figure>
<h3 id="Use-OpenCV"><a href="#Use-OpenCV" class="headerlink" title="Use OpenCV"></a>Use OpenCV</h3><p>—</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">cv2.putText(cv_img, <span class="string">'Frame: %d'</span>%(iframe), (<span class="number">10</span>,<span class="number">20</span>),  cv2.FONT_HERSHEY_SIMPLEX, <span class="number">0.65</span>, (<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>), <span class="number">3</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>programming language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python bottle</title>
    <url>/2022/06/16/programming_language/Python2/Python%20bottle/</url>
    <content><![CDATA[<ol>
<li><p>Use <code>route</code> to indicate path, 80 is the default port for HTTP. </p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> bottle</span><br><span class="line"></span><br><span class="line"><span class="meta">@bottle.route('/hello')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Hello World!"</span></span><br><span class="line"></span><br><span class="line">bottle.run(host=<span class="string">'localhost'</span>, port=<span class="number">80</span>, debug=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Dynamic path V.S. static path. We can use filter such as <code>&lt;id:int&gt;</code> to convert input variables to certain type. </p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> bottle</span><br><span class="line"></span><br><span class="line"><span class="meta">@bottle.route('/hello/&amp;lt;name&gt;/&amp;lt;address&gt;')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">(name,address)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Hello "</span>+name</span><br><span class="line"></span><br><span class="line">bottle.run(host=<span class="string">'localhost'</span>, port=<span class="number">80</span>, debug=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Use local resources. Pay special attention to the path. Local resource names should start with ‘/‘. </p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@bottle.route('/&lt;filename:path&gt;')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">server_static</span><span class="params">(filename)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> bottle.static_file(filename, root=<span class="string">'./resource/'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>The default HTTP request method for <code>route()</code> is <code>get()</code>, we can use other methods <code>post()</code>, <code>put()</code>, <code>delete()</code>, <code>patch()</code>. The <strong>POST</strong> method is commonly used for HTML form submission. When entering URL address, <strong>GET</strong> method is used.</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> bottle</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_login</span><span class="params">(username, password)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> username==<span class="string">'niuli'</span> <span class="keyword">and</span> password==<span class="string">'niuli'</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">@bottle.get('/login') # or @route('/login')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'''</span></span><br><span class="line"><span class="string">        &amp;lt;form action="/login" method="post"&gt;</span></span><br><span class="line"><span class="string">            Username: &amp;lt;input name="username" type="text" /&gt;</span></span><br><span class="line"><span class="string">            Password: &amp;lt;input name="password" type="password" /&gt;</span></span><br><span class="line"><span class="string">            &amp;lt;input value="Login" type="submit" /&gt;</span></span><br><span class="line"><span class="string">        &amp;lt;/form&gt;</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@bottle.post('/login') # or @route('/login', method='POST')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_login</span><span class="params">()</span>:</span></span><br><span class="line">    username = bottle.request.forms.get(<span class="string">'username'</span>)</span><br><span class="line">    password = bottle.request.forms.get(<span class="string">'password'</span>)</span><br><span class="line">    <span class="keyword">if</span> check_login(username, password):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"&amp;lt;p&gt;Your login information was correct.&amp;lt;/p&gt;"</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"&amp;lt;p&gt;Login failed.&amp;lt;/p&gt;"</span></span><br><span class="line"></span><br><span class="line">bottle.run(host=<span class="string">'localhost'</span>, port=<span class="number">80</span>, debug=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Return local files <code>localhost/filename</code>. Note <code>path:path</code> is important, otherwise the filename containing ‘/‘ may not be correctly recognized.</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bottle <span class="keyword">import</span> static_file</span><br><span class="line"></span><br><span class="line"><span class="meta">	@route(’/&lt;filepath:path&gt;’)</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">server_static</span><span class="params">(filepath)</span>:</span></span><br><span class="line">	    <span class="keyword">return</span> static_file(filepath, root=’./resource’)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Use <code>redirect</code> to jump to another page: <code>bottle.redirect(&#39;/login&#39;)</code></p>
</li>
<li><p>Use the HTML template. In the template file, the lines starting with <code>%</code> are python codes and others are HTML codes. We can include other templates in the current template by using <code>% include(&#39;header.tpl&#39;, title=&#39;Page Title&#39;)</code>. Cookies, HTTP header, HTML &lt;form&gt; fields and other request data is available through the global <code>request</code> object.</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@bottle.route('/&lt;name&gt;')</span></span><br><span class="line"><span class="meta">@bottle.view('my_template.html')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">root</span><span class="params">(name=<span class="string">"default"</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> bottle.template(<span class="string">'my_template'</span>, name=name)</span><br><span class="line"></span><br><span class="line"><span class="meta">@bottle.post('/login') # or @route('/login', method='POST')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_login</span><span class="params">()</span>:</span></span><br><span class="line">    username = bottle.request.forms.get(<span class="string">'username'</span>)</span><br><span class="line">    password = bottle.request.forms.get(<span class="string">'password'</span>)</span><br><span class="line">    bottle.redirect(<span class="string">'/'</span>+username)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Some tricks for helping development </p>
<ul>
<li><p><code>bottle.debug(True)</code> The default error page shows a traceback. Templates are not cached. Plugins are applied immediately.</p>
</li>
<li><p><code>run(reloader=True)</code> Every time you edit a module file, the reloader restarts the server process and loads the newest version of your code.</p>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Fastest Way to Read Text, Image, and Video in Python</title>
    <url>/2022/06/16/programming_language/Python2/Fastest%20Way%20to%20Read%20Text,%20Image,%20and%20Video%20in%20Python/</url>
    <content><![CDATA[<p>When comparing the efficiency of different libraries, there may exist a few orders of magnitude difference. In the implementation in demand of high efficiency, locate the time-consuming function and replace it with the most efficient library function.</p>
<ol>
<li><p>Text: Pandas<br>Installation: <code>pip install pandas</code> or <code>conda install pandas</code></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(text_name, sep=<span class="string">','</span>, header=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Image: Pillow-SIMD, skimage, OpenCV, imageio</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> skimage</span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment">#read a 1280x720 image</span></span><br><span class="line">pil_im = Image.open(image_name) <span class="comment">#0.0057s</span></span><br><span class="line">pil_im = pil_im.resize((<span class="number">448</span>,<span class="number">448</span>)) <span class="comment">#0.025s</span></span><br><span class="line"></span><br><span class="line">sk_im = skimage.io.imread(image_name) <span class="comment">#0.026s</span></span><br><span class="line">sk_im = skimage.transform.resize(sk_im, (<span class="number">448</span>, <span class="number">448</span>)) <span class="comment">#0.060S</span></span><br><span class="line"></span><br><span class="line">cv_im = cv2.imread(image_name) <span class="comment">#0.021s</span></span><br><span class="line">cv_im = cv2.resize(cv_im, (<span class="number">448</span>, <span class="number">448</span>)) <span class="comment">#0.0016s</span></span><br><span class="line"></span><br><span class="line">im = imageio.imread(image_name) <span class="comment">#0.033s</span></span><br></pre></td></tr></table></figure>
<p>Pillow-SIMD is faster than Pillow, which is not reported here. OpenCV is the most efficient one here.</p>
</li>
<li><p>Video: OpenCV, skvideo, imageio</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="keyword">import</span> skvideo</span><br><span class="line"></span><br><span class="line"><span class="comment">#read 30fps video with each frame 1280x720 </span></span><br><span class="line">cap = cv2.VideoCapture(video_name)</span><br><span class="line">ret, frame = cap.read() <span class="comment">#0.002s</span></span><br><span class="line"></span><br><span class="line">vid = imageio.get_reader(video_name, <span class="string">'ffmpeg'</span>)</span><br><span class="line"><span class="keyword">for</span> image <span class="keyword">in</span> vid.iter_data(): <span class="comment">#0.004s</span></span><br><span class="line"></span><br><span class="line">skvideo.setFFmpegPath(os.path.dirname(sys.executable))</span><br><span class="line">videogen = skvideo.io.vreader(video_name)</span><br><span class="line"><span class="keyword">for</span> img <span class="keyword">in</span> videogen: <span class="comment">#0.073s</span></span><br></pre></td></tr></table></figure>
<p>For OpenCV in Anaconda, it sometimes fails in reading from video but succeeds in reading from camera. In this case, <code>/usr/bin/python</code> is recommended. imageio and OpenCV are comparable here.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda</title>
    <url>/2022/06/16/programming_language/Python2/Anaconda/</url>
    <content><![CDATA[<p>python virtual environment: creat an isolated environment for each python version set, similar with pyenv.</p>
<h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation:"></a>Installation:</h3><p><strong>For Windows:</strong></p>
<p>Anaconda navigator is a visualization tool for setting up environment and installing packages under each environment.</p>
<p>Python IDEs like Jupyter notebook and spyder can all be treated as packages under each environment.</p>
<p><strong>For Linux:</strong></p>
<pre><code>bash Anaconda2-4.3.1-Linux-x86_64.sh
</code></pre><h3 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h3><p>list: conda info —envs</p>
<p>create: <code>conda create --name $newenv python=2.7</code> (if copy, use —clone $oldenv)</p>
<p>activate: source activate $newenv</p>
<p>revert: source deactivate</p>
<p>delete: conda remove —name $newenv —all</p>
<h3 id="package"><a href="#package" class="headerlink" title="package"></a>package</h3><p>list: conda list</p>
<p>search: conda search pack</p>
<p>install new packages:</p>
<ol>
<li>conda install pack</li>
<li>conda install pack=1.8.2 # install certain version</li>
<li>conda install —name env pack</li>
<li>search on <a href="http://anaconda.org/" target="_blank" rel="noopener">http://anaconda.org/</a>, no need to register</li>
<li>pip install pack</li>
<li>install from local pack</li>
</ol>
<p><strong>Tips:</strong> check available pythons: conda search —full-name python</p>
]]></content>
      <categories>
        <category>programming language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>MATLAB draw plot</title>
    <url>/2022/06/16/programming_language/MATLAB/MATLAB%20draw%20plot/</url>
    <content><![CDATA[<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">% Set some nice settings.</span></span><br><span class="line">grid on;</span><br><span class="line">format long;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Hold the graphics output until we are good to go.</span></span><br><span class="line">hold all;</span><br><span class="line"></span><br><span class="line"><span class="comment">% To create some random test data.</span></span><br><span class="line">x1 = <span class="number">100</span> : <span class="number">100</span> : <span class="number">1000</span>;</span><br><span class="line">raw_y1 = [<span class="number">0.76</span>,<span class="number">0.79</span>,<span class="number">0.80</span>,<span class="number">0.80</span>,<span class="number">0.81</span>,<span class="number">0.82</span>,<span class="number">0.82</span>,<span class="number">0.82</span>,<span class="number">0.82</span>,<span class="number">0.81</span>];</span><br><span class="line">raw_y2 = [<span class="number">0.85</span>,<span class="number">0.81</span>,<span class="number">0.83</span>,<span class="number">0.83</span>,<span class="number">0.82</span>,<span class="number">0.79</span>,<span class="number">0.78</span>,<span class="number">0.80</span>,<span class="number">0.82</span>,<span class="number">0.82</span>];</span><br><span class="line">raw_y3 = [<span class="number">0.85</span>,<span class="number">0.84</span>,<span class="number">0.83</span>,<span class="number">0.83</span>,<span class="number">0.83</span>,<span class="number">0.83</span>,<span class="number">0.82</span>,<span class="number">0.82</span>,<span class="number">0.82</span>,<span class="number">0.82</span>];</span><br><span class="line">legendText = cell(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">plot(x1,y1,<span class="string">'--go'</span>,<span class="string">'MarkerSize'</span>,<span class="number">5</span>, <span class="string">'MarkerFaceColor'</span>,<span class="string">'g'</span>, <span class="string">'LineWidth'</span>,<span class="number">3</span>);</span><br><span class="line">legendText(<span class="keyword">end</span>+<span class="number">1</span>) = &#123; <span class="string">'CUB'</span> &#125;;</span><br><span class="line"></span><br><span class="line">plot(x1,y2,<span class="string">':bo'</span>, <span class="string">'MarkerSize'</span>,<span class="number">5</span>, <span class="string">'MarkerFaceColor'</span>,<span class="string">'b'</span>, <span class="string">'LineWidth'</span>,<span class="number">3</span>);</span><br><span class="line">legendText(<span class="keyword">end</span>+<span class="number">1</span>) = &#123; <span class="string">'SUN'</span> &#125;;</span><br><span class="line"></span><br><span class="line">plot(x1,y3,<span class="string">'-ro'</span>, <span class="string">'MarkerSize'</span>,<span class="number">5</span>,  <span class="string">'MarkerFaceColor'</span>,<span class="string">'r'</span>, <span class="string">'LineWidth'</span>,<span class="number">3</span>);</span><br><span class="line">legendText(<span class="keyword">end</span>+<span class="number">1</span>) = &#123; <span class="string">'Dogs'</span> &#125;;</span><br><span class="line"></span><br><span class="line">xlim([<span class="number">100</span> <span class="number">1000</span>]);</span><br><span class="line">ylim([<span class="number">0.72</span> <span class="number">0.90</span>]);</span><br><span class="line"></span><br><span class="line">set(gca,<span class="string">'fontsize'</span>,<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">% set sticks on x and y axis</span></span><br><span class="line">get(gca, <span class="string">'xtick'</span>);</span><br><span class="line">set(gca, <span class="string">'xtick'</span>, <span class="number">100</span>:<span class="number">100</span>:<span class="number">1000</span>);</span><br><span class="line">get(gca, <span class="string">'ytick'</span>);</span><br><span class="line">set(gca, <span class="string">'ytick'</span>, <span class="number">0.72</span>:<span class="number">0.02</span>:<span class="number">0.90</span>);</span><br><span class="line"></span><br><span class="line">xlabel(<span class="string">'# web training instances per category'</span>);</span><br><span class="line">ylabel(<span class="string">'Accuracy'</span>);</span><br><span class="line"></span><br><span class="line">legend(legendText,<span class="string">'location'</span>,<span class="string">'southeast'</span>);</span><br><span class="line"></span><br><span class="line">hold off;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>programming language</category>
        <category>MATLAB</category>
      </categories>
      <tags>
        <tag>MATLAB</tag>
      </tags>
  </entry>
  <entry>
    <title>HTML+CSS</title>
    <url>/2022/06/16/programming_language/HTML/HTML+CSS/</url>
    <content><![CDATA[<ol>
<li><p>Website consists of three components: structure(HTML), representation(CSS), action(Javascript). XHTML is a strict version of HTML. The structure of webpage is previously done based on table, but currently via CSS.</p>
</li>
<li><p>Escape character in HTML source code: <code>&amp;lt;</code>:&lt;, <code>&amp;gt;</code>:&gt;, <code>&amp;nbsp;</code>:space</p>
</li>
<li><p><code>&lt;p&gt;</code>:paragraph, <code>&lt;br&gt;</code>:break, <code>&lt;blockquote&gt;</code>:indented paragraph, <code>&lt;b&gt;</code>:bold, <code>&lt;i&gt;</code>:italic, <code>&lt;u&gt;</code>:underline, <code>&lt;s&gt;</code>:deleteline, <code>&lt;strong&gt;</code>:emphasize, <code>&lt;ul&gt;&lt;li&gt;</code>:unordered list, <code>&lt;ol&gt;&lt;li&gt;</code>:ordered list. </p>
</li>
<li><p><code>&lt;a href=&quot;&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;</code>:hyperlink.  <code>&lt;a href=&quot;mailto:ustcnewly@gmail.com&quot;&gt;&lt;/a&gt;</code>. An area on the image can also be used to establish hyperlink based on <code>&lt;map&gt;</code> and <code>&lt;ared&gt;</code>. </p>
</li>
<li><p>Use <code>&lt;frameset&gt;</code> and <code>&lt;frame&gt;</code> to split webpage. Note <code>&lt;frameset&gt;</code> and <code>&lt;frame&gt;</code> belong to the the same level as <code>&lt;body&gt;</code>. <code>&lt;frameset cols=&quot;30%, 30%, *&quot;&gt;&lt;frame src=left.html&gt;</code>. <code>&lt;frame&gt;</code> is obsolete in HTML5. </p>
</li>
<li><p><code>&lt;table border=&quot;1&quot; cellpadding=&quot;4&quot; cellspacing=&quot;6&quot; &gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;tr&gt;&lt;/tr&gt;&lt;/table&gt;</code>. Don’t miss any <code>&lt;tr&gt;</code> or <code>&lt;td&gt;</code>, otherwise the table will be messy. Use <code>&lt;colspan&gt;</code> or <code>&lt;rowspan&gt;</code> to merge cells. For professional table, we can use <code>&lt;caption&gt;</code>, <code>&lt;thead&gt;</code>, <code>&lt;tbody&gt;</code>, and <code>&lt;tfoot&gt;</code>. <code>&lt;table&gt;</code> is now rarely used for layout design. </p>
</li>
<li><p>Insert multimedia elements: </p>
<ul>
<li>image: <code>&lt;img src=&quot;a.jpg&quot; height=&quot;200&quot; width=&quot;200&quot; alt=&quot;desc&quot;/&gt;</code>  </li>
<li>flash: <code>&lt;embed src=&quot;a.swf&quot;  width=&quot;490&quot; height=&quot;400&quot; wmode=&quot;transparent&quot; &gt;&lt;/embed&gt;</code> </li>
<li>bgmusic: <code>&lt;audio src=&quot;a.mp3&quot; hidden=&quot;true&quot; autoplay=&quot;true&quot; loop=&quot;true&quot;&gt;&lt;/audio&gt;</code></li>
<li>bgtile: <code>&lt;body background=&quot;a.png&quot;&gt;&lt;/body&gt;</code></li>
</ul>
</li>
<li><p>Css: object, attribute, and value. Refer to <a href="https://www.w3.org/Style/CSS/" target="_blank" rel="noopener">official website</a> for details.</p>
 <figure class="highlight html"><table><tr><td class="code"><pre><span class="line">h1&#123;</span><br><span class="line">    font-family:Calibri;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">.myclass&#123;</span><br><span class="line">    font-family:Calibri;</span><br><span class="line">&#125;</span><br><span class="line">&amp;lt;p class="myclass"&gt;&amp;lt;/p&gt;</span><br><span class="line"></span><br><span class="line">#myid&#123;</span><br><span class="line">    font-family:Calibri;</span><br><span class="line">&#125;</span><br><span class="line">&amp;lt;p id="myid"&gt;&amp;lt;/p&gt;</span><br><span class="line"># one id name should be used only once in one html file.</span><br></pre></td></tr></table></figure>
</li>
<li><p>Methods to use Css. </p>
<ul>
<li><p>a. Directly insert contents into <code>&lt;style&gt;&lt;/style&gt;</code>.</p>
</li>
<li><p>b. <code>&lt;link href=&quot;mycss.css&quot; type=&quot;text/css&quot; rel=&quot;stylesheet&quot;&gt;</code></p>
</li>
<li><p>c. <code>&lt;style type=&quot;text/css&quot;&gt;@import &quot;mycss.css&quot;&lt;/style&gt;</code>. One html file can import more than one Css files. A Css file can also import other Css files.</p>
<p> The priority of a is higher than b/c. The more special, the higher priority. For the methods with equal priority, override principle is applied. The difference between b and c is that c loads all the Css codes while b only loads the corresponding Css code when necessary. </p>
</li>
</ul>
</li>
<li><p>Some special usage of Css.</p>
<ul>
<li>class for specific tag: <code>p.special{}</code></li>
<li>union of multiple tags: <code>h1,h2,p{}</code></li>
<li>embeded tags: <code>p span{}</code> for descendents and <code>p&gt;span{}</code> for child. complex embedding mixing tags, classes, and ids: <code>td.out .inside strong{}</code>. The easiest way to recognize this kind of embedding is “(tag)(.class) (tag)(#id)”</li>
<li>consecutive tags: <code>th+td+td+td{}</code>, the style is applied to the third <code>td</code>.</li>
</ul>
</li>
<li><p>Css formats for some commonly used tags.</p>
<ul>
<li><p>body:</p>
   <figure class="highlight html"><table><tr><td class="code"><pre><span class="line">body&#123;</span><br><span class="line">    background-image:a.jpg;</span><br><span class="line">    background-repeat:no-repeat;</span><br><span class="line">    background-position:200px 100px;</span><br><span class="line">	background-attachment:fixed;</span><br><span class="line">	background-size: 20% 100%;</span><br><span class="line">	cursor:pointer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>text:</p>
   <figure class="highlight html"><table><tr><td class="code"><pre><span class="line">p&#123;	    </span><br><span class="line">    float:left;</span><br><span class="line">    font-family:Calibri;</span><br><span class="line">    font-size:15px;</span><br><span class="line">    font-style:italic;</span><br><span class="line">    font-weight:bold;</span><br><span class="line">    color:red;</span><br><span class="line">    text-indent:2em;</span><br><span class="line">    text-decoration:underline;</span><br><span class="line">    text-transform:lowercase; #capitalize</span><br><span class="line">    text-align:justify; </span><br><span class="line">    margin:5px 0px #top/down margin left/right margin</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>image:</p>
   <figure class="highlight html"><table><tr><td class="code"><pre><span class="line">img&#123;</span><br><span class="line">	float:left;</span><br><span class="line">	border:1px #9999CC dashed;</span><br><span class="line">	margin:5px; #margin-left/right/top/bottom</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><code>&lt;div&gt;</code> box layout from inside to outside: content, padding, border, margin. Note <code>width</code> and <code>height</code> are for content instead of the whole box. margin can be assigned negative values. Assigning attribute values in <strong>clock-wise</strong> order. Missing values are equal to the values of opposite side. We should first understand the standard flow without the constraints of boxes.</p>
<ul>
<li>block-level (vertical arrangement): <code>&lt;ul&gt;</code>, <code>&lt;li&gt;</code>, <code>&lt;div&gt;</code></li>
<li><p>inline (horizontal arrangement): <code>&lt;strong&gt;</code>,<code>&lt;a&gt;</code>, <code>&lt;span&gt;</code></p>
<p><code>&lt;div&gt;</code> and <code>&lt;span&gt;</code> are both blocks. <code>&lt;div&gt;</code> is vertial while <code>&lt;span&gt;</code> is horizontal. <code>&lt;span&gt;</code> can be used where no other proper tags can be used,s for instance, <code>&lt;span&gt;&lt;img src=&quot;a.jpg&quot;&gt;&lt;/span&gt;</code>.</p>
<p>when using <code>&lt;div&gt;</code>, the margin between the top block and bottom block is max(top_block.bottom_margin,bottom_block.top_margin). when using <code>&lt;span&gt;</code>, the margin between the top block and bottom block is top_block.bottom_margin+bottom_block.top_margin.</p>
</li>
</ul>
</li>
<li><p>floating box: Arrange the non-floating boxes first and then float the floating box to the left or right in the father content. The stuff in the non-floating boxes surrounds the floating box. For floating box: <code>float:left</code>. For non-floating box: <code>clear:left</code>.</p>
</li>
<li><p>box location: static(default), relative, absolute</p>
<ul>
<li><p>relative: <code>position:relative; left:30px; top:30px</code>. Relative shift from original position. Relative has no impact on father box and sibling box.</p>
</li>
<li><p>absolute: <code>position:absolute; top:0px; right:0px</code>. Absolute coordinate in the nearest non-static ancestor. Other boxes treat this absolute box as non-existence.</p>
</li>
</ul>
</li>
<li><p><code>&lt;div style=&quot;display:inline&quot;&gt;&lt;/div&gt;</code>, <code>&lt;span style=&quot;display:block&quot;&gt;&lt;/span&gt;</code>, use display to modifiy the vertical or horizontal order. Set as <code>none</code> to make it invisible. </p>
</li>
<li><p>For hyperlink, <code>a</code> has pseudo classes:<code>link</code>, <code>visited</code>, <code>hover</code>, for example:</p>
 <figure class="highlight html"><table><tr><td class="code"><pre><span class="line">#navigation li a:link, #navigation li a:visited</span><br><span class="line">&#123;</span><br><span class="line">	background-color:#1136c1;</span><br><span class="line">	color:#FFFFFF;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Use <code>div</code> to split columns</p>
<ul>
<li><p>absolute method: left column uses absolute position (Note to change the position of father div to relative) and right column use left margin. The drawback is that bottom row will ignore the left column.</p>
</li>
<li><p>float method: float left and float right. It is easy for fixed width or fixed ratio. For the mixed width such as <code>100%-30px</code>, use the wrapper trick (negative margin).</p>
</li>
</ul>
</li>
</ol>
<hr>
<h2 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h2><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line"><span class="keyword">var</span> name =<span class="string">"newly"</span></span><br><span class="line"><span class="built_in">document</span>.write(<span class="string">"my name is: "</span>+name)</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure>
<ol>
<li><p>variable types</p>
<ul>
<li><code>null</code>: empty variable</li>
<li>string: <code>parseFloat(string)</code>, <code>parseInt(string)</code>,  <code>str.substring(0,3)</code>, <code>str.slice(3,5)</code>, <code>str.charAt(0)</code>, <code>str.bold()</code>, <code>str.fontcolor(&quot;red&quot;)</code>, <code>str.length</code></li>
<li>number: <code>Math.PI</code>, <code>Math.max</code>, <code>isNaN(value)</code>,</li>
<li>bool: <code>true</code> or <code>false</code></li>
<li><p>Array: can be a mixture of numbers and strings, <code>var a = new Array(10, 20, &quot;newly&quot;)</code></p>
  <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(n <span class="keyword">in</span> actorAry)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">document</span>.write(<span class="string">"&amp;lt;li&amp;gt;"</span>+actorAry[n]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>myarr.toString()</code>, <code>myarr.join(&#39;-&#39;)</code>, <code>myarr.concat(tailarr)</code>, <code>myarr.reverse()</code>, <code>myarr.sort(cmpfunc)</code>, <code>slice</code>, <code>splice</code><br><br>stack&amp;&amp;queue operations: <code>myarr.push(&quot;newstr&quot;)</code>, <code>myarr.pop()</code>, <code>myarr.shift()</code>(dequeue), <code>myarr.unshift()</code>(pushfront)<br></p>
</li>
<li><p>Structure: There is no concept: class. Use function to construct an object. <code>var mycard = Card(&quot;newly&quot;, 20)</code>, <code>showCardInof.call(mycard,arg1,arg2)</code> or <code>mycard.showCardInfo(arg1,arg2)</code>, <code>mycard=null</code></p>
  <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">showCardInfo</span>(<span class="params">arg1,arg2</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">document</span>.write(<span class="keyword">this</span>.owner);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">Card</span>(<span class="params">owner, rate</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.owner = owner;</span><br><span class="line">    <span class="keyword">this</span>.rate = rate;</span><br><span class="line">    <span class="keyword">this</span>.showCardInfo=<span class="function"><span class="keyword">function</span>(<span class="params">arg1,arg2</span>)</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">document</span>.write(<span class="keyword">this</span>.owner);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>type related: <code>nameList instanceof Array</code>, <code>typeof(&quot;newly&quot;)</code></p>
</li>
</ul>
</li>
<li><p>commonly used built-in functions or classes</p>
<ul>
<li><p>alert dialogue: <code>alert(&quot;msg&quot;)</code></p>
</li>
<li><p>input diaglogue: <code>var age=prompt(&quot;Input your age&quot;, &quot;0&quot;)</code></p>
</li>
<li><p>confirm dialogue: <code>confirm(&quot;Are you sure?&quot;)</code></p>
</li>
<li><p>Date and time:</p>
   <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> cur = <span class="keyword">new</span> <span class="built_in">Date</span>();</span><br><span class="line">cur.setYear(<span class="number">2016</span>);</span><br><span class="line"><span class="keyword">var</span> seconds = cur.getSeconds();</span><br></pre></td></tr></table></figure>
</li>
<li><p>Error:</p>
   <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> e = <span class="keyword">new</span> <span class="built_in">Error</span>();</span><br><span class="line"><span class="built_in">document</span>.write(e.number&amp;<span class="number">0xFFFF</span>+e.description);</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>operate on the html elements </p>
<p> <code>&lt;img src=&quot;./images/web_fig.jpg&quot; width=500px&gt;</code></p>
<p> The whole HTML page is a DOM tree. </p>
<ul>
<li><p>Visit the DOM elements</p>
  <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="built_in">document</span>.myform.elements[<span class="number">0</span>].value  <span class="comment">//myform is the form name</span></span><br><span class="line"><span class="built_in">document</span>.getElementById(<span class="string">"myid"</span>).childNodes[<span class="number">0</span>].nodeName/nodeValue/nodeType</span><br><span class="line"><span class="built_in">document</span>.getElementById(<span class="string">"myid"</span>).childNodes[<span class="number">0</span>].getAttribute(<span class="string">"attr"</span>)/setAttribute(<span class="string">"attr"</span>,<span class="string">"attval"</span>)	</span><br><span class="line"><span class="built_in">document</span>.getElementsByTagName(<span class="string">"tag"</span>)	</span><br><span class="line"><span class="built_in">document</span>.getElementsByName(<span class="string">"name"</span>)</span><br></pre></td></tr></table></figure>
<p>Since <code>id</code> is unique, we use single form element for <code>Id</code>, but plural form elements for <code>Name</code> and <code>TagName</code>.</p>
</li>
<li><p>Change the DOM elements</p>
  <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> docBody = <span class="built_in">document</span>.getElementById(<span class="string">"DocBody"</span>);</span><br><span class="line"><span class="keyword">var</span> imgObj = <span class="built_in">document</span>.createElement(<span class="string">"&lt;img&gt;"</span>);</span><br><span class="line"><span class="keyword">var</span> newTextNode = <span class="built_in">document</span>.createElement(<span class="string">"content"</span>);</span><br><span class="line">imgObj.src = url;</span><br><span class="line">docBody.appendChild(imgObj);</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>use Javascript to modify the CSS style, <code>document.vlinkColor</code>, <code>document.linkColor</code>, <code>document.alinkColor</code>, <code>document.bgColor</code>, <code>document.fgColor</code>, <code>document.body.text</code>, <code>name1.vspace</code>, <code>name1.hspace</code>. </p>
</li>
<li><p>event handling: we can set event triggering function for certain element or a class of elements.</p>
<ul>
<li>commonly used events (case insensitive): <code>onblur</code>, <code>onchange</code>, <code>onclick</code>, <code>onfocus</code>, <code>onload</code>, <code>onmouseover</code>, <code>onmouseout</code>,  <code>onmousedown</code>, <code>onmouseup</code>, <code>onselect</code>, <code>onsubmit</code>, <code>onunload</code>. Form-related events: <code>myform.onReset</code>, <code>myform.onSubmit</code>, <code>myform.action=&quot;mailto:ustcnewly@gmail.com&quot;</code></li>
<li><p>three approaches to trigger event function<br>a) directly embed function scripts: <code>onlick=&quot;javascript:alert(&quot;msg&quot;)&quot;</code>  //<code>javascript:</code> can be eliminated<br>   b) call the event function: <code>onkeyup = &quot;OnKeyUp()&quot;</code></p>
   <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">OnKeyUp</span>(<span class="params">_e</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">var</span> e = _e?_e:<span class="built_in">window</span>.event;</span><br><span class="line">    <span class="keyword">if</span> (event.keyCode==<span class="number">13</span>)</span><br><span class="line">    &#123;</span><br><span class="line">	    alert(<span class="string">"Your input is "</span>+Text1.value);</span><br><span class="line">    &#125;	</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>c) set the event attribute for certain elements:</p>
<pre><code class="lang-javascript">   &lt;script for=&quot;document&quot; event=&quot;oncontextmenu&quot;&gt; 
   window.event.returnValue = false;
   document.oncontextmenu = hideContextMenu2;
   &lt;/script&gt;;
</code></pre>
</li>
</ul>
</li>
<li><p>time control:<code>to=window.setTimeout(&quot;func()&quot;,3000)</code>, <code>clearTimeout(to)</code>, <code>tm=setInterval(&quot;func()&quot;,1000)</code>, <code>clearInterval(tm)</code></p>
</li>
<li><p>redirect href location or set anchor point:<code>window.location.href=&quot;www.baidu.com&quot;</code>, <code>&lt;a name=&quot;anchor1&quot;&gt;&lt;/a&gt;</code></p>
</li>
<li><p>history: <code>history.back()</code>,  <code>history.forward()</code>, <code>history.go(n)</code>, n&gt;0 means go back n pages, otherwise go forward n pages. </p>
</li>
<li><p>cookie contains key-value pairs: <code>document.cookie=&quot;user=newly;passwd=hehe;&quot;</code></p>
</li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>HTML</category>
      </categories>
      <tags>
        <tag>HTML</tag>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title>Call C++ in Python</title>
    <url>/2022/06/16/programming_language/cross-language/Call%20C++%20in%20Python/</url>
    <content><![CDATA[<h3 id="1-ctype"><a href="#1-ctype" class="headerlink" title="1.ctype"></a>1.ctype</h3><p>similar as in Python_call_C.md<br>add extern “C” before each function, otherwise an error ‘undefined symbol’ will be thrown.</p>
<p>However, something might go wrong on terms with complicated Class.</p>
<h3 id="2-Use-Boost-Python"><a href="#2-Use-Boost-Python" class="headerlink" title="2.Use Boost.Python"></a>2.Use Boost.Python</h3><p>more powerful and complicated</p>
]]></content>
      <categories>
        <category>programming language</category>
        <category>cross-language</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Call C in Python</title>
    <url>/2022/06/16/programming_language/cross-language/Call%20C%20in%20Python/</url>
    <content><![CDATA[<p><strong>C:</strong> build a library<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gcc -c -fPIC libtest.c</span><br><span class="line">gcc -shared libtest.o -o libtest.so</span><br></pre></td></tr></table></figure></p>
<p><strong>Python:</strong> load the library<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> ctypes <span class="keyword">import</span> *</span><br><span class="line">libtest = cdll.LoadLibrary(libtest.so<span class="string">')</span></span><br><span class="line"><span class="string">print libtest.func(arg_list)</span></span><br></pre></td></tr></table></figure></p>
<p><a href="\code\call_C_in_Python\Makefile">Makefile sample</a> and <a href="\code\call_C_in_Python\sample.py">python sample code</a></p>
<h3 id="c-void-p"><a href="#c-void-p" class="headerlink" title="c_void_p"></a>c_void_p</h3><p>For the types without need to know the detailed layout, we can just use <code>c_void_p</code>, especially when we cannot find the strictly matched self-defined type such as <code>LP_cfloat_1024</code>.</p>
<h3 id="pointer-POINTER-byref"><a href="#pointer-POINTER-byref" class="headerlink" title="pointer, POINTER, byref"></a>pointer, POINTER, byref</h3><p>POINTER is used for defining type.</p>
<p>pointer and byref function similarly. However, pointer does a lot more work since it constructs a real pointer object, so it is faster to use byref if you don’t need the pointer object in Python itself.</p>
<h3 id="memory-issue"><a href="#memory-issue" class="headerlink" title="memory issue:"></a>memory issue:</h3><p>write free function in C code<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">freeme</span><span class="params">(<span class="keyword">char</span> *ptr)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"freeing address: %p\n"</span>, ptr);</span><br><span class="line">    <span class="built_in">free</span>(ptr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>call free function in Python<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lib = cdll.LoadLibrary(<span class="string">'./string.so'</span>)</span><br><span class="line">lib.freeme.argtypes = c_void_p,</span><br><span class="line">lib.freeme.restype = <span class="keyword">None</span></span><br><span class="line">lib.freeme(ptr)</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>programming language</category>
        <category>cross-language</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title>Caffe</title>
    <url>/2022/06/16/programming_language/Caffe/Caffe/</url>
    <content><![CDATA[<p><strong>Fundamental Stuff:</strong></p>
<ol>
<li><a href="http://caffe.berkeleyvision.org/tutorial/" target="_blank" rel="noopener">Caffe Tutorial</a></li>
<li><a href="https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.p" target="_blank" rel="noopener">Caffe Official Slides</a></li>
</ol>
<p><strong>Key Notes:</strong></p>
<p>Required Packages:</p>
<ol>
<li><p><a href="http://www.nvidia.com/object/cuda_home_new.html" target="_blank" rel="noopener">CUDA</a></p>
</li>
<li><p><a href="http://opencv.org/" target="_blank" rel="noopener">OpenCV</a></p>
</li>
<li><p><a href="http://www.netlib.org/blas/" target="_blank" rel="noopener">BLAS</a>: (Basic Linear Algebra Subprograms)<br>operations like matrix multiplication, matrix addition,<br>both implementation for CPU(cBLAS) and GPU(cuBLAS).<br>provided by MKL(INTEL), ATLAS, openBLAS, etc. </p>
</li>
<li><p><a href="http://www.boost.org/" target="_blank" rel="noopener">Boost</a>: a c++ library, use some of its math functions and shared_pointer.</p>
</li>
<li><p><a href="https://github.com/google/glog" target="_blank" rel="noopener">glog</a>,<a href="http://gflags.github.io/gflags/" target="_blank" rel="noopener">gflags</a>:provide logging &amp; command line utilities. Essential for debugging. </p>
</li>
<li><p><a href="https://github.com/google/leveldb" target="_blank" rel="noopener">leveldb</a><a href="http://symas.com/mdb/" target="_blank" rel="noopener">lmdb</a>: database io for your program. Need to know this for preparing your own data.</p>
</li>
<li><p><a href="https://github.com/google/protobuf" target="_blank" rel="noopener">protobuf</a>:  an efficient and flexible way to define data structure. Need to know this for defining new layers.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
      </tags>
  </entry>
  <entry>
    <title>Boost_tokenizer</title>
    <url>/2022/06/16/programming_language/C++/boost/Boost_tokenizer/</url>
    <content><![CDATA[<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt; boost/tokenizer.hpp&gt;</span></span></span><br></pre></td></tr></table></figure>
<ol>
<li><p>split string: default space</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">string</span> s = <span class="string">"This is,  a test"</span>;  </span><br><span class="line">tokenizer&lt;&gt; tok(s);  </span><br><span class="line"><span class="keyword">for</span>(tokenizer&lt;&gt;::iterator beg=tok.begin(); beg!=tok.end();++beg)</span><br><span class="line">&#123;  </span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; *beg &lt;&lt; <span class="string">"\n"</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>split string: drop delimiter</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> boost::tokenizer&lt;boost::char_separator&lt;<span class="keyword">char</span>&gt;&gt;  tokenizer;</span><br><span class="line">boost::char_separator&lt; <span class="keyword">char</span>&gt; sep(<span class="string">"-;|"</span>);</span><br><span class="line"><span class="function">tokenizer <span class="title">tokens</span><span class="params">(str, sep)</span></span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>split string: drop delimiter and keep delimiter</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> boost::tokenizer&lt; boost::char_separator&lt; <span class="keyword">char</span>&gt; &gt; tokenizer;</span><br><span class="line">boost::char_separator&lt; <span class="keyword">char</span>&gt; sep(<span class="string">"-;"</span>, <span class="string">"|"</span>, boost::keep_empty_tokens);</span><br><span class="line"><span class="function">tokenizer <span class="title">tokens</span><span class="params">(str, sep)</span></span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>special kinds of tokenizer</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> offsets[] = &#123;<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>&#125;;   <span class="comment">//three segment length  </span></span><br><span class="line"><span class="function">offset_separator <span class="title">f</span><span class="params">(offsets, offsets+<span class="number">3</span>)</span></span>;  </span><br><span class="line">tokenizer&lt; offset_separator&gt; tok(s,f);</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Boost_regex</title>
    <url>/2022/06/16/programming_language/C++/boost/Boost_regex/</url>
    <content><![CDATA[<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt; boost/regex.hpp&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> boost;</span><br></pre></td></tr></table></figure>
<ol>
<li><p>regex_match</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">regex <span class="title">expression</span><span class="params">(<span class="string">"^select ([a-zA-Z]*) from ([a-zA-Z]*)"</span>)</span></span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">string</span> in=<span class="string">"select name from table"</span>;</span><br><span class="line">cmatch what;</span><br><span class="line"><span class="keyword">if</span>(regex_match(in.c_str(), what, expression))</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt; what.size();i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">cout</span>&lt;&lt; what[i].first&lt;&lt; <span class="string">"|"</span>&lt;&lt;what[i].second&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span>&lt;&lt; <span class="string">"str :"</span>&lt;&lt; what[i].str()&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>regex_search</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">boost::<span class="function">regex <span class="title">r</span><span class="params">(<span class="string">"(a+)"</span>)</span></span>;</span><br><span class="line"><span class="built_in">string</span> content = <span class="string">"bbbaaaaacccaaaaddddaaaeeeaaa"</span>;</span><br><span class="line">boost::smatch m;</span><br><span class="line"><span class="built_in">string</span>::const_iterator strstart = content.begin();</span><br><span class="line"><span class="built_in">string</span>::const_iterator strend = content.end();</span><br><span class="line"><span class="keyword">while</span>(boost::regex_search(strstart, strend, m, r))</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//0 for the whole, m[i] is the i-th group</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; m[<span class="number">0</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="comment">//m[i].first and m[i].second stands for the begin idx and end idx</span></span><br><span class="line">    strstart = m[<span class="number">0</span>].second;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Note <code>regex_match(in.c_str(), what, expression)</code> is for complete match while <code>regex_match(in.c_str(), what, expression)</code> is for incomplete match.<br> <code>regex_match(in.c_str(), expression, target_exp).</code></p>
</li>
<li><p>special setting for regex</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">boost::<span class="function">regex <span class="title">e2</span><span class="params">(my_expression, boost::regex::icase)</span></span>; \\<span class="keyword">case</span> insensitive</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Boost_lexical_cast</title>
    <url>/2022/06/16/programming_language/C++/boost/Boost_lexical_cast/</url>
    <content><![CDATA[<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt; boost/lexical_cast.hpp&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> boost;</span><br></pre></td></tr></table></figure>
<ol>
<li><p>conversion between number and string: replace the atoi, itoa, and etc</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cout</span>&lt;&lt; lexical_cast&lt; <span class="built_in">string</span>&gt;(i)&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt; lexical_cast&lt; <span class="keyword">double</span>&gt;(s)&lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>mixed types: for type generalization</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt; boost/fusion/adapted/boost_tuple.hpp&gt;</span></span></span><br><span class="line">boost::tuple&lt;<span class="keyword">char</span>, <span class="keyword">int</span>, <span class="keyword">char</span>, <span class="keyword">int</span>&gt; decim(<span class="string">'-'</span>, <span class="number">10</span>, <span class="string">'e'</span>, <span class="number">5</span>);</span><br><span class="line">assert(stringize(decim) == <span class="string">"-10e5"</span>);</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Boost_installation</title>
    <url>/2022/06/16/programming_language/C++/boost/Boost_installation/</url>
    <content><![CDATA[<p><strong>Problem:</strong> When installing boost, “cl not found”. Or unresolved reference.</p>
<p><strong>Solution:</strong> cd Microsoft Visual Studio 11.0\VC, run <code>vcvarsall.bat amd64</code>. add Microsoft Visual Studio 11.0\VC\bin into environmental Path. Refer to <a href="http://www.boost.org/doc/libs/1_59_0/more/getting_started/windows.html" target="_blank" rel="noopener">http://www.boost.org/doc/libs/1_59_0/more/getting_started/windows.html</a> for installation details. </p>
<p>Note that 32 bit and 64 bit should be treated differently when using b2.exe. For 32 bit, run <code>b2 toolset=msvc-11.0 --build-type=complete --libdir=C:\Boost\lib\i386 stage</code>. For 64 bit, run <code>b2 toolset=msvc-11.0 --build-type=complete --libdir=C:\Boost\lib\x64 architecture=x86 address-model=64 stage</code>. </p>
<p>To put it simple, download binary files from <a href="http://sourceforge.net/projects/boost/files/boost-binaries/" target="_blank" rel="noopener">http://sourceforge.net/projects/boost/files/boost-binaries/</a>. Boost is auto-link which means you don’t need to add all lib files manually.</p>
<hr>
<p>If you have installed boost, using it in a VC project just takes two steps:</p>
<ol>
<li><p>C/C++-&gt;General: Additional Include Directories D:\Program_Files\boost_1_59_0_binary\boost;</p>
</li>
<li><p>Linker-&gt;General: Additional Library Directories D:\Program_Files\boost_1_59_0_binary\lib64-msvc-11.0;</p>
</li>
<li><p>Add D:\Program_Files\boost_1_59_0_binary\lib64-msvc-11.0 in the environmental variable Path.</p>
</li>
</ol>
<p>Notes:</p>
<ol>
<li>x64 and x86 conflicts: Linker-&gt;advanced-&gt;target machine</li>
<li>build-&gt;configuration manager</li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Boost_file_system</title>
    <url>/2022/06/16/programming_language/C++/boost/Boost_file_system/</url>
    <content><![CDATA[<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt; boost/filesystem.hpp&gt;  </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> boost::filesystem;</span><br></pre></td></tr></table></figure>
<ol>
<li><p>file name operation    </p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//Note the difference between /= and +=, /= means appending directory， += means string concatenation </span></span><br><span class="line"><span class="function">path <span class="title">dir</span><span class="params">(<span class="string">"C:\\Windows"</span>)</span></span>;</span><br><span class="line">dir /= <span class="string">"System32"</span>;       </span><br><span class="line">dir /= <span class="string">"services.exe"</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; dir.<span class="built_in">string</span>() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;           </span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; dir.parent_path()&lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;        /C:\Windows\System32</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; dir.filename()&lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;           /services.exe</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; dir.stem()&lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;               <span class="comment">//services</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; dir.extension()&lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;          <span class="comment">//.exe</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>file operation: rename, remove, copy</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">exists(path);                               </span><br><span class="line">is_directory(path); </span><br><span class="line">is_regular_file(path);                    </span><br><span class="line">                                          </span><br><span class="line">remove_all(<span class="keyword">const</span> Path&amp; p);  <span class="comment">//remove all files recursively                                                              </span></span><br><span class="line">rename(<span class="keyword">const</span> Path1&amp; from_p, <span class="keyword">const</span> Path2&amp; to_p);                        </span><br><span class="line">copy_file(<span class="keyword">const</span> Path1&amp; from_fp, <span class="keyword">const</span> Path2&amp; to_fp);                    </span><br><span class="line">create_directory(<span class="keyword">const</span> Path &amp; p);</span><br><span class="line">create_directories(<span class="keyword">const</span> Path &amp; p); <span class="comment">//make directory recursively&lt;/pre&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>shallow visit directory</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">path <span class="title">dir2</span><span class="params">(<span class="string">"c:\\Windows\\System32"</span>)</span></span>;</span><br><span class="line">directory_iterator end;</span><br><span class="line"><span class="keyword">for</span> (directory_iterator pos(dir2); pos != end; pos++)</span><br><span class="line">&#123;</span><br><span class="line">	<span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; *pos &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>deep visit directory: like DFS, stack push&amp;&amp;pop</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> recursive_directory_iterator rd_iterator;</span><br><span class="line"><span class="function">path <span class="title">dir2</span><span class="params">(<span class="string">"E:\\Student"</span>)</span></span>;</span><br><span class="line">rd_iterator end;</span><br><span class="line"><span class="keyword">for</span> (rd_iterator pos(dir); pos != end; pos++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (is_directory(*pos) &amp;&amp; pos.level() &gt; <span class="number">4</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        pos.no_push();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (*pos == <span class="string">"nofind.txt"</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        pos.pop();</span><br><span class="line">    &#125;	</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt; pos-&gt;path().filename().<span class="built_in">string</span>()&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>catch error</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function">path <span class="title">dir2</span><span class="params">(<span class="string">"c:\\Windows\\System32"</span>)</span></span>;</span><br><span class="line">    assert(is_directory(dir2));          </span><br><span class="line">    assert(exists(dir2));          </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span>(filesystem_error&amp; e)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; e.path1() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; e.what() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Boost_array</title>
    <url>/2022/06/16/programming_language/C++/boost/Boost_array/</url>
    <content><![CDATA[<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"boost/array.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"boost/multi_array.hpp"</span></span></span><br></pre></td></tr></table></figure>
<ol>
<li><p>Array: similar with STL vector</p>
 <figure class="highlight"><table><tr><td class="code"><pre><span class="line">array&lt; std::string, 3&gt; a&lt;/pre&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Multi-Array: refer to <a href="http://www.boost.org/doc/libs/1_59_0/libs/multi_array/doc/user.html" target="_blank" rel="noopener">this</a> for more details, similar with matlab matrix operation</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> boost::multi_array&lt;<span class="keyword">double</span>, <span class="number">3</span>&gt; array_type;</span><br><span class="line"><span class="function">array_type <span class="title">A</span><span class="params">(boost::extents[<span class="number">3</span>][<span class="number">4</span>][<span class="number">2</span>])</span></span>;</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Android</title>
    <url>/2022/06/16/programming_language/Android/Android/</url>
    <content><![CDATA[<h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation:"></a>Installation:</h3><ol>
<li><p>Download and install <a href="https://developer.android.com/sdk/index.html" target="_blank" rel="noopener">SDK</a></p>
</li>
<li><p>Install via SDK manager and AVD manager</p>
</li>
<li><p>Add  <font color="green">C:\Program Files (x86)\Android\android-sdk\tools</font> to environment path</p>
</li>
<li><p>Install ADT plugin in Eclipse by entering “ADT Plugin” for the Name and the following URL for the Location: <a href="https://dl-ssl.google.com/android/eclipse/" target="_blank" rel="noopener">https://dl-ssl.google.com/android/eclipse/</a></p>
</li>
<li><p>The first time to lauch AVD may take rather long time, just be patient. Leave the launched AVD open until your project is finished.</p>
</li>
</ol>
<p><strong>Possible problems during installation:</strong></p>
<ol>
<li><p>java basis is unresolved: Project-&gt;Properties-&gt;Java Build Path-&gt;Libraries, add JRE lib</p>
</li>
<li><p>Android is unresolved: Project-&gt;Properties-&gt;Android, choose project build target and apply. For some Eclipse versions, clean and rebuild.</p>
</li>
</ol>
<h3 id="Resource"><a href="#Resource" class="headerlink" title="Resource:"></a>Resource:</h3><ol>
<li><p><a href="http://developer.android.com/reference/packages.html" target="_blank" rel="noopener">Android API</a> </p>
</li>
<li><p><a href="http://zetcode.com/mob/android/" target="_blank" rel="noopener">http://zetcode.com/mob/android/</a></p>
</li>
<li><p><a href="https://www.embeddedlinux.org.cn/AndroidEssentials/" target="_blank" rel="noopener">https://www.embeddedlinux.org.cn/AndroidEssentials/</a></p>
</li>
<li><p>Begining Android2: code is downloaded from <a href="https://www.apress.com" target="_blank" rel="noopener">https://www.apress.com</a></p>
</li>
</ol>
<h3 id="Application"><a href="#Application" class="headerlink" title="Application:"></a>Application:</h3><p><code>Application</code> consists of <code>Activity</code>, <code>Service</code>, <code>BroadcastReceiver</code>, and <code>Content Provider</code>, in which <code>Activity</code> is a regular App, <code>Service</code> can run independently in the background, such as MP3 player, <code>BroadcastReceiver</code> reacts to outcoming events, <code>Content Provider</code> save data via SQLite or share data with other applications.</p>
<ol>
<li><p><font color="red">activity</font>: Most common application. <a href="\code\android\AndroidManifest_activity.xml">[AndroidManifest example]</a> <a href="http://bcmi.sjtu.edu.cn/~niuli/github_images/BPUcRFH.jpg" target="_blank" rel="noopener">[activity cycle]</a></p>
</li>
<li><p><font color="red">service</font>: Similar with activity but without user interface.   <a href="\code\android\AndroidManifest_service.xml">[AndroidManifest example]</a>    <a href="http://bcmi.sjtu.edu.cn/~niuli/github_images/cira5BF.jpg" target="_blank" rel="noopener">[service cycle]</a></p>
<ul>
<li><p>independent service</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">final Intent intent = new Intent();</span><br><span class="line">intent.setAction(&quot;org.crazyit.service.FIRST_SERVICE&quot;);</span><br><span class="line">startService(intent);</span><br><span class="line">stopService(intent);</span><br></pre></td></tr></table></figure>
</li>
<li><p>service has connection with activity via <code>IBinder</code></p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BindService.MyBinder binder;</span><br><span class="line">private ServiceConnection conn = new ServiceConnection()&#123;</span><br><span class="line">	public void onServiceConnected(ComponentName name, IBinder service)&#123;</span><br><span class="line">		binder = (BindService.MyBinder) service; </span><br><span class="line">	&#125;</span><br><span class="line">	public void onServiceDisconnected(ComponentName name)&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line">bindService(intent, conn, Service.BIND_AUTO_CREATE);</span><br><span class="line">unbindService(conn);</span><br></pre></td></tr></table></figure>
</li>
<li><p>to create a new thread, extend class <code>IntentService</code></p>
</li>
<li><p>for interprocess communication, use AIDL, i.e., extends <code>Stub</code> as IBinder interface. The others are the same as <code>Service</code>.</p>
</li>
</ul>
</li>
<li><p><font color="red">receiver</font>: a global listener. <a href="\code\android\AndroidManifest_receiver.xml">AndroidManifest example</a></p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Intent intent = new Intent();</span><br><span class="line">intent.setAction(&quot;org.crazyit.action.CRAZY_BROADCAST&quot;);</span><br><span class="line">intent.putExtra(&quot;msg&quot;, &quot;hello world&quot;);</span><br><span class="line">sendBroadcast(intent);</span><br></pre></td></tr></table></figure>
<p>use IntentFilter to filter the received broadcast</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">IntentFilter filter = new IntentFilter();</span><br><span class="line">filter.addAction(MusicBox.CTL_ACTION);</span><br><span class="line">registerReceiver(serviceReceiver, filter);</span><br></pre></td></tr></table></figure>
</li>
<li><p><font color="red">provider</font>: globally share data. <a href="\code\android\AndroidManifest_provider.xml">AndroidManifest example</a></p>
<p> The joint point is Uri:  <code>content://org.crazyit.providers.dictprovider/words/id</code></p>
<p> ContentProvider: an extended class <code>public class DictProvider extends ContentProvider</code> which provides <code>query</code>, <code>insert</code>, <code>delete</code>, <code>update</code> functions.</p>
<p> ContentResolver: </p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ContentResolver contentResolver = getContentResolver();</span><br><span class="line">contentResolver.insert(Words.Word.DICT_CONTENT_URI, values);</span><br></pre></td></tr></table></figure>
<p> Most system applications provide their own ContentProvider, we need to know their Uri. </p>
</li>
</ol>
<h3 id="Adapter"><a href="#Adapter" class="headerlink" title="Adapter:"></a>Adapter:</h3><p>Adapters are the link between a set of data (ArrayList, Cursor) and the AdapterView (ListView, GridView, Spinner) that displays the data. AdapterViews are ViewGroups that display child views given to it by an adapter.</p>
<ol>
<li><p>ArrayAdapter: </p>
<p> In the simplest case, just use the String array <code>items</code></p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ArrayAdapter&lt;String&gt;aa = new ArrayAdapter&lt;String&gt;(this, adnroid.R.layout.simple_spinner_item, items);</span><br><span class="line">aa.setDropDownViewResource(android.R.layout.simple_spinner_dropdown_item);</span><br><span class="line">spin.setAdapter(aa);</span><br></pre></td></tr></table></figure>
<p> Extend ArrayAdapter to satisfy your own needs,</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MyArrayAdapter adapter = new MyArrayAdapter(this, R.layout.my_list_view, MyObjectItemData);</span><br><span class="line">ListView listViewItems = new ListView(this);</span><br><span class="line">listViewItems.setAdapter(adapter);</span><br><span class="line">listViewItems.setOnItemClickListener(new OnItemClickListenerListViewItem());</span><br></pre></td></tr></table></figure>
<p> The code for class <code>MyArrayAdapter</code> is <a href="\code\android\MyArrayAdapter.java">here</a>. The code for class <code>OnItemClickListenerListViewItem</code> is <a href="\code\android\OnItemClickListenerListViewItem.java">here</a>. Note here <code>inflate</code> is used to convert XML format to View. One trick is to check whether convertView is null so that duplicated reflation could be avoided, otherwise your device would be slowed down when crolling the screen. ConvertView can be bined with a newly defined class by using <code>setTag</code> and <code>getTag</code>. When the adapter is changed (e.g., add a new row), <code>adapter.notifyDataSetChanged()</code>. </p>
</li>
<li><p>CursorAdapter:</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SimpleCursorAdapter adapter=new SimpleCursorAdapter(this, R.layout.row, constantsCursor, new String[] &#123;&quot;tag&quot;, &quot;value&quot;&#125;, new int[] &#123;R.id,tag, R.id.value&#125;);</span><br><span class="line">listviewItems.setAdapter(adapter);</span><br></pre></td></tr></table></figure>
<p> Extend CursorAdapter, see <a href="\code\android\MyCursorAdapter.java">here</a></p>
</li>
</ol>
<h3 id="XML-format"><a href="#XML-format" class="headerlink" title="XML format:"></a>XML format:</h3><ol>
<li><p>Use XML to format Layout: In XML file, <code>android:id=&quot;@+id/button</code>. In Java, <code>btn=(Button)findViewById(R.id.button)</code>. Similarly,  in XML file, <code>android:id=&quot;@string/app_name</code>, in Java, <code>Resources myres = getResources(); myres.getString();</code>.</p>
</li>
<li><p>Use XML to format componenet style: in res\values\<a href="\code\android\text_style.xml">text_style.xml</a>,  in \res\layout\main.xml, <code>style=&quot;@style/style2&quot;</code>. </p>
</li>
<li><p>Use XML to format window style: in res\values\<a href="\code\android\window_style.xml">window_style.xml</a>, in <code>onCreate()</code> function, add <code>setTheme(R.style.crazytheme)</code>, or <code>&lt;application android:theme=&quot;@style/crazytheme&quot;&gt;</code>.</p>
</li>
<li><p>Use XML to format menu style: \res\menu\<a href="\code\android\contextmenu_style.xml">contextmenu_style.xml</a> <a href="\code\android\optionmenu_style.xml">optionmenu_style.xml</a>. </p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MenuInflater inflator = new MenuInflater(this);</span><br><span class="line">inflator.inflate(R.menu.my_menu, menu);</span><br><span class="line">//return super.onCreateOptionsMenu(menu);</span><br></pre></td></tr></table></figure>
</li>
<li><p>Use XML to inflate a view and thus format dialogue style, click <a href="\code\android\login.xml">login.xml</a></p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TableLayout loginForm = (TableLayout)getLayoutInflater().inflate(R.layout.login, null);</span><br><span class="line">ad.setView(loginForm);</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Event"><a href="#Event" class="headerlink" title="Event:"></a>Event:</h3><ol>
<li><p><font color="red">Add EventListener</font>: there exist several types of implement. We take the most common event “click” as an example to demonstrate five types of implements.</p>
<ul>
<li><p>Override function <code>onClick</code> in class <code>OnClickListener</code></p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">	button1.setOnClickListener(new OnClickListener()&#123; </span><br><span class="line">	    public void onClick(View v)&#123;</span><br><span class="line">	&#125;);</span><br><span class="line">	```	</span><br><span class="line">* Extend class `OnClickListener`, essentially equal to a)</span><br><span class="line">	```android</span><br><span class="line">	private Button1_OnClickListener mListener1 = new Button1_OnClickListener();</span><br><span class="line">	class Button1_OnClickListener implements OnClickListener &#123; </span><br><span class="line">		public void onClick(View v) &#123; </span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	mButton1.setOnClickListener(mListener1);</span><br></pre></td></tr></table></figure>
</li>
<li><p>Bind event listener in XML</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">android:onClick=&quot;clickHandler&quot;</span><br><span class="line"></span><br><span class="line">function clickHandler()&#123;</span><br></pre></td></tr></table></figure>
<p>For key event:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">new OnKeyListener()&#123;</span><br><span class="line">	public boolean onKey(View source, int keyCode, KeyEvent event&#123;</span><br><span class="line">		switch(event.getKeyCode())</span><br></pre></td></tr></table></figure>
<p>For long click:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">new OnLongClickListener&#123;</span><br><span class="line">	public boolean onLongClick(View source)&#123;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><font color="red">Override Recall Function</font>: extend View or Activity class to override event recall functions.</p>
<pre><code>* Extends Activity Class 
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class TestEvent2 extends Activity implements OnClickListener&#123;</span><br><span class="line">	public void onClick(View v) &#123; 			</span><br><span class="line"></span><br><span class="line">public class TestEvent2 extends Activity&#123; </span><br><span class="line">	boolean onKeyDown(int keycode, KeyEvent event)&#123;</span><br></pre></td></tr></table></figure>
</code></pre><ul>
<li>Extends View Class  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class MyButton extends Button&#123;</span><br><span class="line">	public boolean onKeyDown(int keyCode, KeyEvent event)&#123;</span><br><span class="line"></span><br><span class="line">public class DrawView extends View&#123;</span><br><span class="line">	public boolean onTouchEvent(MotionEvent event)&#123;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<h4 id="Layout"><a href="#Layout" class="headerlink" title="Layout:"></a>Layout:</h4><ol>
<li><p>Common attributes</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">android:layout_width|height=&quot;fill_parent&quot;|&quot;wrap_content&quot;|&quot;10dip&quot;	</span><br><span class="line">	android:gravity=&quot;center_horizontal&quot;|&quot;right&quot;</span><br><span class="line">	android:background=&quot;#FF909090&quot;</span><br><span class="line">	android:padding=&quot;3dip&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>FrameLayout: fundamental layout, allow overlap or replacement for flexible usage.</p>
</li>
<li><p>LinearLayout: <a href="\code\android\linear.xml">example code</a>    </p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">android:layout_weight=&quot;1&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>AbsoluteLayout: <a href="\code\android\absolute.xml">example code</a></p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">android:layout_x|layout_y=&quot;250dip&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>RelativeLayout: <a href="\code\android\relative.xml">example code</a></p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">android:layout_below=&quot;@id/label&quot;</span><br><span class="line">android:layout_toLeftOf=&quot;@id/ok&quot;</span><br><span class="line">android:layout_alignParentRight=&quot;true&quot;</span><br><span class="line">android:layout_alignTop=&quot;@id/ok&quot;</span><br><span class="line">android:layout_marginLeft=&quot;10dip&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>TableLayout: <a href="\code\android\table.xml">example code</a></p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">android:layout_span=&quot;3&quot;</span><br><span class="line">android:layout_column=&quot;2&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Make the layout scrollable, add <code>&lt;ScrollView&gt;</code> as in <a href="\code\android\scroll.xml">example code</a></p>
</li>
</ol>
<h3 id="Componenet"><a href="#Componenet" class="headerlink" title="Componenet:"></a>Componenet:</h3><ol>
<li><p>EditText | TextView <a href="\code\android\TextView.java">example code</a> </p>
</li>
<li><p>RadioGroup <a href="\code\android\RadioGroupDemo.java">example code</a> </p>
</li>
<li><p>CheckBox <a href="\code\android\CheckBoxDemo.java">example code</a></p>
</li>
<li><p>Spinner: dropdown menu <a href="\code\android\SpinnerDemo.java">example code</a></p>
</li>
<li><p>ListView: <a href="\code\android\ListViewDemo.java">example code</a></p>
</li>
<li><p>GridView: <a href="\code\android\GridDemo.java">example code</a></p>
</li>
<li><p>OptionsMenu: in the corner of screen <a href="\code\android\OptionsMenu.java">example code</a></p>
</li>
<li><p>ContextMenu: long press to show the menu. <a href="\code\android\MenuDemo.java">example code</a>.</p>
</li>
</ol>
<h3 id="Advanced-Componenet"><a href="#Advanced-Componenet" class="headerlink" title="Advanced Componenet:"></a>Advanced Componenet:</h3><ol>
<li><p><font color="red">Picker</font>: The DatePicker code is as follows and the TimePicker code is similar.</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Calendar dateAndTime=Calendar.getInstance();</span><br><span class="line">DatePickerDialog.OnDateSetListener mydiag=new DatePickerDialog.OnDateSetListener() &#123;</span><br><span class="line">	public void onDateSet(DatePicker view, int year, int monthOfYear,int dayOfMonth) &#123;</span><br><span class="line">		dateAndTime.set(Calendar.YEAR, year);</span><br><span class="line">		dateAndTime.set(Calendar.MONTH, monthOfYear);</span><br><span class="line">		dateAndTime.set(Calendar.DAY_OF_MONTH, dayOfMonth);</span><br><span class="line">		updateLabel();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Button btn=(Button)findViewById(R.id.dateBtn);</span><br><span class="line">	</span><br><span class="line">btn.setOnClickListener(new View.OnClickListener() &#123;</span><br><span class="line">	public void onClick(View v) &#123;</span><br><span class="line">		new DatePickerDialog(ChronoDemo.this, mydiag, year, month, day).show();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>Number picker</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NumberPicker np = (NumberPicker) findViewById(R.id.npId);</span><br><span class="line"></span><br><span class="line">np.setOnValueChangedListener(new OnValueChangeListener()</span><br><span class="line">&#123;</span><br><span class="line">    public void onValueChange(NumberPicker picker, int oldVal, int newVal)</span><br><span class="line">    &#123;</span><br><span class="line">        tv.setText(String.valueOf(newVal)); </span><br><span class="line">    &#125;        </span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">np.setMaxValue(100);</span><br><span class="line">np.setMinValue(0);</span><br></pre></td></tr></table></figure>
</li>
<li><p>Toast</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import android.widget.Toast;</span><br><span class="line"></span><br><span class="line">Toast.makeText(Demo.this, &quot;warning msg&quot;, Toast.LENGTH_LONG).show();</span><br></pre></td></tr></table></figure>
</li>
<li><p>Notification <a href="\code\android\Notification.java">example code</a></p>
</li>
<li><p>Alert dialog <a href="\code\android\Dialog.java">example code</a></p>
</li>
<li><p>Dial phone number</p>
<ul>
<li><p>add permission in AndroidManifest.xml <code>&lt;uses-permission android:name=&quot;android.permission.CALL_PHONE&quot;&gt;&lt;/uses-permission&gt;</code></p>
</li>
<li><p>java code</p>
   <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Intent phoneIntent = new Intent(&quot;android.intent.action.CALL&quot;, Uri.parse(&quot;tel:&quot; + inputStr));</span><br><span class="line">startActivity(phoneIntent);</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Send message</p>
<ul>
<li><p>add permission in AndroidManifest.xml <code>&lt;uses-permission android:name=&quot;android.permission.SEND_SMS&quot;/&gt;</code></p>
</li>
<li><p>java code</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SmsManager smsManager = SmsManager.getDefault();</span><br><span class="line">PendingIntent sentIntent = PendingIntent.getBroadcast(act, 0, new Intent(), 0);</span><br><span class="line">smsManager.sendTextMessage(addressStr, null,contentStr, sentIntent, null);</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Gallary <a href="\code\android\Gallary.java">example code</a></p>
</li>
<li><p>Timer</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">new Timer().schedule(new TimerTask()</span><br><span class="line">&#123;</span><br><span class="line">	public void run()</span><br><span class="line">	&#123;</span><br><span class="line">		handler.sendEmptyMessage(0x123);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;, 0, 200);</span><br><span class="line"></span><br><span class="line">Handler handler = new Handler()</span><br><span class="line">&#123;</span><br><span class="line">	public void handleMessage(Message msg)</span><br><span class="line">	&#123;</span><br><span class="line">		if (msg.what == 0x123)&#123;</span><br><span class="line">		&#125;</span><br><span class="line">		super.handleMessage(msg);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Gist"><a href="#Gist" class="headerlink" title="Gist"></a>Gist</h2><ol>
<li><p><code>&lt;uses-sdk android:minSdkVersion=&quot;2&quot; /&gt;</code> indicates the minimum requirement of SDK. <code>&lt;uses-permission android:name=&quot;android.permission.SEND_SMS&quot; /&gt;</code> indicates the required permission.</p>
</li>
<li><p>To utilize the resouce in res/values: in java code, , R.id….R.string….; in XML file, @id/…..@string/……</p>
</li>
<li><p>Set no title or fullscreen, add the following code in <code>AndroidManifest.xml</code></p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">	&lt;activity android:theme=&quot;@android:style/Theme.NoTitleBar.Fullscreen&quot;</span><br><span class="line">	```  </span><br><span class="line"></span><br><span class="line">1. There are two types of intents: explicit and implicit. In explicit intents you provide the name of the Activity class. In implicit intents, you tell the system what to do rather than name the Activity class to launch.</span><br><span class="line"></span><br><span class="line">## Game Development ##</span><br><span class="line"></span><br><span class="line">1. Prepare music and image resources uder folder &quot;res&quot;</span><br><span class="line"></span><br><span class="line">1. Use Handler in the MainActivity</span><br><span class="line">	```android</span><br><span class="line">	hd=new Handler()&#123;</span><br><span class="line">		public void handleMessage(Message msg)&#123;</span><br><span class="line">	    	super.handleMessage(msg);        		</span><br><span class="line">	    	switch(msg.what)</span><br></pre></td></tr></table></figure>
</li>
<li><p>MenuView</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class ViewMainMenu extends SurfaceView implements SurfaceHolder.Callback&#123;</span><br><span class="line">	public void onDraw(Canvas canvas)&#123;</span><br><span class="line">	public void surfaceCreated(SurfaceHolder holder) &#123;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Create on thread for each function</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class KeyThread extends Thread&#123;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Use GLRender for GameView, android opengl</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class MySurfaceView extends GLSurfaceView &#123;</span><br><span class="line">	public MySurfaceView(Context context) &#123;</span><br><span class="line">			super(context);</span><br><span class="line">	        mRenderer = new SceneRenderer();</span><br><span class="line">	        setRenderer(mRenderer);						</span><br><span class="line">	        setRenderMode(GLSurfaceView.RENDERMODE_CONTINUOUSLY);</span><br><span class="line">	&#125;	 </span><br><span class="line">	class SceneRenderer implements GLSurfaceView.Renderer&#123;</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>programming language</category>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Adroid</tag>
      </tags>
  </entry>
  <entry>
    <title>Simple Bayesian Optimization</title>
    <url>/2022/06/16/others/Simple%20Bayesian%20Optimization/</url>
    <content><![CDATA[<p>We release a light-weighted user-friendly Bayesian optimization tool for tuning hyper-parameters based on Gaussian profess. The code is modified based on <a href="https://github.com/fmfn/BayesianOptimization" target="_blank" rel="noopener">https://github.com/fmfn/BayesianOptimization</a> and released on <a href="https://github.com/ustcnewly/simple_Bayesian_optimization" target="_blank" rel="noopener">https://github.com/ustcnewly/simple_Bayesian_optimization</a>, in which README shows the dependency and usage of the tool. </p>
<ul>
<li><p>bo = BayesianOptimization(func, param_bound_dict): ‘func’ is a function with parameters as input and performance as output, ‘param_dict’ is a dictionary containing the upper/lower bound of each parameter.</p>
</li>
<li><p>bo.explore(param_value_dict, eager=True): ‘param_value_dict’ is a dictionary containing multiple parameter values. This function does no calculate the performance corresponding to the parameter values, does not add items into bo.X and bo.Y, only add new parameters. Do not forget to set eager as True to make the added parameters take effect.</p>
</li>
<li><p>bo.initialize(param_value_target_dict): compared with ‘param_value_dict’, ‘param_value_target_dict’ additionally contains the performance corresponding to the parameter values. This function does not add items into bo.X and bo.Y.</p>
</li>
<li><p>bo.maximize(init_points=5, n_iter=15, **kwargs): ‘init_points’ is to include extra points for fitting. ‘n_iter’ indicates the iterative process of inferring the next point and add it for fitting. All the init points and inferred points will be added into bo.X and bo.Y. The model is fitted on bo.X and bo.Y. </p>
<p>The algorithm requires at least two initial points. When setting ‘init_points=0’, we can use ‘bo.explore’ and ‘bo.initialization’ for initialization. </p>
<p>For inference, acq=’ucb’ (upper confidence bound), ‘ei’ (expected improvement) or ‘poi’ (probability of improvement). ‘poi’ and ‘ucb’ work better empirically. There is a trade-off beteween exploitation and exploration. When acq=’ucb’,  smaller kappa prefers exploitation while larger kappa prefers exploration. When acq=’poi’, similarly, smaller xi prefers exploitation while larger xi prefers exploration. kappa and xi can be set within [10^-3, 10^-2, …, 10^3].</p>
<p>For the Gaussian process model itself, it contains parameters like ‘kernel’ and ‘alpha’. Refer to <a href="http://scikit-learn.org/stable/modules/gaussian_process.html" target="_blank" rel="noopener">scikit-learn</a> for the details. When warnings occur in ‘gpr.py’, you can try using larger alpha, i.e., 10^-3. </p>
</li>
<li><p>bo.gp.fit(bo.X, bo.Y): use Gaussian process regression to fit bo.X and bo.Y.</p>
</li>
<li><p>mu, sigma = bo.gp.predict(x, return_std=True): use the learnt model to predict x, output mean and variance.</p>
</li>
<li><p>utility = bo.util.utility(test_params, bo.gp): return the utility of test parameters, based on which the next point is recommended.</p>
</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>hyper-parameter</tag>
      </tags>
  </entry>
  <entry>
    <title>Reasons to Reject a Paper</title>
    <url>/2022/06/16/others/Reasons%20to%20Reject%20a%20Paper/</url>
    <content><![CDATA[<p><strong>Strong reasons:</strong> </p>
<ol>
<li>Promise more than delivered (this is the first work….).</li>
<li>Miss important references (closely related).</li>
<li>Results are too incremental or too unconvincing, or lower than state-of-the-art.</li>
<li>Poorly written or oraganized.</li>
<li>Incorrect statements or statements without support.</li>
<li>Lack of component analysis for important components.</li>
</ol>
<p><strong>Weak reasons:</strong></p>
<ol>
<li>Novelty is minor or incremental. Just an extension of A, or similar to A, or combination of A and B.</li>
<li>Motivation is unclear or arguable.</li>
<li>No theoretical guarantee of the effectiveness.</li>
<li>No technical contribution: too few math formulations or the proposed method is too straightforward.</li>
<li>Formulations are too dense and hard to follow.</li>
<li>Paper writing has some flaws (e.g., ambiguity, redundancy, a few typos or grammar mistakes).</li>
<li>Improvement is not very significant.</li>
<li>Lack of component analysis for less important components.</li>
<li>Lack of qualitative analysis.</li>
<li>Unfair or insufficient comparison with state-of-the-art. Miss some baselines. Sometimes we need to create baselines if necessary.</li>
<li>Hyper-parameter analysis: too many hyper-parameters, unclear how to set hyper-parameter, sensitivity to hyper-parameters. </li>
<li>No significant test.</li>
<li>Miss some details (e.g., technical details or experimental details), not self-contained. </li>
<li>Miss less important references.</li>
<li>Miss analyses on time/memory/model complexity</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper Writing</title>
    <url>/2022/06/16/others/Paper%20Writing/</url>
    <content><![CDATA[<p>Paper is an information carrier to present and sell your work，so the paper should be well-written, well-organized, and easily understood. You should make the reviewer happy and buy your contribution.</p>
<h2 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h2><ul>
<li>Abstract is the extension of title.</li>
<li>Introduction is the extension of abstract.</li>
<li>Conclusion is the rewritten version of abstract.</li>
</ul>
<h2 id="Language"><a href="#Language" class="headerlink" title="Language"></a>Language</h2><ul>
<li>Make sure there is no typo and syntax mistake (you can use assistant tools like Grammarly <a href="https://app.grammarly.com/" target="_blank" rel="noopener">https://app.grammarly.com/</a> to assist you with this tedious job). </li>
<li>For technical term, unify them throughout the paper, use at most two sayings throughout the paper. For non-technical term, use at least two sayings alternatively throughout the paper and avoid simple words. Substitutions for common simple words can be found <a href="https://ustcnewly.github.io/2018/09/28/paper_note/Commonly%20Used%20Words%20in%20Paper%20Writing/">here</a>.</li>
<li>Your technical writing may look amateur and unprofessional. This problem should be addressed from the following two aspects: domain-invariant writing and domain-specific writing. Even if you have already written dozens of papers in one domain but plan to write a paper in another domain, you also need domain adaptation to fit the new domain.<ul>
<li><strong>Domain-invariant writing:</strong> read CV/ML papers in a wide range of fields and learn their paper writing. </li>
<li><strong>Domain-specific writing:</strong> read papers in the related narrow field annd learn their paper writing. Generally, before I write or revise a paper in a new field, I will read at least 10 recent papers and jot down the well-written key words/phrases as raw corpus. </li>
</ul>
</li>
<li>Merge fragmented short sentences into a coherent long sentence. Use connective words/phrases to glue neighboring sentences to make the transition smoother.</li>
</ul>
<h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><p>Writing paper is not randomly putting together what you have in mind. Intuitively, the left subimage is a poorly-written paper and the right one is a well-written paper. Imagine you can enter a cake store and the waitress serves you the left cake, the feeling is the same as that of reviewer when reading a poorly-written paper. The logic relation can be roughly categorized into “and”, “or”, “then”, “but”, “so”, “specifically”.</p>
<p>   <img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/MxjkrL0.jpg" width="30%"></p>
<p>   One paper should be well-structured, composed of hierarchical logic blocks. Writing paper is writing logic flow. Generally speaking, abstract is the extension of title, introduction is the extension of abstract, conclusion is rewritten abstract. In each section, the sentences should be clustered as hierarchical logic blocks in an agglomerative manner.</p>
<p>   <img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/ChreGuB.jpg" width="20%"></p>
<h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><ul>
<li><strong>Accuracy:</strong> Each sentence should exactly present what you want to present and cannot be ambiguous with multiple versions of interpretations.</li>
<li><strong>Completeness:</strong> Your paper should be self-explanatory. You have to ensure that readers do not have to read extra materials to understand your paper. With all necessary ingredients, the remaining problem is the struture issue: how to organize all these ingredients. </li>
<li><strong>Brevity:</strong> Remove redundant expressions and keep it simple. Brevity is the spirit.</li>
</ul>
<h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><ul>
<li>Classify existing works into several groups. For example, “the existing works can be roughly categorized into XXX, XXX, and XXX.”</li>
<li>Compared with each group of existing works, state your difference and advantage. For example, “All these methods XXX, but XXX. In contrast, our method XXX”.</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul>
<li>For baselines in experimental section, compare with state-of-the-art baselines and yourself (e.g., ablation study or component analysis) qualitatively or quantitatively. It would be great to offer in-depth qualitative analysis.</li>
<li>When comparing with state-of-the-art baselines, the most important thing is fair comparison. Check whether the experimental settings (e.g., train/test split, features, backbone network) are the same. If the results reported in original papers are from different experimental settings, you need to run their code or even implement their method by yourself. <ul>
<li>If one baseline uses more information and underperforms your method, that is OK.</li>
<li>If one baseline uses less information and outperforms your method, that is a serious problem.</li>
<li>If one baseline uses more information and outperforms your method, remove their extra information and compare with it again. Do not leave a better baseline using more information on your paper, which is extremely risky.</li>
<li>If one baseline uses less information and underperforms your method, see whether this baseline can use the extra information easily. If this baseline can incorporate the extra information in a simple way, compare with it with extra information again. </li>
</ul>
</li>
</ul>
<h2 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h2><ul>
<li>Sometimes when it is hard to describe something using natural language, try describing it using math.</li>
<li>For each variable, please define it before using it. Merge unncessarily duplicated variables and keep as few variables as possible. Think clearly about the meaning of each variable. Sometimes, a variable was defined a long time ago and you need to help reviewer recall that variable.</li>
<li>Math expressions should be regulated and rigorous.</li>
<li>For mathematically dense paper, it would be better to start with a general instruction before entering math. For example, “a vector/matrix is denoted by a lowercase/uppercase letter in boldface. The transpose of a vector/matrix is denoted by the superscript ‘. Moreover, $A^{-1}$ is used to denote the inverse matrix of A.<br>We use $\langle A,B\rangle$ (resp., $A\circ B$) to denote the inner product (resp., element-wise product) of two matrices”.</li>
</ul>
<h2 id="Show-and-Hide"><a href="#Show-and-Hide" class="headerlink" title="Show and Hide"></a>Show and Hide</h2><ul>
<li>For your contribution, you should highlight it multiple times (e.g., abstract, introduction, related work, method, experiment, and conclusion). Otherwise, the reviewer will claim your contribution is minor.</li>
<li>For your weakness, try to hide it if possible. If you could not hide it, provide a reasonable and compromising explanation.</li>
</ul>
<h2 id="Attack-and-defend"><a href="#Attack-and-defend" class="headerlink" title="Attack and defend"></a>Attack and defend</h2><p>Imagine you are the reviewer, what questions are you going to ask and how the paper could be defended.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper Proofread</title>
    <url>/2022/06/16/others/Paper%20Proofread/</url>
    <content><![CDATA[<p>Do not go over the paper once and for all. Go over the paper again and again to check each rule one by one, according to the following order: High-level 6 -&gt; High-level 1-5 -&gt; Low-level 1-3.</p>
<h2 id="Low-level"><a href="#Low-level" class="headerlink" title="Low-level"></a>Low-level</h2><ol>
<li><p><strong>Typo:</strong> Print out the paper and read the paper character by character (not word by word). You can use auxiliary tools (e.g., Grammarly) to help you check spelling and grammar mistakes. The easily made mistakes are including but not limited to: single/plural form, third-person singular, etc. When time is limited, check the most notable parts in the paper, e.g., figure and captions, table captions, section titles.</p>
</li>
<li><p><strong>Math annotation:</strong> a) All the variables are defined before used; b) Avoid repetitive use of the same symbol; c) Mind the difference between capital and small letters, boldface and plain font type; d) Unpaired brackets; e) Check the variables to be optimized; d) Punctuations (e.g., ‘,’ and ‘.’) in the formulas; e) Dimension mismatch; f) The value of each hyper-parameter is mentioned.</p>
</li>
<li><p><strong>Reference:</strong> a) no question mark ‘?’; b) Check the correctness of each reference including Citation [XXX], Table XXX, Figure XXX, Eqn. XXX, Section XXX. </p>
</li>
</ol>
<h2 id="High-level"><a href="#High-level" class="headerlink" title="High-level"></a>High-level</h2><ol>
<li><p><strong>Objective statement:</strong> Check whether each objective statement is accurate or unambiguous. Avoid obvious collision.</p>
</li>
<li><p><strong>Subjective statement:</strong> Check whether each subjective statement is appropriate. For example, “XXX cannot XXX”, “the first XXXX”, “all XXXX”. Avoid too strong subjective statement.</p>
</li>
<li><p><strong>Experimental results:</strong> Check whether your experimental results are reasonable. For example, A is quite similar to B, but far better/worse than B. A uses more information than B, but underperforms B.</p>
</li>
<li><p><strong>Experimental observation:</strong> Check whether your comments on experimental results are correct.</p>
</li>
<li><p><strong>Important references:</strong> Make sure all important references appear in your paper.</p>
</li>
<li><p><strong>Fill in holes:</strong> Imagine your are the reviewer, what questions will you ask? Answer them in the paper before they heckle you.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>My Clumsy Research Experience</title>
    <url>/2022/06/16/others/My%20Clumsy%20Research%20Experience/</url>
    <content><![CDATA[<ol>
<li><p><a href="https://ustcnewly.github.io/2018/10/31/others/Entering%20a%20New%20Research%20Field/">Entering a new research field</a></p>
</li>
<li><p><a href="https://ustcnewly.github.io/2018/09/07/others/How%20to%20Come%20up%20with%20Ideas/">How to come up with ideas</a></p>
</li>
<li><p><a href="https://ustcnewly.github.io/2018/10/31/others/Ensure%20the%20Novelty%20of%20Your%20Idea/">Ensure the novelty of your idea</a></p>
</li>
<li><p><a href="https://ustcnewly.github.io/2018/11/18/others/How%20to%20Conduct%20Experiments/">Determine experimental setting and conduct experiments</a></p>
</li>
<li><p><a href="https://ustcnewly.github.io/2018/10/31/others/Paper%20Writing/">Paper writing</a></p>
</li>
<li><p><a href="https://ustcnewly.github.io/2018/10/12/others/Paper%20Proofread/">Paper proofread</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Make Presentation</title>
    <url>/2022/06/16/others/Make%20Presentation/</url>
    <content><![CDATA[<ul>
<li><p>The organization of slides should be hierarchical. By only reading the titles on each slide, the reader should capture the whole story. </p>
</li>
<li><p>Use as many pictures, animations, videos as possible. These are called hooks, which are more enticing than text and formulas.</p>
</li>
<li><p>Do not stuff too many words, or even worse, too many formulas on each slide. One point per slide and one slide per point.</p>
</li>
<li><p>Unify the font, color, etc. At most three different colors (e.g., pure blue, pure red, pure black), one font type (e.g., Times New Roman (good for PDF), Arial) and three different font sizes (e.g., 32, 24, 18) to distinguish title, subtitle, and context.</p>
</li>
<li><p>For non-professional readers, use as few jargons as possible. Make sure each jargon should be used before well defined. Try to replace jargons with plain words.</p>
</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Hyper Parameter Tuning</title>
    <url>/2022/06/16/others/Hyper%20Parameter%20Tuning/</url>
    <content><![CDATA[<h5 id="Parameter-searching"><a href="#Parameter-searching" class="headerlink" title="Parameter searching:"></a>Parameter searching:</h5><ul>
<li><p>grid search</p>
</li>
<li><p><a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank" rel="noopener">random search</a></p>
</li>
<li><p>Bayesian Optimization: <a href="https://sigopt.com/" target="_blank" rel="noopener">SigOpt</a> is providing parameter tuning service based on Bayesian optimization. <a href="https://ustcnewly.github.io/2018/07/18/paper_note/Simple%20Bayesian%20Optimization/">This</a> is a good place to start.</p>
<ul>
<li><a href="https://github.com/hyperopt/hyperopt" target="_blank" rel="noopener">hyperopt</a> (tree parzen estimator)</li>
<li><a href="https://github.com/fmfn/BayesianOptimization" target="_blank" rel="noopener">BayesianOptimization</a>, <a href="https://github.com/JasperSnoek/spearmint" target="_blank" rel="noopener">Spearmint</a>, <a href="https://github.com/Yelp/MOE" target="_blank" rel="noopener">MOE</a> (Gaussian process) </li>
<li><p><a href="http://www.cs.ubc.ca/labs/beta/Projects/SMAC/" target="_blank" rel="noopener">SMAC</a> (random forest regression)</p>
<p>related papers:  <a href="http://papers.nips.cc/paper/5086-multi-task-bayesian-optimization.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://pdfs.semanticscholar.org/f6c3/ba9e1ccb7931ab1bacb806c7e314ab4d8b06.pdf" target="_blank" rel="noopener">[2]</a> <a href="http://proceedings.mlr.press/v37/snoek15.pdf" target="_blank" rel="noopener">[3]</a> <a href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf" target="_blank" rel="noopener">[4]</a></p>
</li>
</ul>
</li>
<li><p>Spectral Optimization: <a href="https://arxiv.org/pdf/1706.00764.pdf" target="_blank" rel="noopener">Harmonica</a> <a href="https://github.com/callowbird/Harmonica" target="_blank" rel="noopener">[github]</a> currently only supports tuning binary parameters.</p>
</li>
</ul>
<h4 id="Model-ensembling"><a href="#Model-ensembling" class="headerlink" title="Model ensembling:"></a>Model ensembling:</h4><ul>
<li>different initialization</li>
<li>different epochs</li>
<li>different hyperparameters</li>
</ul>
<h4 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h4><ul>
<li><p>A toolkit sealing all types of parameter tuning methods: <a href="https://github.com/tobegit3hub/advisor" target="_blank" rel="noopener">Google Vizier</a></p>
</li>
<li><p><a href="https://github.com/google-research/tuning_playbook/tree/main" target="_blank" rel="noopener">Tuning playbook</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>hyper-parameter</tag>
      </tags>
  </entry>
  <entry>
    <title>How to Conduct Experiments</title>
    <url>/2022/06/16/others/How%20to%20Conduct%20Experiments/</url>
    <content><![CDATA[<p>Experimental setting:</p>
<ul>
<li>Select more than 3 popular benchmark datasets. Most papers perform evaluation on 3~5 datasets.</li>
<li>Determine the training/testing split. By default, strictly follow the setting in recent papers. If their settings are not applicable, we can determine our own setting reasonably and fully describe the details in the paper. </li>
</ul>
<p>Experimental implementation:</p>
<ul>
<li>For the commonly used model, avoid unnecessary reimplementation. Just use the released code and model.</li>
<li>If closely related papers release their code, modify based on their code and avoid starting from scratch. </li>
<li>Document your code with readable and meaningful comments. Otherwise, even your cannot understand your own code after a long time.</li>
<li>Manage different versions of code carefully, otherwise your code will get messy very rapidly. Use advanced tool such as github to create branches, revert to old version, etc. The brutal approach is to create an individual folder for each version, which is not recommended.</li>
</ul>
<p>Experimental running:</p>
<ul>
<li>Avoid redundant or unncessary experiments.</li>
<li>Run experiments parallelly using all available computing resources.</li>
<li><a href="https://ustcnewly.github.io/2018/09/07/paper_note/Parameter%20Tuning/">Tune hyper-parameters wisely</a>.</li>
<li>Save final output and important intermediate output for future analysis.</li>
</ul>
<p>Experimental record:</p>
<ul>
<li>Make a to-do list for what experiments you are going to do.</li>
<li>For the experiments you have done, summarize your observation and conclusion. Then, adjust the remaining to-do list accordingly. </li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>How to Come up with Ideas</title>
    <url>/2022/06/16/others/How%20to%20Come%20up%20with%20Ideas/</url>
    <content><![CDATA[<p><u>Non-trivial ideas</u> require deep understanding and thinking, solid math background, excellent engineering skills, and sometimes good luck. <u>Trivial ideas</u> are much easier with the aid of some short formulas. We do not encourage students to generate trivial ideas based on short formulas, but this can help beginners take the first step and gain confidence.</p>
<ol>
<li><p>naive combination: e.g., combine regularizers, combine network modules</p>
</li>
<li><p>kernelize: project from low dimension to high and even infinite dimension, which is very popular before the deep learning era. </p>
</li>
<li><p>from single to multiple: create multiple-XXX learning setting, e.g., multi-view learning, multi-instance learning, multi-label learning, multi-task learning.</p>
</li>
<li><p>separate common and specific components: in multi-XXX learning setting, we can separate XXX-invariant components from XXX-specific components. e.g., for multiple domains (resp, categories), we can separate domain (resp., category)-invariant components from domain (resp., category)-specific components.</p>
</li>
<li><p>combine local information with global information: jointly use local and global information from images/videos.</p>
</li>
<li><p>from  discrete to continous (e.g., using integral) or from continuous to discrete (e.g., using basis).</p>
</li>
<li><p>from coarse-grained to fine-grained: generate a coarse-grained result first and then refine the coarse-grained result to fine-grained result.</p>
</li>
<li><p>from simple structure to advanced structure: e.g., from sequence network to tree network to graph network, form vector representation to matrix representation to tensor representation.</p>
</li>
</ol>
<ol>
<li><p>introduce auxiliary/side information to help the original task: the auxiliary/side information is usually more accessible.</p>
</li>
<li><p>learn different weights on different components: the essence of machine learning is learning weights, e.g., assign different weights on different feature dimensions (linear classifier), on different kernels (multiple-kernel learning), on different training samples (sample reweighting).</p>
</li>
<li><p>fill in the table: analyzie the existing methods and summarize them in one table. Then, find the hole in the table and fill it with your own method. </p>
</li>
</ol>
<p>My previous research works can be organized into the following groups:</p>
<ol>
<li>data-centric: how to obtain training data</li>
<li>input format: e.g., color space, spatial/frequency domain</li>
<li>output format: redefine the output format </li>
<li>basic model: upgrade CNN to transformer or diffusion model</li>
<li>essential math: extract and solve the essential math problem </li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Fair comparison</title>
    <url>/2022/06/16/others/Fair%20Comparison/</url>
    <content><![CDATA[<ul>
<li><p>Carefully compare your experiments settings with those described in previous papers. The experimental settings include training/testing split, input format (e.g., image size), evaluation metric, backbone network, etc. If they are not exactly the same, you need to re-run their methods with exactly the same experimental settings as yours for fair comparison. For example, if the baseline uses 32x32 image size while you use 244x244 image size, that is unfair comparison. If the baseline uses AlexNet as backbone while you use ResNet as backbone, that is unfair comparison.</p>
</li>
<li><p>The baseline methods may use more or less information, compared with your method. Based on using more/less information and their performance is better/worse than yours, we could have the following table. </p>
<ul>
<li>If the baseline uses more information but achieves worse results, never mind. Just brag your own method.</li>
<li>If the baseline uses less information but achieves better results, it is a serious problem. You have to check your method.</li>
<li>If the baseline uses less information and achieves worse results, that is reasonable. But you may be accused of unfair comparison because your method uses more information. For safety, please augment the baseline with extra information and compare again. </li>
<li><p>If the baseline uses more information and achieves better results, that is reasonable. But do not casually put them in the paper because dumb reviewers may ignore these details and point out your method is not good enough. That is very risky! Please remove the extra information from the baseline and compare again.</p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/Its6ekq.jpg" width="80%"></p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>Entering a New Research Field</title>
    <url>/2022/06/16/others/Entering%20a%20New%20Research%20Field/</url>
    <content><![CDATA[<ol>
<li><p><strong>See the large picture</strong></p>
<ul>
<li>Search keyword + ‘survey/tutorial’ in google/google scholar. For example, if you want to work on zero-shot learning, search ‘zero-shot learning’ + ‘survey/tutorial’. </li>
<li>Find recent survey/tutorial on top conference/journal or conjunct workshop. You need to know the list of top conferences/journals in the related field (e.g., computer vision, machine learning).</li>
</ul>
</li>
<li><p><strong>Know the state-of-the-art</strong>  </p>
<ul>
<li>Search keyword in the most recent top conferences (e.g., CVPR2018). Journal is generally lagging behind the conference, so focus on conference papers. You collect the paper list and will have a sense whether this topic is popular or crowded (e.g., only 1 paper or more than 20 papers on one conference).</li>
</ul>
</li>
<li><p><strong>Summarize existing methods</strong></p>
<ul>
<li>You need to collect a paper list. Initiate the paper list following Step 2. Read the related works in these papers and add them to the paper list. Repeat this procedure to enrich the paper list. Try to categorize the used methods in these papers based on certain taxonomy.</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Ensure the Novelty of Your Idea</title>
    <url>/2022/06/16/others/Ensure%20the%20Novelty%20of%20Your%20Idea/</url>
    <content><![CDATA[<p>There is no automatic system that can tell you whether your idea has been done or the similarity between your idea and the most similar previous work. </p>
<ul>
<li><p>Search all possible keywords (e.g., synonym) in Google/Google Scholar.</p>
</li>
<li><p>If there exist old and recent surveys, carefully read them. Otherwise, make a survey on your own.</p>
</li>
<li><p>Quickly go through previous works to ensure that your idea has not been done. You do not need to understand the details of their methods. Actually, telling whether their methods are close to yours is much simpler. For instance, you can just pay attention to the  most obvious parts (e.g., flowchart, objective function, and abstract) in their papers. </p>
</li>
</ul>
<p>Through the above steps, identify the most related work, think about the difference and your advantage. If you cannot clearly state the difference and advantage of your idea, do not work on it.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Dictionary for Paper Writing</title>
    <url>/2022/06/16/others/Dictionary%20for%20Paper%20Writing/</url>
    <content><![CDATA[<h2 id="Between-Logical-Blocks"><a href="#Between-Logical-Blocks" class="headerlink" title="Between Logical Blocks"></a>Between Logical Blocks</h2><ul>
<li><p>因果: because, since, as, so, consequently, therefore, thus, hence</p>
</li>
<li><p>转折: but, however, nevertheless, nonetheless, despite, whereas, although, while, albeit</p>
</li>
<li><p>结果: generate, produce, yield, lead to, result in, give rise to</p>
</li>
<li><p>递进: besides, furthermore, moreover</p>
</li>
<li><p>顺承: following, followed by, prior to, in the wake of</p>
</li>
<li><p>相反: on the contrary, in constrast with, in opposition to, as opposed to, vice versa, the other way around</p>
</li>
<li><p>换言之: in other words, that is, that being said</p>
</li>
<li><p>具体来说: particularly, in particular, specifically, to be exact, in detail, concretely</p>
</li>
</ul>
<h2 id="Within-Logical-Blocks"><a href="#Within-Logical-Blocks" class="headerlink" title="Within Logical Blocks"></a>Within Logical Blocks</h2><ul>
<li><p>优势：advantage, benefit</p>
</li>
<li><p>劣势：disadvantage, drawback, shortage, shortcoming, defficiency</p>
</li>
<li><p>超过: outperform, exceed, surpass</p>
</li>
<li><p>基于: based on, according to, on the basis of, in the light of, on the premise of</p>
</li>
<li><p>除了: besides, apart from, aside from </p>
</li>
<li><p>促进: facilitate, fuel, advance, benefit, aid in, assist in, spur</p>
</li>
<li><p>损害: degrade, impair, hinder, eliminate, compromise, harm, hamper</p>
</li>
<li><p>解决: address, solve, handle, mitigate, tackle, cope with, overcome, circumvent, bypass</p>
</li>
<li><p>缓解: alleviate, suppress, mitigate, assuage, relieve, ameliorate</p>
</li>
<li><p>介绍: introduce, describe, discuss, elaborate, review</p>
</li>
<li><p>展示: demonstrate, show, indicate, exhibit, display, illustrate</p>
</li>
<li><p>验证: prove, justify, verify</p>
</li>
<li><p>怀疑: cast doubt on</p>
</li>
<li><p>使用: use, utilize, employ, leverage, harness</p>
</li>
<li><p>代替: instead of, in lieu of, alternative, surrogate, supersede, replace</p>
</li>
<li><p>相似: similar to, analogous to, in analogy to, resemble, bear resemblance to, is reminiscent of, akin to</p>
</li>
<li><p>包含： contain, include, be composed of, be comprised of, consist of, be formed by</p>
</li>
<li><p>到目前为止: up to now, so far</p>
</li>
<li><p>重要的: important, significant, crucial, vital, critical</p>
</li>
<li><p>明显的: notable, evident, pronounced</p>
</li>
<li><p>大量的: considerable, massive, vast, myriads of, a variety of, a wide range of, substantial, adequate, plenty of, a plethora of, unprecedented</p>
</li>
<li><p>很大程度: dramatically, greatly, significantly, considerably</p>
</li>
<li><p>必需的: necessary, imperative, demanding, in high demand</p>
</li>
<li><p>良好的: superior, favorable, compelling,  competitive, remarkable, excellent, impressive</p>
</li>
<li><p>有害的: harmful, detrimental</p>
</li>
<li><p>流行的: popular, prevailing, prevalent, attractive, enticing, inviting, dominant</p>
</li>
<li><p>差不多的: comparable, on par with</p>
</li>
<li><p>鲁棒的: robust, agnostic, insensitive, insusceptible</p>
</li>
<li><p>敏感的: sensitive, susceptible, brittle</p>
</li>
<li><p>适合的: well-suited, suitable, applicable, well-tailored </p>
</li>
<li><p>艰难的: tough, challenging, formidable</p>
</li>
<li><p>笨拙的: cumbersome, unwieldy</p>
</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>Determine baselines</title>
    <url>/2022/06/16/others/Determine%20Baselines/</url>
    <content><![CDATA[<p>Generally speaking, for the baselines, you need to compare with other (i.e., state-of-the-art) and compare with yourself (i.e., component analysis or ablation study).</p>
<h2 id="Compare-with-others"><a href="#Compare-with-others" class="headerlink" title="Compare with others"></a>Compare with others</h2><ul>
<li><p>How to select baselines?</p>
<ul>
<li>Select the baselines from top conferences. You can refer to related paper published on recent top conferences and find out which baselines they compare with. The intersection of their baselines should be the most popular ones.</li>
<li>The selected baselines should be discussed in the related work in your paper.</li>
<li>The selected baselines should cover at least several ones from the most recent top conferences.</li>
<li>The selected baselines should cover the researchers who are very famous in this field or has many publications in this field. If you do not cite his/her paper and your paper unfortunately goes under his/her review, then you are doomed.</li>
</ul>
</li>
<li><p>Could we directly copy the results from previous papers?</p>
<ul>
<li>Carefully compare your experiments settings with those described in previous papers. The experimental settings include training/testing split, input format (e.g., image size), evaluation metric, backbone network, etc. If they are exactly the same, just copy the results. </li>
<li>Otherwise, you need to re-run their methods with exactly the same experimental settings as yours for fair comparison.  </li>
</ul>
</li>
<li><p>Need we implement the baselines?</p>
<ul>
<li>Search the code online and contact the authors for code.</li>
<li>If you could not get the code, you need to implement the baseline by yourself according to the details provided in the paper. Theoretically, it is impossible to completely re-implement the baseline unless the method is frustratingly easy (e.g., 10 lines of matlab code), so just follow your understanding and implement a reasonable version.</li>
</ul>
</li>
</ul>
<h2 id="Compare-with-yourself"><a href="#Compare-with-yourself" class="headerlink" title="Compare with yourself"></a>Compare with yourself</h2><ul>
<li><p>Why is it necessary?</p>
<ul>
<li>Because you need to understand which component of your method really works.</li>
<li>If you do not compare with yourself, you provide a perfect reason for reviewers to reject your paper.</li>
</ul>
</li>
<li><p>How many special cases do we need?</p>
<ul>
<li>That mainly depends on the technical contribution of your paper. If you claim regularizer XXX or strategy XXX or subnetwork XXX is proposed by yourself and very effective, you have to verify that in the experiments.</li>
<li>For some naive special cases, you may just need to set certain hyper-parameter as 0 or freeze some components in your network, so the experiments will be quite simple. For other advanced special cases, that will take some more work. </li>
</ul>
</li>
</ul>
<h2 id="Checklist"><a href="#Checklist" class="headerlink" title="Checklist"></a>Checklist</h2><p>Please select and compare with baselines meticulously. You can summarize and check your baseline information in the following table:</p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/UHKtu7Y.jpg" width="100%"></p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>Conference Rebuttal</title>
    <url>/2022/06/16/others/Conference%20Rebuttal/</url>
    <content><![CDATA[<ol>
<li><p>To determine whether the rebuttal is necessary. If the scores are too low, just give up.</p>
</li>
<li><p>Summarize the questions from reviewers and rank them based on the importance. </p>
</li>
<li><p>Pick out the questions which call for experiments and conduct those experiments immediately.</p>
</li>
<li><p>When running experiments, draft the response file. Pay attention to the format of response file (e.g., limited characters or one page of pdf, URL allowed or not).</p>
</li>
<li><p>In the response file, try to cover all the questions if possible. Otherwise, igore the questions with least importance.</p>
</li>
<li><p>The tone of response file cannot be rude and offensive. Be cool and confident.</p>
</li>
<li><p>If the maximum number of characters is quite limited, there are many tricks to save space.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>Bibliography Collection</title>
    <url>/2022/06/16/others/Bibliography%20Collection/</url>
    <content><![CDATA[<ol>
<li><p>For the full name of journals and venue information of conferences, click <a href="https://docs.google.com/spreadsheets/d/1-KlvSeO1ULdUkXCByqED1bQBeFXTG_e5LfHeWwtei0A/edit#gid=0" target="_blank" rel="noopener">Conference and Journal Info</a>.</p>
</li>
<li><p>For the collected full-length bib, click <a href="https://docs.google.com/document/d/1T8lmj8ZHfErMSVwrsQc_FVYNYbEaRETVPf9SKMfiGoY/edit" target="_blank" rel="noopener">egbib collection</a>.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu Software Management</title>
    <url>/2022/06/16/operation_system/Ubuntu/Ubuntu%20Software%20Management/</url>
    <content><![CDATA[<h3 id="raw-dpkg"><a href="#raw-dpkg" class="headerlink" title="raw: dpkg"></a>raw: dpkg</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">dpkg -i xxx.deb</span><br><span class="line">dpkg -r xxx.deb</span><br><span class="line">dpkg --purge xxx.deb</span><br><span class="line">dpkg -L xxx.deb</span><br><span class="line">dpkg --info xxx.deb</span><br><span class="line">dpkg -reconfigure xxx</span><br></pre></td></tr></table></figure>
<p>dkpg is raw method to install without solving dependencies and existing software.</p>
<h3 id="mature-apt-get"><a href="#mature-apt-get" class="headerlink" title="mature: apt-get"></a>mature: apt-get</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apt-get install softname1 softname2 softname3……</span><br><span class="line">apt-get remove softname1 softname2 softname3……</span><br><span class="line">apt-get remove --purge softname1</span><br><span class="line">apt-get autoremove</span><br><span class="line">apt-get clean //clean /var/cache/apt/archives </span><br><span class="line">apt-get autoclean //clean the out-of-date files in /var/cache/apt/archives</span><br><span class="line"></span><br><span class="line">apt-get update //update software information and database command</span><br><span class="line">apt-get upgrade //update the system</span><br><span class="line"></span><br><span class="line">apt-cache search rough_name</span><br><span class="line">apt-cache show exact_name</span><br><span class="line"></span><br><span class="line">pkg-config --libs opencv</span><br><span class="line">pkg-config --cflags opencv </span><br><span class="line">pkg-config --modversion opencv</span><br></pre></td></tr></table></figure>
<p>apt-get is built on dkpg without saving the deb file. apt-get can solve dependencies and existing software. Note that when using dkpg, dkpg can circumvent apt-get, so apt-get don’t know the software installed by dkpg.</p>
<h3 id="more-mature-aptitude-GUI"><a href="#more-mature-aptitude-GUI" class="headerlink" title="more mature: aptitude (GUI)"></a>more mature: aptitude (GUI)</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">aptitude install softname1</span><br><span class="line">aptitude remove softname1</span><br></pre></td></tr></table></figure>
<p>aptitude is also built on dkpg and more powerful than apt-get.</p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu Shell Scripts</title>
    <url>/2022/06/16/operation_system/Ubuntu/Ubuntu%20Shell%20Scripts/</url>
    <content><![CDATA[<p><code>#!/bin/bash</code> at the head of file indicates shell type</p>
<ol>
<li><p>strict format</p>
<p> For <code>if [[ $input == &quot;hello&quot; ]]</code>, note that the space after [[ and before ]] is very strict, since [[]] can be used for matching regular expression</p>
</li>
<li><p>arguments</p>
<ul>
<li>$@: stores all the arguments in a list of string</li>
<li>$*: stores all the arguments as a single string</li>
<li>$#: stores the number of arguments</li>
<li>shift: remove the first argument</li>
</ul>
</li>
</ol>
<p>When starting login or interative shells, certain files will be executed based on the following tables:</p>
<p>For bash:<br>login-y interactive-y: profile<br>login-y interactive-n: profile<br>login-n interactive-y: bashrc</p>
<p>For zsh:<br>login-y interactive-y: zshenv zprofile zshrc zlogin<br>login-y interactive-n: zshenv zprofile zlogin<br>login-n interactive-y: zshenv zshrc<br>login-n interactive-n: zshenv</p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu Mount</title>
    <url>/2022/06/16/operation_system/Ubuntu/Ubuntu%20Mount/</url>
    <content><![CDATA[<h3 id="Mount-remote-folder-on-Windows-Linux"><a href="#Mount-remote-folder-on-Windows-Linux" class="headerlink" title="Mount remote folder on Windows/Linux:"></a>Mount remote folder on Windows/Linux:</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install cifs-utils</span><br><span class="line">sudo mount -t cifs -o username=XXX,password=XXX //10.70.1.82/src_dir tgt_dir</span><br></pre></td></tr></table></figure>
<h3 id="Mount-sharefolder-between-host-machine-and-virtualbox"><a href="#Mount-sharefolder-between-host-machine-and-virtualbox" class="headerlink" title="Mount sharefolder between host machine and virtualbox:"></a>Mount sharefolder between host machine and virtualbox:</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo mount -t vboxsf -o uid=$UID,gid=$(id -g) share_name tgt_dir</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>mount</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu Log-in Failure</title>
    <url>/2022/06/16/operation_system/Ubuntu/Ubuntu%20Log-in%20Failure/</url>
    <content><![CDATA[<p>You may fail to log in Ubuntu due to the following reasons:</p>
<ol>
<li><p>/etc/environment or /etc/profile is modified to a wrong format: Just modify them back.</p>
</li>
<li><p>startX is used improperly: run <code>sudo rm -r .Xauthority*</code> </p>
</li>
</ol>
<p>Tips: use Ctr+Alt+F1~6 corresponding to tty 1~6 to use command line</p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu language</title>
    <url>/2022/06/16/operation_system/Ubuntu/Ubuntu%20Language/</url>
    <content><![CDATA[<ol>
<li><p><code>$sudo gedit /etc/default/locale</code></p>
</li>
<li><p>modify as follows,</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LANG=&quot;en_US.UTF-8&quot;</span><br><span class="line">LANGUAGE=&quot;en_US:en&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>$locale-gen -en_US:en</code></p>
</li>
<li><p>log out or reboot</p>
</li>
</ol>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu Environment Variable</title>
    <url>/2022/06/16/operation_system/Ubuntu/Ubuntu%20Environment%20Variable/</url>
    <content><![CDATA[<p>For user-wide: ~/.profile or ~/.bashrc<br>For system-wide: /etc/profile</p>
<p><code>source ~/.bashrc</code> or <code>source /etc/profile</code> can make the newly added (not the removed) environment variables for the current cmd window available immediately. However, you need to re-login to make them user-wide or system-wide.</p>
<p>Note that after you <code>sudo su</code> (not using sudo privilege), the environment variables will be lost. You need to re-login. Because <code>sudo su</code> will erase newly exported variables.</p>
<p>For permanent system-wide change even after <code>sudo su</code>, you should modify /etc/environment, which is not recommended. Because /etc/environment cannot recognize intermediate variable such as $JAVA_HOME. Sometimes misusing /etc/environment may result in your failure in login.</p>
<p>Not recommend running <code>sudo su</code> and then modifying ~/.profile or ~/.bashrc.</p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu Crontab</title>
    <url>/2022/06/16/operation_system/Ubuntu/Ubuntu%20Crontab/</url>
    <content><![CDATA[<ul>
<li><p>crontab -l //list crontab</p>
</li>
<li><p>crontab -r //remove crontab</p>
</li>
<li><p>crontab -e //edit crontab</p>
<ul>
<li>minute hour day-of-month month day-of-week cmd </li>
<li>each term can be a single number, <em>e.g.</em>, 3, or a range, <em>e.g.</em>, 3-6, or a set, <em>e.g.</em>, 3,5,7, or interval, <em>e.g.</em>,  */10</li>
<li>for example, “<em> </em>/10 <em> </em> * sh ~/cmd.sh” means executing cmd.sh every 10 minutes.</li>
</ul>
</li>
</ul>
<p>In windows, the function of contrab can be realized by using “task scheduler”.</p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>crontab</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu Command Lines</title>
    <url>/2022/06/16/operation_system/Ubuntu/Linux%20Command%20Lines/</url>
    <content><![CDATA[<p>Here is a website <a href="https://linux.gaomeluo.com/" target="_blank" rel="noopener">link</a> to query linux commands.</p>
<hr>
<ol>
<li><p>create new user with home folder</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">adduser XXX</span><br></pre></td></tr></table></figure>
</li>
<li><p>sudo privilege: <code>vim /etc/sudoers</code> and add <code>$username ALL=(ALL) ALL</code> at the bottom.</p>
</li>
<li><p>start ssh on the server, the default port is 22</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br><span class="line">sudo/etc/init.d/ssh start</span><br></pre></td></tr></table></figure>
</li>
<li><p>list</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">list -a #including hidden files</span><br><span class="line">list -S #arrange by size</span><br><span class="line">list -t #arrange by time</span><br></pre></td></tr></table></figure>
</li>
<li><p>view text</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat</span><br><span class="line">head/tail -n 10 tmp.txt #view the first/last 10 lines</span><br><span class="line">less #more powerful than more</span><br></pre></td></tr></table></figure>
</li>
<li><p>Change the privilege</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod 777 ./</span><br><span class="line">chmod a+x ./</span><br></pre></td></tr></table></figure>
</li>
<li><p>Check disk or file size</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">df -h </span><br><span class="line">du ./ --max-depth 2 -h</span><br></pre></td></tr></table></figure>
</li>
<li><p>Compress or uncompress files, refer to this <a href="http://www.cnblogs.com/eoiioe/archive/2008/09/20/1294681.html" target="_blank" rel="noopener">link</a>.</p>
</li>
<li><p>Grep + regular expression</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">grep [xyz]</span><br></pre></td></tr></table></figure>
</li>
<li><p>Search file</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">locate "keyword" #fast</span><br><span class="line">find ./ -maxdepth 1 -name "*keyword*" </span><br><span class="line">find ./ -name "*keyword*" -size +50M -size -100M </span><br><span class="line">find ./ -name "*keyword*" -mmin -10 #m:modify min:minute </span><br><span class="line">find ./ -name "*keyword*" -exec rm -r &#123;&#125; \;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Pipe commands</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ls -l | tr -s ' ' | cut -d ' ' -f 2 #tr to truncate space</span><br><span class="line">ls -l | sort -rnk2 #-r:reverse -n:numerical -k:k-th column</span><br><span class="line">ls -l | wc -l #count line</span><br></pre></td></tr></table></figure>
</li>
<li><p>xargs:</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat python/requirements.txt | xargs -L 1 sudo pip install</span><br><span class="line">find . -name "*.c" | xargs rm -rf</span><br><span class="line">find . -name '*.c' | xargs grep 'stdlib.h'</span><br></pre></td></tr></table></figure>
</li>
<li><p>alias: temporary alias command </p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias lnew="cd /home/niuli/caffe"</span><br><span class="line">unalias lnew</span><br></pre></td></tr></table></figure>
</li>
<li><p>export: The export command is one of the bash shell built-in commands, which means it is part of your shell.</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export a=linux.com</span><br><span class="line">echo $a</span><br><span class="line">export -n a #remove variable</span><br><span class="line"></span><br><span class="line">printname () &#123; echo "Linuxcareer.com"; &#125;</span><br><span class="line">export -f printname #export function</span><br></pre></td></tr></table></figure>
<p>add LD_LIBRARY_PATH</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/lib' &gt;&gt; ~/.bashrc</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
</li>
<li><p>shellscript sample</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line">read -p "Please input your first name: " </span><br><span class="line">echo -e "\nYour full name is: $firstname $lastname" </span><br><span class="line">test ! -e $filename &amp;&amp; echo "The filename '$filename' DO NOT exist" &amp;&amp; exit 0</span><br><span class="line">test -f $filename &amp;&amp; filetype="regulare file" </span><br><span class="line">test -d $filename &amp;&amp; filetype="directory" </span><br><span class="line">[ "$yn" == "Y" -o "$yn" == "y" ] &amp;&amp; echo "OK, continue" &amp;&amp; exit 0</span><br><span class="line"></span><br><span class="line">if [ "$yn" == "Y" ] || [ "$yn" == "y" ]; then </span><br><span class="line">	echo "OK, continue" </span><br><span class="line">elif [ "$yn" == "N" ] || [ "$yn" == "n" ]; then </span><br><span class="line">	echo "Oh, interrupt!" </span><br><span class="line">else </span><br><span class="line">	echo "I don't know what your choice is" </span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">while [ "$yn" != "yes" -a "$yn" != "YES" ] </span><br><span class="line">do </span><br><span class="line">	read -p "Please input yes/YES to stop this program: " yn </span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">for animal in dog cat elephant </span><br><span class="line">do </span><br><span class="line">	echo "There are $&#123;animal&#125;s.... " </span><br><span class="line">done</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows Command Lines</title>
    <url>/2022/06/16/operation_system/Windows%20Command%20Lines/</url>
    <content><![CDATA[<ol>
<li><p>zip/unzip</p>
<p> <code>winrar x -y -ibck zip_file_name unzip_file_name</code> //x means unzip, -y means yes to interrupted queries, -ibck means running in the background</p>
</li>
<li><p>generate the file list under one folder</p>
<p> <code>dir D:\test /b &gt;list.txt</code> or <code>dir D:\test /b &gt;list.xls</code></p>
</li>
<li><p>combine multiple compressed volumes</p>
<p> <code>copy /b logs.tar.gza* logs.tar.gz</code></p>
</li>
</ol>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title>Install Ubuntu under Windows</title>
    <url>/2022/06/16/operation_system/Install%20Ubuntu%20under%20Windows/</url>
    <content><![CDATA[<p>First check whether the file system is MBR+BIOS or GPT+UEFI. Nowadays, most computers are GPT+UEFI, so we only talk about GPT+UEFI here.</p>
<ol>
<li><p>Download Ubuntu iso and make a U-installer using rufus. When using rufus, notice the option of GPT+UEFI.</p>
</li>
<li><p>Start the computer from U-installer. The most important thing is making a proper partition for Ubuntu. At least make three partitions: / (ext4), swap (2G), reserved for UEFI (1G).  If necessary, make another partition /home. Note that ubuntu UEFI should be on the same disk as Windows UEFI.   </p>
</li>
</ol>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title>BIOS</title>
    <url>/2022/06/16/operation_system/BIOS/</url>
    <content><![CDATA[<p>If something is wrong when attempting to enter the BIOS, try the following approaches:</p>
<ul>
<li>press F2, F12, ESC, DEL</li>
<li>change the cable, adaptor, interface, etc (hardware reason)</li>
<li>update BIOS to the latest version</li>
<li>for Windows10, settings-&gt;recover-&gt;firmware</li>
<li>close fast-boot option in the operation system because fast-boot may directly skip the BIOS stage</li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>BIOS</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu SSH</title>
    <url>/2022/06/16/network/Ubuntu%20VPN/</url>
    <content><![CDATA[<p>Sometimes Ubuntu fails to connect VPN. Open the PPTP advanced options, uncheck EAP and check MPPE.</p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/cr0lyNn.jpg" alt=""></p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu VNC</title>
    <url>/2022/06/16/network/Ubuntu%20VNC/</url>
    <content><![CDATA[<h3 id="Server"><a href="#Server" class="headerlink" title="Server:"></a>Server:</h3><ol>
<li><p>Install VNC server under Ubuntu: $sudo apt-get install vnc4server</p>
</li>
<li><p>Configure the VNC server, $vi ~/.vnc/xstartup<br>change the style to gnome-session &amp; in the last line.</p>
</li>
<li><p>start the VNC server: $vncserver</p>
</li>
</ol>
<h3 id="Client"><a href="#Client" class="headerlink" title="Client:"></a>Client:</h3><ol>
<li>Install VNC client under Windows: use http:\IP:580$ID to log in.</li>
</ol>
<p><strong>Tips:</strong> For 10060 error, run <code>$iptables -F</code> to close firewall.</p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>VNC</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu SSH</title>
    <url>/2022/06/16/network/Ubuntu%20SSH/</url>
    <content><![CDATA[<p><strong>Server:</strong></p>
<ol>
<li><p>run <code>$sudo apt-get install openssh-server</code></p>
</li>
<li><p>run <code>$ps -A |grep ssh</code>.<br>If there is sshd, then ssh-server has been started,<br>else run <code>$sudo /etc/init.d/ssh start</code> to start ssh-server.</p>
</li>
</ol>
<p>Tips: To stop ssh-server, execute <code>$sudo /etc/init.d/ssh stop</code>.<br>To modify the configuration, edit <code>/etc/ssh/sshd_config</code>. </p>
<p><strong>Client:</strong></p>
<p>ssh-client is usually already installed. If not, run<br><code>$sudo apt-get install openssh-client</code>.</p>
<p>After installing ssh-client, run <code>$ssh username@192.168.1.112</code>.</p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title>Remote Folder Mount</title>
    <url>/2022/06/16/network/Remote%20Folder%20Mount/</url>
    <content><![CDATA[<h3 id="Mount-Linux-folder-under-Linux"><a href="#Mount-Linux-folder-under-Linux" class="headerlink" title="Mount Linux folder under Linux"></a>Mount Linux folder under Linux</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install cifs-utils</span><br><span class="line">sudo mount -t cifs -o username=XXX,password=XXX //10.70.1.82/src_dir tgt_dir</span><br></pre></td></tr></table></figure>
<h3 id="Mount-Windows-folder-under-Windows"><a href="#Mount-Windows-folder-under-Windows" class="headerlink" title="Mount Windows folder under Windows"></a>Mount Windows folder under Windows</h3><p>add network device</p>
<h3 id="Mount-Linux-folder-under-Windows"><a href="#Mount-Linux-folder-under-Windows" class="headerlink" title="Mount Linux folder under Windows"></a>Mount Linux folder under Windows</h3><hr>
<ol>
<li><p>Install Dokan and WinSSHFS: for Win10, recommend 1.6.1.13 <a href="https://github.com/feo-cz/win-sshfs/releases" target="_blank" rel="noopener">win-sshfs</a> and 1.0.3 <a href="https://github.com/dokan-dev/dokany/releases" target="_blank" rel="noopener">Dokan</a>.</p>
</li>
<li><p>Open WinSSHFS, fill in the drive name, host, port, username, password, directory. The other entries can be left blank.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>mount</tag>
      </tags>
  </entry>
  <entry>
    <title>Remote Desktop Connection</title>
    <url>/2022/06/16/network/Remote%20Desktop%20Connection/</url>
    <content><![CDATA[<h3 id="Remote-Connect-to-Ubuntu-in-Windows"><a href="#Remote-Connect-to-Ubuntu-in-Windows" class="headerlink" title="Remote Connect to Ubuntu in Windows"></a>Remote Connect to Ubuntu in Windows</h3><hr>
<p><strong>a) cmdline:</strong> putty, secureCRT</p>
<p><strong>b) window:</strong></p>
<p>xrdp: <a href="http://jingyan.baidu.com/article/8ebacdf0cdc64949f75cd555.html" target="_blank" rel="noopener">http://jingyan.baidu.com/article/8ebacdf0cdc64949f75cd555.html</a><br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install xrdp</span><br><span class="line">sudo apt-get install vnc4server</span><br><span class="line">sudo apt-get install xubuntu-desktop</span><br><span class="line">echo "xfce4-session" &gt;~/.xsession</span><br><span class="line">sudo service xrdp restart</span><br></pre></td></tr></table></figure></p>
<p>xrdp mm process login response login failed<br>When this error occurs, try the following methods, followed by “sudo service xrdp restart”.</p>
<ol>
<li><p>delete vnc processes</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ps -ef | grep vnc</span><br><span class="line">kill -9 XXX</span><br></pre></td></tr></table></figure>
</li>
<li><p>delete X sessions, .xrdp, .X11-unix, .X0-20</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /tmp &amp;&amp; ls -a</span><br></pre></td></tr></table></figure>
</li>
<li><p>modify “MaxSessions” number</p>
<pre><code class="lang-shell"> sudo vim /etc/xrdp/sesman.ini
</code></pre>
</li>
</ol>
]]></content>
      <categories>
        <category>network</category>
      </categories>
  </entry>
  <entry>
    <title>kinit</title>
    <url>/2022/06/16/network/kinit/</url>
    <content><![CDATA[<p>Credential ticket for principles without need to type in password, from MIT Kerberos.<br>A ticket has ticket lifetime and renewable lifetime.<br>Ticket lifetime is shorter than renewable lifetime.<br>For liniu@ANT.AMAZON.COM, the default ticket lifetime is 10h (resp., 6h40m) when using kinit -l (resp., -r), why?</p>
<p>From KDC server side: </p>
<ol>
<li>modify the max_life in /etc/krb5kdc/kdc.conf and restart the KDC daemon  /var/kerberos/krb5kdc/kdc.conf</li>
<li>Via “kadmin”, changed the “maxlife” for a test principal via “modprinc -maxlife 14hours “</li>
</ol>
<p>From Kerberos client side:<br>modify in /etc/krb5.conf</p>
<p>In fact, the ticket lifetime is the minimum of the following values:</p>
<ul>
<li><p>max_life in kdc.conf on the KDC servers.</p>
</li>
<li><p>ticket_lifetime in krb5.conf on the client machine.</p>
</li>
<li><p>maxlife for the user principal.</p>
</li>
<li><p>maxlife for the service principal “krbtgt/[REALM_in_CAPS]” </p>
</li>
<li><p>requested lifetime in the ticket request. For example: kinit -l 14h</p>
</li>
<li><p>maxlife for the AFS service principal “afs/[realm_in_lower_case]”, if you want to increase the lifetime of your AFS token.</p>
</li>
</ul>
<p>commonly used commands:<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">klist</span><br><span class="line">kdestroy</span><br></pre></td></tr></table></figure></p>
<p>An example:</p>
<p>Ticket cache: FILE:/tmp/krb5cc_4126574_GM19Ct<br>Default principal: liniu@ANT.AMAZON.COM</p>
<p>Valid starting     Expires            Service principal<br>08/18/16 08:13:20  08/18/16 18:13:20  krbtgt/ANT.AMAZON.COM@ANT.AMAZON.COM</p>
<p>time format is like 4d5h30m</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kinit -l lifetime //request a ticket with ticket lifetime of lifetime</span><br><span class="line">-r renewable-life //request renewable ticket with a total lifetime of renewable-life</span><br><span class="line">//I'm still unclear about the difference between -l and -r</span><br><span class="line">-f //forwardable</span><br><span class="line">-F //non-forwardable</span><br><span class="line">-R //requests renewal of the ticket-granting ticket. No need for password but must be within ticket lifetime instead of renewable lifetime.</span><br></pre></td></tr></table></figure>
<p>Automatically renew tickets: Since you need to renew a ticket before its ticket lifetime expires, the easiest way to renew tickets is to put it in a cron job since renewing a ticket is non-interactive.</p>
<p>Run ‘crontab -e’ to edit the file in /var/spool/cron/. Use ‘crontab -l’ to see the file.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Renew the kerberos ticket every 8 hours, this will extend the lifetime of </span><br><span class="line"># the ticket until the renew lifetime expiers, after that this command will </span><br><span class="line"># fail to renew the ticket and you will need to interactively </span><br><span class="line"># run `kinit -f -l 86400 -r 2592000`</span><br><span class="line">#</span><br><span class="line"># minute  hour  day_of_month  month  weekday  command</span><br><span class="line">59 00,08,16 * * * /usr/kerberos/bin/kinit -R</span><br><span class="line">//59 minute, 0 or 8 or 16 o&apos;clock, any, any, any, the command to be executed is &apos;/usr/kerberos/bin/kinit -R&apos;</span><br><span class="line">//some short notations: 1-3 means 1,2,3; */15 means every 15</span><br></pre></td></tr></table></figure>
<p>Key Notes:</p>
<ol>
<li><p>do not use sudo kinit</p>
</li>
<li><p>when no credential ticket can be found, add -c $KRB5CCNAME, where KRB5CCNAME is the environment variable recording the path of credential ticket.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>network</category>
      </categories>
  </entry>
  <entry>
    <title>GitHub+Hexo for Personal Blog</title>
    <url>/2022/06/16/network/GitHub+Hexo%20for%20Personal%20Blog/</url>
    <content><![CDATA[<ol>
<li>Preinstallation<ul>
<li>install Node.js <a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">https://nodejs.org/en/download/</a>    </li>
<li>install git <a href="https://git-scm.com/download/win" target="_blank" rel="noopener">https://git-scm.com/download/win</a>    </li>
<li>install hexo: right click “git bash here”, <code>$npm install hexo-cli -g</code></li>
</ul>
</li>
<li>SSH keys<ul>
<li>check whether ssh exists <code>$cd ~/.ssh</code>. If not, <code>$ssh-keygen -t rsa -C &quot;your_email@example.com&quot;</code> to generate key file</li>
<li>get the SSH key <code>$cat ~/.ssh/id_rsa.pub</code></li>
<li>create the key in github: account setting-&gt;SSH</li>
<li>check the SSH key <code>$ ssh -T git@github.com</code> </li>
</ul>
</li>
<li><p>create an empty folder as Hexo folder</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span>hexo init</span><br><span class="line"><span class="meta">$</span>hexo generate</span><br><span class="line"><span class="meta">$</span>hexo server</span><br></pre></td></tr></table></figure>
<p>local test: <a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a> </p>
</li>
<li><p>deploy local Hexo folder</p>
<ul>
<li><p>in Hexo folder, modify the _config.yml file as follows,                                     </p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">	type: git</span><br><span class="line">	repository: $(SSH address from github)</span><br><span class="line">	branch: master</span><br></pre></td></tr></table></figure>
</li>
<li><p>in Hexo folder,  install the deployer by <code>$npm install hexo-deployer-git --save</code></p>
</li>
<li>type <code>$hexo g</code> and <code>$hexo d</code>, or <code>$hexo d -g</code>. </li>
</ul>
</li>
<li>change theme: go to the folder “/themes” and <code>git clone https://github.com/iissnan/hexo-theme-next</code>. config the theme in the file “/themes/XXXX/_config.yml”.</li>
<li><p>local search</p>
<ul>
<li><code>$npm install hexo-generator-searchdb --save</code></li>
<li><p>in the site _config.yml file, set <code>local_search: enable: true</code>. paste the following lines anywhere.</p>
   <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">	path: search.xml</span><br><span class="line">	field: post #post, page, all</span><br><span class="line">	format: html</span><br><span class="line">	limit: 10000</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>add new page for some subcategory (e.g., write): in the theme _config.yml file</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">   write: /categories/write</span><br></pre></td></tr></table></figure>
</li>
<li><p>add social links: in the theme _config.yml file</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Social links</span><br><span class="line">social:</span><br><span class="line">	GitHub: https://github.com/ustcnewly</span><br><span class="line">	Linkedin: https://www.linkedin.com/in/li-niu-b0905133/</span><br></pre></td></tr></table></figure>
</li>
<li><p>latex: </p>
<ul>
<li>install <code>$npm install hexo-math --save</code> and restart Hexo</li>
<li><p>in the theme _config.yml file, modify as follows (notice that <code>cdn</code> may change) </p>
   <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">	enable: true</span><br><span class="line">	per_page: false</span><br><span class="line">	cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span><br></pre></td></tr></table></figure>
</li>
<li><p>to address the conflict between MathJax and Hexo</p>
   <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span>npm uninstall hexo-renderer-marked --save</span><br><span class="line"><span class="meta">$</span>npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>push posts to the top</p>
<ul>
<li><code>$ npm install hexo-generator-index-pin-top --save</code>.</li>
<li>for the target post, add <code>top: true</code> in Front-matter, or <code>top: 10</code> with larger number indicating higher priority.</li>
</ul>
</li>
<li><p>insert code block</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% codeblock lang:python %&#125;</span><br><span class="line">code snippet</span><br><span class="line">&#123;% endcodeblock %&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>insert image</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;img src=&quot;http://bcmi.sjtu.edu.cn/~niuli/github_images/t0IXoZq.jpg&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>Tips:</strong> </p>
<ol>
<li><p>If something is wrong with the representation and hard to tune, you can try deleting extra spaces or adopting an alternative format (e.g., two code block formats).</p>
</li>
<li><p>Case sensitive: sometimes you switch between capital letter and small letter, which may lead to 404 not found errors.</p>
<ul>
<li>set <code>ignorecase</code> as <code>false</code> in the file <code>.deploy_git/.git/config</code></li>
<li>clean the folder <code>.deploy_git</code></li>
<li><code>hexo clean</code> and <code>hexo d -g</code></li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Vote Aggregation</title>
    <url>/2022/06/16/machine_learning/Vote%20Aggregation/</url>
    <content><![CDATA[<p>We use the <a href="http://crowdsourcing-class.org/readings/downloads/ml/EM.pdf" target="_blank" rel="noopener">Dawid-Skene vote aggregation</a> algorithm to obtain the ground truth label for each snippet, since this is often considered ‘gold standard’ for aggregation in practice. DawidSkene is an unsupervised inference algorithm that gives the Maximum Likelihood Estimate of observer error rates using the EM algorithm.</p>
<p>1) Using the labels given by multiple annotators, estimate the most likely “correct” label for each video snippet.</p>
<p>2) Based on the estimated correct answer for each object, compute the error rates for each annotator.</p>
<p>3) Taking into consideration the error rates for each annotator, recompute the most likely “correct” label for each object.</p>
<p>4) Repeat steps 2 and 3 until one of the termination criteria is met (error rates are below a pre-specified threshold or a pre-specified number of iterations are completed).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Vector Quantization</title>
    <url>/2022/06/16/machine_learning/Vector%20Quantization/</url>
    <content><![CDATA[<p>Vector Quantization: VQVAE <a href="https://arxiv.org/pdf/1711.00937.pdf" target="_blank" rel="noopener">[1]</a>,VQVAE2 <a href="https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf" target="_blank" rel="noopener">[2]</a>, VQGAN <a href="https://arxiv.org/pdf/2012.09841.pdf" target="_blank" rel="noopener">[6]</a>. </p>
<p>Residual Quantization: RQVAE <a href="https://arxiv.org/pdf/2203.01941.pdf" target="_blank" rel="noopener">[3]</a> </p>
<p>Accelerate auto-regression: <a href="https://arxiv.org/pdf/2111.12701.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://arxiv.org/pdf/2202.04200.pdf" target="_blank" rel="noopener">[5]</a></p>
<p>Hierarchical residual quantization: VAR <a href="https://arxiv.org/pdf/2404.02905" target="_blank" rel="noopener">[7]</a></p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Oord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu. “Neural discrete representation learning.” arXiv preprint arXiv:1711.00937 (2017).</p>
<p>[2] Razavi, Ali, Aaron van den Oord, and Oriol Vinyals. “Generating diverse high-fidelity images with vq-vae-2.” Advances in neural information processing systems. 2019.</p>
<p>[3] Lee, Doyup, et al. “Autoregressive Image Generation using Residual Quantization.” arXiv preprint arXiv:2203.01941 (2022).</p>
<p>[4] Bond-Taylor, Sam, et al. “Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes.” arXiv preprint arXiv:2111.12701 (2021).</p>
<p>[5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman, “MaskGIT: Masked Generative Image Transformer”, arXiv preprint arXiv:2202.04200.</p>
<p>[6] Patrick Esser, Robin Rombach, Björn Ommer, “Taming Transformers for High-Resolution Image Synthesis”.</p>
<p>[7] Tian, Keyu, et al. “Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction.” arXiv preprint arXiv:2404.02905 (2024).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Unsupervised Attribute Learning</title>
    <url>/2022/06/16/machine_learning/Unsupervised%20Attribute%20Learning/</url>
    <content><![CDATA[<ol>
<li><p>learn attribute vector based on the relation and difference between different categories (each dimension if uninterpretable): <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yu_Designing_Category-Level_Attributes_2013_CVPR_paper.pdf" target="_blank" rel="noopener">[1]</a> (Laplacian matrix),  <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Discriminative_Learning_of_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[2]</a> (triplet loss)</p>
</li>
<li><p>exploit local information and encode them into attribute vector (each dimension is interpretable): <a href="https://link.springer.com/chapter/10.1007/978-3-642-33709-3_6#copyrightInformation" target="_blank" rel="noopener">[3]</a> (discriminative cluster, doublets),  <a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Unsupervised_Learning_of_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[4]</a> (joint attribute learning and feature learning)</p>
</li>
<li><p>learn attention map for each latent attribute <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Towards_Rich_Feature_Discovery_With_Class_Activation_Maps_Augmentation_for_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><p>Yu, Felix X., et al. “Designing category-level attributes for discriminative visual recognition.” CVPR, 2013.</p>
</li>
<li><p>Li, Yan, et al. “Discriminative learning of latent features for zero-shot recognition.” CVPR, 2018.</p>
</li>
<li><p>Singh, Saurabh, Abhinav Gupta, and Alexei A. Efros. “Unsupervised discovery of mid-level discriminative patches.” ECCV, 2012.</p>
</li>
<li><p>Huang, Chen, Chen Change Loy, and Xiaoou Tang. “Unsupervised learning of discriminative attributes and visual representations.” CVPR, 2016.</p>
</li>
<li><p>Yang, Wenjie, et al. “Towards rich feature discovery with class activation maps augmentation for person re-identification.” CVPR, 2019.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>attribute</tag>
      </tags>
  </entry>
  <entry>
    <title>Training Categories and Test Categories</title>
    <url>/2022/06/16/machine_learning/Training%20Categories%20and%20Test%20Categories/</url>
    <content><![CDATA[<p>Let us use $S$ to denote the set of training categories and $T$ to denote the set of testing categories.</p>
<ul>
<li>$S=T$: the most common case</li>
<li>$S\cap T=\emptyset$: zero-shot learning</li>
<li>$S\subset T$: generalized zero-shot learning</li>
<li>$S\supset T$: pretrained model</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Synthetic Text Images</title>
    <url>/2022/06/16/machine_learning/Synthetic%20Text%20Images/</url>
    <content><![CDATA[<ol>
<li><p>Blend text and background images. </p>
<ul>
<li>text image (font, color, border, blending): <a href="https://arxiv.org/pdf/1406.2227.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>scene-text image (font, color, border, blending, <u>geometry</u>): <a href="http://www.robots.ox.ac.uk/~ankush/textloc.pdf" target="_blank" rel="noopener">[2]</a> <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Fangneng_Zhan_Verisimilar_Image_Synthesis_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[3]</a></li>
</ul>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h3><ol>
<li><p>Jaderberg, Max, et al. “Synthetic data and artificial neural networks for natural scene text recognition.” arXiv preprint arXiv:1406.2227 (2014).</p>
</li>
<li><p>Gupta, Ankush, Andrea Vedaldi, and Andrew Zisserman. “Synthetic data for text localisation in natural images.” CVPR, 2016.</p>
</li>
<li><p>Zhan, Fangneng, Shijian Lu, and Chuhui Xue. “Verisimilar image synthesis for accurate detection and recognition of texts in scenes.” ECCV, 2018.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>scene text</tag>
      </tags>
  </entry>
  <entry>
    <title>Subjective Annotation</title>
    <url>/2022/06/16/machine_learning/Subjective%20Annotation/</url>
    <content><![CDATA[<p>As mentioned in <a href="https://arxiv.org/pdf/1606.01621.pdf" target="_blank" rel="noopener">[1]</a> One major concern of subjective annotation is that the annotations provided by different workers for each image may not be reliable, which calls for consistency analysis on the annotations. We use Spearman’s rank correlation ρ between pairs of workers to measure consistency and estimate p-values to evaluate statistical significance of the correlation relative to a null hypothesis of uncorrelated responses. We use the Benjamini-Hochberg<br>procedure to control the false discovery rate (FDR) for multiple comparisons <a href="ftp://swanson.seas.upenn.edu/pub/datamining/public_html/ReadingGroup/papers/fdr-dep.pdf" target="_blank" rel="noopener">[2]</a>. At an FDR level of 0.05, we find 98.45% batches have significant agreement among raters. Further consistency analysis of the dataset can be found in the supplementary material of <a href="https://arxiv.org/pdf/1606.01621.pdf" target="_blank" rel="noopener">[1]</a>.</p>
<p>[1] Kong, Shu, et al. “Photo aesthetics ranking network with attributes and content adaptation.” European Conference on Computer Vision. Springer, Cham, 2016.</p>
<p>[2] Benjamini, Yoav, and Daniel Yekutieli. “The control of the false discovery rate in multiple testing under dependency.” Annals of statistics (2001): 1165-1188.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Soft Loss</title>
    <url>/2022/06/16/machine_learning/Soft%20Loss/</url>
    <content><![CDATA[<p>Given the predicted softmax logits $p_i$, ground-truth softmax logits or free-form weights $w_i$.</p>
<ol>
<li><p>weighted softmax loss: $-\sum_{i} w_i \log p_i$</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1611.05916.pdf" target="_blank" rel="noopener">EMD softmax loss</a>: $-\sum_{i} w_i p_i$</p>
</li>
<li><p>softmax loss after <a href="https://arxiv.org/pdf/1406.2080.pdf" target="_blank" rel="noopener">label flip layer</a>: $-\log{\sum_{i} w_i p_i}$</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noopener">knowledge distillation</a>: $\sum_{i} (p_i-w_i)$</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Shadow-related Application</title>
    <url>/2022/06/16/machine_learning/Shadow-related%20Application/</url>
    <content><![CDATA[<h3 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h3><ul>
<li><p>Shadow detection: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[1]</a> </p>
</li>
<li><p>Object-shadow pair detection/matting: <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Shadow_Detection_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Single-Stage_Instance_Shadow_Detection_With_Bidirectional_Relation_Learning_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[10]</a> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Lu_Omnimatte_Associating_Objects_and_Their_Effects_in_Video_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[11]</a></p>
</li>
<li><p>Shadow removal: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Mask-ShadowGAN_Learning_to_Remove_Shadows_From_Unpaired_Data_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[3]</a> <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Le_Shadow_Removal_via_Shadow_Image_Decomposition_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://arxiv.org/pdf/1911.08718.pdf" target="_blank" rel="noopener">[5]</a> <a href="https://arxiv.org/pdf/2008.00267.pdf" target="_blank" rel="noopener">[6]</a></p>
</li>
<li><p>Shadow generation: <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_ARShadowGAN_Shadow_Generative_Adversarial_Network_for_Augmented_Reality_in_Single_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[7]</a> <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Zhan_Adversarial_Image_Composition_with_Auxiliary_Illumination_ACCV_2020_paper.pdf" target="_blank" rel="noopener">[8]</a></p>
</li>
<li><p>Remove occluder and its associated shadow <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_No_Shadow_Left_Behind_Removing_Objects_and_Their_Shadows_Using_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[9]</a></p>
</li>
</ul>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>Shadow Generation</p>
<ol>
<li>Shadow-AR (rendered) <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_ARShadowGAN_Shadow_Generative_Adversarial_Network_for_Augmented_Reality_in_Single_CVPR_2020_paper.pdf" target="_blank" rel="noopener">paper</a></li>
<li>RGB-AO-depth (rendered) <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/cgf.13943" target="_blank" rel="noopener">paper</a></li>
<li>Composition datasets: WILDTRACK, Penn-Fudan, UA-DETRAC, Cityscapes, ShapeNet <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Zhan_Adversarial_Image_Composition_with_Auxiliary_Illumination_ACCV_2020_paper.pdf" target="_blank" rel="noopener">paper</a></li>
<li>Soft shadow dataset (rendered) <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Sheng_SSN_Soft_Shadow_Network_for_Image_Compositing_CVPR_2021_paper.pdf" target="_blank" rel="noopener">paper</a></li>
<li>ShadowGAN (rendered, 12,400 rendered images, 9265 objects, 110 textures for rendering the plane, up to four objects in each scene) <a href="https://dc.tsinghuajournals.com/cgi/viewcontent.cgi?article=1127&amp;context=computational-visual-media" target="_blank" rel="noopener">paper</a></li>
<li>SID (single object, 25, 000 images, 12, 500 3D objects, 50 homogeneous color and 200 variable set of textured patterns) <a href="https://arxiv.org/pdf/2009.06295.pdf" target="_blank" rel="noopener">paper</a></li>
<li>SID2 (45,000 images, similar to SID, more than one object in each scene) <a href="https://arxiv.org/pdf/2009.08941.pdf" target="_blank" rel="noopener">paper</a></li>
<li>SHAD3S <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Venkataramaiyer_SHAD3S_A_Model_to_Sketch_Shade_and_Shadow_WACV_2021_paper.pdf" target="_blank" rel="noopener">paper</a></li>
<li>DESOBA <a href="https://www.aaai.org/AAAI22Papers/AAAI-1360.HongY.pdf" target="_blank" rel="noopener">paper</a></li>
</ol>
<p>Shadow Removal/Detection</p>
<ol>
<li>ISTD/ ISTD+ (1870 0 triplets of shadow, shadow mask and shadow-free images) <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf" target="_blank" rel="noopener">paper</a></li>
<li>USR(unpaired, 2,445 shadow images, 1,770 shadow-free) <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Mask-ShadowGAN_Learning_to_Remove_Shadows_From_Unpaired_Data_ICCV_2019_paper.pdf" target="_blank" rel="noopener">paper</a></li>
<li>SRD/ SRD+ (3088 pairs, paired shadow and shadow-free, without the ground-truth shadow mask) <a href="">paper</a></li>
<li>LRSS (37 image pairs, soft shadow) <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Qu_DeshadowNet_A_Multi-Context_CVPR_2017_paper.pdf" target="_blank" rel="noopener">paper</a></li>
<li>UIUC (76 pairs, paired shadow/shadow-free) <a href="https://dl.acm.org/doi/pdf/10.1145/2732407" target="_blank" rel="noopener">paper</a></li>
<li>GTAV (5723 pairs, 5110 daylight scenes, occlude objects inside camera) <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Sidorov_Conditional_GANs_for_Multi-Illuminant_Color_Constancy_Revolution_or_yet_Another_CVPRW_2019_paper.pdf" target="_blank" rel="noopener">paper</a></li>
<li>SynShadow (based on USR, occlude objects outside camera, shadow/shadow-free/matte image triplets synthesized from rendered 10,000 matte images and about 1,800 background images) <a href="https://arxiv.org/pdf/2101.01713.pdf" target="_blank" rel="noopener">paper</a></li>
<li>UCF (245 pairs, shadow/shadow mask, only for detection)</li>
<li>SBU (4727 pairs, shadow/shadow mask, only for detection)</li>
<li>CUHK-Shadow (10,500 pairs, shadow/shadow mask, only for detection) <a href="https://arxiv.org/pdf/1911.06998.pdf" target="_blank" rel="noopener">paper</a></li>
<li>SOBA (1013 images) <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Shadow_Detection_CVPR_2020_paper.pdf" target="_blank" rel="noopener">paper</a></li>
<li>AISD (514 pairs, shadow/shadow mask, only for detection, areial images) <a href="https://doi.org/10.1016/j.isprsjprs.2020.07.016" target="_blank" rel="noopener">paper</a></li>
<li>video shadow removal dataset (8 videos, shadow/shadow mask/shadow free) <a href="https://link.springer.com/content/pdf/10.1007/978-3-030-58621-8_16.pdf" target="_blank" rel="noopener">paper</a></li>
<li>CMU dataset(135 pairs, shadow/shadow boundaries) <a href="https://link.springer.com/content/pdf/10.1007/978-3-642-15552-9_24.pdf" target="_blank" rel="noopener">paper</a></li>
<li>ViSha (120 videos with 11685 frames) <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Triple-Cooperative_Video_Shadow_Detection_CVPR_2021_paper.pdf" target="_blank" rel="noopener">paper</a></li>
<li>VISAD (82 videos, half-annotated) <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Video_Shadow_Detection_via_Spatio-Temporal_Interpolation_Consistency_Training_CVPR_2022_paper.pdf" target="_blank" rel="noopener">paper</a></li>
</ol>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol>
<li><p>Zhu, Lei, et al. “Bidirectional feature pyramid network with recurrent attention residual modules for shadow detection.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.</p>
</li>
<li><p>Wang, Tianyu, et al. “Instance shadow detection.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>
</li>
<li><p>Hu, Xiaowei, et al. “Mask-ShadowGAN: Learning to remove shadows from unpaired data.” Proceedings of the IEEE International Conference on Computer Vision. 2019.</p>
</li>
<li><p>Le, Hieu, and Dimitris Samaras. “Shadow removal via shadow image decomposition.” Proceedings of the IEEE International Conference on Computer Vision. 2019.</p>
</li>
<li><p>Xiaodong, Cun, Pun Chi-Man, and Shi Cheng. “Towards Ghost-free Shadow Removal via Dual Hierarchical Aggregation Network and Shadow Matting GAN.” arXiv preprint arXiv:1911.08718 (2019).</p>
</li>
<li><p>Le, Hieu, and Dimitris Samaras. “From Shadow Segmentation to Shadow Removal.” European Conference on Computer Vision. Springer, Cham, 2020.</p>
</li>
<li><p>Liu, Daquan, et al. “ARShadowGAN: Shadow Generative Adversarial Network for Augmented Reality in Single Light Scenes.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>
</li>
<li><p>Zhan, Fangneng, et al. “Adversarial Image Composition with Auxiliary Illumination.” Proceedings of the Asian Conference on Computer Vision. 2020.</p>
</li>
<li><p>Zhang, Edward, et al. “No Shadow Left Behind: Removing Objects and their Shadows using Approximate Lighting and Geometry.” CVPR, 2021.</p>
</li>
<li><p>Wang, Tianyu, et al. “Single-stage instance shadow detection with bidirectional relation learning.” CVPR, 2021.</p>
</li>
<li><p>Lu, Erika, et al. “Omnimatte: Associating objects and their effects in video.” CVPR, 2021.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>shadow</tag>
      </tags>
  </entry>
  <entry>
    <title>Outlier Detection</title>
    <url>/2022/06/16/machine_learning/Outlier%20Detection/</url>
    <content><![CDATA[<h3 id="Statistical-methods"><a href="#Statistical-methods" class="headerlink" title="Statistical methods"></a>Statistical methods</h3><ul>
<li>use a model (e.g., Gaussian) to fit the distribution of all data </li>
<li>use two models to fit the distributions of non-outliers and outliers separately</li>
<li>Grubbs’ test</li>
</ul>
<h3 id="Distance-based-methods"><a href="#Distance-based-methods" class="headerlink" title="Distance based methods"></a>Distance based methods</h3><ul>
<li>the density within a neighborhood</li>
<li>the distance from a nearest neighbor</li>
</ul>
<h3 id="Learning-based-method"><a href="#Learning-based-method" class="headerlink" title="Learning based method"></a>Learning based method</h3><ul>
<li>clustering, the smallest cluster is likely to contain outliers</li>
<li>one-class classifier (e.g., one-class SVM)</li>
<li>binary classifier (e.g., naive bayes for spam filtering, weighted binary SVM)</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>outlier detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Optical Flow</title>
    <url>/2022/06/16/machine_learning/Optical%20Flow/</url>
    <content><![CDATA[<ol>
<li><p>Estimate optical flow based on video: FlowNet <a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[1]</a>, FlowNet2 <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Ilg_FlowNet_2.0_Evolution_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>Estimate optical flow based on image: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Im2Flow_Motion_Hallucination_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://arxiv.org/pdf/1803.06951.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Walker_Dense_Optical_Flow_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
</ol>
<p>[1] Dosovitskiy, Alexey, et al. “Flownet: Learning optical flow with convolutional networks.” ICCV, 2015.</p>
<p>[2] Ilg, Eddy, et al. “Flownet 2.0: Evolution of optical flow estimation with deep networks.” CVPR, 2017.</p>
<p>[3] Gao, Ruohan, Bo Xiong, and Kristen Grauman. “Im2flow: Motion hallucination from static images for action recognition.” CVPR, 2018.</p>
<p>[4] Silvia L. Pintea, Jan C. van Gemert, and Arnold W. M. Smeulders, “Deja Vu: Motion Prediction in Static Images”, arxiv, 2018.</p>
<p>[5] Walker, Jacob, Abhinav Gupta, and Martial Hebert. “Dense optical flow prediction from a static image.” ICCV, 2015.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>optical flow</tag>
      </tags>
  </entry>
  <entry>
    <title>Normalization</title>
    <url>/2022/06/16/machine_learning/Normalization/</url>
    <content><![CDATA[<p><strong>Normalize weights:</strong></p>
<ol>
<li>weight normalization <a href="https://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf" target="_blank" rel="noopener">[1]</a>:  $\mathbf{w}=\frac{g}{|\mathbf{v}|} \mathbf{v}$, weight normalization can be viewed as a cheaper and less noisy approximation to batch normalization</li>
</ol>
<p><strong>Normalize outputs:</strong></p>
<ol>
<li><p>batch normalization <a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">[2]</a>: make the input and output have the same variance</p>
</li>
<li><p>layer normalization <a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
<li><p>instance normalization <a href="https://arxiv.org/pdf/1607.08022.pdf" target="_blank" rel="noopener">[4]</a></p>
</li>
<li><p>group normalization <a href="https://arxiv.org/pdf/1803.08494.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
</ol>
<p><img src="https://i.imgur.com/5rzQSsn.jpg" width="80%"></p>
<p>N as the batch axis, C as the channel axis, and (H, W)<br>as the spatial axes</p>
<p>[1] Salimans T, Kingma D P. Weight normalization: A simple reparameterization to accelerate training of deep neural networks[C]//Advances in Neural Information Processing Systems. 2016: 901-909.</p>
<p>[2] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.</p>
<p>[3] Ba J L, Kiros J R, Hinton G E. Layer normalization[J]. arXiv preprint arXiv:1607.06450, 2016.</p>
<p>[4] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv:1607.08022, 2016.</p>
<p>[5] Wu Y, He K. Group normalization[J]. arXiv preprint arXiv:1803.08494, 2018.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Mutual Information</title>
    <url>/2022/06/16/machine_learning/Mutual%20Information/</url>
    <content><![CDATA[<ol>
<li><a href="https://arxiv.org/pdf/1612.00410.pdf" target="_blank" rel="noopener">[1]</a>: use KL divergence as the upper-bound of mutual information (MI), which can be used to minimize MI. r(z) can be set as unit Gaussian for simplicity.</li>
</ol>
<center><img src="https://i.imgur.com/i3j6VrX.png" width="50%" border="0"></center>

<ol>
<li>MINE<a href="http://proceedings.mlr.press/v80/belghazi18a/belghazi18a.pdf" target="_blank" rel="noopener">[2]</a>: lower-bound of MI based on KL divergence. Due to strong consistency, MINE can be used as a tight estimation of MI.</li>
</ol>
<center><img src="https://i.imgur.com/e6EaQpZ.png" width="50%" border="0"></center>


<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol>
<li><p>Alemi, Alexander A., et al. “Deep variational information bottleneck.” arXiv preprint arXiv:1612.00410 (2016).</p>
</li>
<li><p>Belghazi, M. I., Baratin, A., Rajeshwar, S., Ozair, S., Bengio, Y., Courville, A., &amp; Hjelm, D. Mutual information neural estimation, ICML, 2018.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>MI</tag>
      </tags>
  </entry>
  <entry>
    <title>Meta Learning</title>
    <url>/2022/06/16/machine_learning/Meta%20Learning/</url>
    <content><![CDATA[<h3 id="Taxonomy"><a href="#Taxonomy" class="headerlink" title="Taxonomy"></a>Taxonomy</h3><p>1) <strong>metric-based:</strong>  learn a good metric</p>
<ul>
<li>matching network <a href="https://arxiv.org/pdf/1606.04080.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>relation network <a href="https://arxiv.org/pdf/1711.06025.pdf" target="_blank" rel="noopener">[2]</a></li>
<li>prototypical network <a href="http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://arxiv.org/pdf/1803.00676.pdf" target="_blank" rel="noopener">[4]</a></li>
</ul>
<p>2) <strong>optimization-based:</strong> gradient</p>
<ul>
<li>Meta-Learner LSTM <a href="https://openreview.net/pdf?id=rJY0-Kcll" target="_blank" rel="noopener">[5]</a></li>
<li>MAML <a href="https://arxiv.org/pdf/1703.03400.pdf" target="_blank" rel="noopener">[6]</a> <a href="https://arxiv.org/pdf/1710.11622.pdf" target="_blank" rel="noopener">[7]</a> <a href="https://arxiv.org/pdf/1801.08930.pdf" target="_blank" rel="noopener">[8]</a></li>
<li><p>REPTILE (an approximation of MAML) <a href="https://arxiv.org/abs/1803.02999" target="_blank" rel="noopener">[9]</a></p>
<center><img src="https://i.imgur.com/ZYN2cQ5.png" width="60%" border="0"></center>

<p>Optimization based methods aim to obtain good parameter initilization. If we simply train multiple tasks, the obtained model parameters may lead to sub optimum for each task. </p>
<center><img src="https://i.imgur.com/FBax49a.jpg" width="60%" border="0"></center>

</li>
</ul>
<p>3) <strong>model-based:</strong> predict model parameters</p>
<ul>
<li>MANN <a href="http://proceedings.mlr.press/v48/santoro16.pdf" target="_blank" rel="noopener">[10]</a></li>
<li>MetaNet <a href="https://arxiv.org/pdf/1703.00837.pdf" target="_blank" rel="noopener">[11]</a></li>
</ul>
<p><strong>Reference:</strong></p>
<ol>
<li>Vinyals, Oriol, et al. “Matching networks for one shot learning.” NIPS, 2016.</li>
<li>Sung, Flood, et al. “Learning to compare: Relation network for few-shot learning.” CVPR, 2018.</li>
<li>Snell, Jake, Kevin Swersky, and Richard Zemel. “Prototypical networks for few-shot learning.” NIPS, 2017.</li>
<li>Ren, Mengye, et al. “Meta-learning for semi-supervised few-shot classification.” arXiv preprint arXiv:1803.00676 (2018).</li>
<li>Sachin Ravi and Hugo Larochelle. “Optimization as a Model for Few-Shot Learning.” ICLR, 2017.</li>
<li>Chelsea Finn, Pieter Abbeel, and Sergey Levine. “Model-agnostic meta-learning for fast adaptation of deep networks.” ICML, 2017.</li>
<li>Finn, Chelsea, and Sergey Levine. “Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm.” arXiv preprint arXiv:1710.11622 (2017).</li>
<li>Grant, Erin, et al. “Recasting gradient-based meta-learning as hierarchical bayes.” arXiv preprint arXiv:1801.08930 (2018).</li>
<li>A. Nichol, J. Achiam, and J. Schulman. On first-order meta-learning algorithms. arXiv, 1803.02999v2, 2018.</li>
<li>Adam Santoro, et al. “Meta-learning with memory-augmented neural networks.” ICML. 2016.</li>
<li>Munkhdalai, Tsendsuren, and Hong Yu. “Meta networks.” ICML, 2017.</li>
</ol>
<h3 id="Tutorials"><a href="#Tutorials" class="headerlink" title="Tutorials:"></a>Tutorials:</h3><ul>
<li><p><a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html</a></p>
</li>
<li><p><a href="http://metalearning-symposium.ml/files/vinyals.pdf" target="_blank" rel="noopener">http://metalearning-symposium.ml/files/vinyals.pdf</a></p>
</li>
<li><p><a href="https://github.com/floodsung/Meta-Learning-Papers" target="_blank" rel="noopener">https://github.com/floodsung/Meta-Learning-Papers</a></p>
</li>
<li><p>A more comprehensive survey: <a href="https://arxiv.org/pdf/1810.03548.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.03548.pdf</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>meta learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Lifelong Learning</title>
    <url>/2022/06/16/machine_learning/Lifelong%20Learning/</url>
    <content><![CDATA[<p>Related concepts: online learning, incremental learning, continual learning</p>
<h3 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h3><ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0893608019300231" target="_blank" rel="noopener">Continual lifelong learning with neural networks: A review</a></li>
<li><a href="https://homes.esat.kuleuven.be/~konijn/publications/2020/DeLange2.pdf" target="_blank" rel="noopener">A continual learning survey: Defying forgetting in classification tasks</a></li>
<li><a href="https://arxiv.org/pdf/2010.15277" target="_blank" rel="noopener">Class-incremental learning: survey and performance evaluation</a></li>
</ul>
<h3 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h3><ul>
<li><a href="https://cs330.stanford.edu/slides/cs330_lifelonglearning_karol.pdf" target="_blank" rel="noopener">Stanford course</a></li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>lifelong learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Latent Variable Regularization</title>
    <url>/2022/06/16/machine_learning/Latent%20Variable%20Regularization/</url>
    <content><![CDATA[<p>Regulating latent variables or latent features can improve the generalizability of classifier and lower the error bound. </p>
<p>Regulating latent variables is essentially decrease the entropy of latent variables. There are some common tricks to decrease the entropy of latent variables, for example,</p>
<ol>
<li>dropout</li>
<li>weight decay</li>
<li>add random noise to the latent variables in VAE and GAN.</li>
<li>add random perturbation to model parameters</li>
</ol>
<p>For theoretical proof, please refer to <a href="http://bcmi.sjtu.edu.cn/~niuli/download/information_theory_for_latent_variable_regulation.pdf" target="_blank" rel="noopener">here</a>.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Vote Aggregation</title>
    <url>/2022/06/16/machine_learning/Label%20Vector%20Interpolation/</url>
    <content><![CDATA[<ol>
<li><p>label smoothing: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[1]</a> interpolating ground-truth label and uniform label</p>
</li>
<li><p>bootstrapping: <a href="https://arxiv.org/pdf/1412.6596.pdf" target="_blank" rel="noopener">[2]</a> interpolate noisy label and label from previous iteration</p>
</li>
<li><p>noisy data+clean data: <a href="https://arxiv.org/pdf/1703.02391.pdf" target="_blank" rel="noopener">[3]</a> interpolate noisy label and distilled label</p>
</li>
</ol>
<p>[1] Szegedy, Christian, et al. “Rethinking the inception architecture for computer vision.” CVPR, 2016.</p>
<p>[2] Reed, Scott, et al. “Training deep neural networks on noisy labels with bootstrapping.” arXiv preprint arXiv:1412.6596 (2014).</p>
<p>[3] Li, Yuncheng, et al. “Learning from noisy labels with distillation.” ICCV, 2017.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Knowledge Graph</title>
    <url>/2022/06/16/machine_learning/Knowledge%20Graph/</url>
    <content><![CDATA[<p>Definition: entities, attributes, and relationships</p>
<p>Two ways to construct knowledge graph:</p>
<ol>
<li><p>probabilistic models (graphical model/random walk)</p>
</li>
<li><p>embedding based models</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>knowledge graph</tag>
      </tags>
  </entry>
  <entry>
    <title>Incremental SVM</title>
    <url>/2022/06/16/machine_learning/Incremental%20SVM/</url>
    <content><![CDATA[<p>1) Approximate incremental SVM: pass through the dataset many times</p>
<ul>
<li><p>Pegasos: select a training batch in each iteration</p>
<ul>
<li>python: <a href="https://github.com/ejlb/pegasos" target="_blank" rel="noopener">https://github.com/ejlb/pegasos</a><pre><code>   https://github.com/avaitla/Pegasos
</code></pre></li>
<li>C: <a href="https://www.cs.huji.ac.il/~shais/code/index.html" target="_blank" rel="noopener">https://www.cs.huji.ac.il/~shais/code/index.html</a></li>
<li>matlab: <a href="https://www.mathworks.com/matlabcentral/fileexchange/31401-pegasos-primal-estimated-sub-gradient-solver-for-svm?focused=5188208&amp;tab=function" target="_blank" rel="noopener">https://www.mathworks.com/matlabcentral/fileexchange/31401-pegasos-primal-estimated-sub-gradient-solver-for-svm?focused=5188208&amp;tab=function</a></li>
</ul>
</li>
<li><p>sklearn.linear_model: SGD</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf= sklearn.linear_model.SGDClassifier(learning_rate = <span class="string">'constant'</span>, eta0 = <span class="number">0.1</span>, shuffle = <span class="keyword">False</span>, n_iter = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># get x1, y1 as a new instance</span></span><br><span class="line">clf.partial_fit(x1, y1)</span><br><span class="line"><span class="comment"># get x2, y2</span></span><br><span class="line"><span class="comment"># update accuracy if needed</span></span><br><span class="line">clf.partial_fit(x2, y2)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>2) Exact incremental or decremental SVM: only pass through the dataset once</p>
<ul>
<li><p>Incremental and Decremental Support Vector Machine Learning<br><a href="http://www.isn.ucsd.edu/svm/incremental" target="_blank" rel="noopener">http://www.isn.ucsd.edu/svm/incremental</a></p>
</li>
<li><p>SVM Incremental Learning, Adaptation and Optimization: extend the work above<br>matlab: <a href="https://github.com/diehl/Incremental-SVM-Learning-in-MATLAB" target="_blank" rel="noopener">https://github.com/diehl/Incremental-SVM-Learning-in-MATLAB</a></p>
</li>
<li><p>Incremental and decremental training for linear classification: extension of liblinear focusing on linear problem<br><a href="http://www.csie.ntu.edu.tw/~cjlin/papers/ws/index.html" target="_blank" rel="noopener">http://www.csie.ntu.edu.tw/~cjlin/papers/ws/index.html</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Implicit Modelling</title>
    <url>/2022/06/16/machine_learning/Implicit%20Modelling/</url>
    <content><![CDATA[<ol>
<li>simulates an infinite-depth network by fixed point iteration $h=f_{\theta}(h;x)$, in which $x$ is initial input, $\theta$ is the model parameter of one-time transformation. After infinite times of transformations, $x$ will approach the fixed point $h$. DEQ<a href="https://papers.nips.cc/paper/2019/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf" target="_blank" rel="noopener">[1]</a>, MDEQ<a href="https://arxiv.org/pdf/2006.08656.pdf" target="_blank" rel="noopener">[2]</a>, iFPN<a href="https://arxiv.org/pdf/2012.13563.pdf" target="_blank" rel="noopener">[3]</a></li>
</ol>
<p><strong>Reference:</strong></p>
<ol>
<li>Bai, Shaojie, J. Zico Kolter, and Vladlen Koltun. “Deep equilibrium models.” Advances in Neural Information Processing Systems. 2019.</li>
<li>Bai, Shaojie, Vladlen Koltun, and J. Zico Kolter. “Multiscale deep equilibrium models.” arXiv preprint arXiv:2006.08656 (2020).</li>
<li>Wang, Tiancai, Xiangyu Zhang, and Jian Sun. “Implicit Feature Pyramid Network for Object Detection.” arXiv preprint arXiv:2012.13563 (2020).</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Image Matching</title>
    <url>/2022/06/16/machine_learning/Image%20Matching/</url>
    <content><![CDATA[<h3 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h3><ul>
<li><a href="https://link.springer.com/article/10.1007/s11263-020-01359-2" target="_blank" rel="noopener">Image Matching from Handcrafted to Deep Features: A Survey</a></li>
</ul>
<h3 id="Deep-learning-methods"><a href="#Deep-learning-methods" class="headerlink" title="Deep learning methods"></a>Deep learning methods</h3><ul>
<li>Correlation tensor: <a href="https://arxiv.org/pdf/1810.10510" target="_blank" rel="noopener">[1]</a> <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Rocco_Convolutional_Neural_Network_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://nyuscholars.nyu.edu/en/publications/arbicon-net-arbitrary-continuous-geometric-transformation-network" target="_blank" rel="noopener">[3]</a></li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>Rocco, Ignacio, et al. “Neighbourhood consensus networks.” arXiv preprint arXiv:1810.10510 (2018).</li>
<li>Rocco, Ignacio, Relja Arandjelovic, and Josef Sivic. “Convolutional neural network architecture for geometric matching.” CVPR, 2017.</li>
<li>Chen, Jianchun, et al. “Arbicon-net: Arbitrary continuous geometric transformation networks for image registration.” NIPS, 2019.</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Image and Video Proposals</title>
    <url>/2022/06/16/machine_learning/Image%20and%20Video%20Proposals/</url>
    <content><![CDATA[<h2 id="Image-proposals"><a href="#Image-proposals" class="headerlink" title="Image proposals:"></a>Image proposals:</h2><ul>
<li><p>Selective search <a href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" target="_blank" rel="noopener">[1]</a>: hierarchical grouping based on different similarity metrics <a href="http://disi.unitn.it/~uijlings/MyHomepage/index.php#page=projects1" target="_blank" rel="noopener">[code]</a></p>
</li>
<li><p>Salient object detection <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.306.6986&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[2]</a>: identify the segment  which is easy to compose from itself but hard from remaining parts of the image.</p>
</li>
<li><p>EdgeBox <a href="https://pdollar.github.io/files/papers/ZitnickDollarECCV14edgeBoxes.pdf" target="_blank" rel="noopener">[3]</a>: identify the boxes that tightly enclose a set of edges are likely to contain an object.</p>
</li>
<li><p>ACF detector <a href="https://pdollar.github.io/files/papers/DollarPAMI14pyramids.pdf" target="_blank" rel="noopener">[4]</a>: compute gradient histograms on image pyramids</p>
</li>
<li><p>Region Proposal Network (RPN) from faster-RCNN <a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
</ul>
<h2 id="Video-proposals"><a href="#Video-proposals" class="headerlink" title="Video proposals:"></a>Video proposals:</h2><ul>
<li><p>Video edgebox <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_A_Key_Volume_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[1]</a>: an extension of EdgeBox</p>
</li>
<li><p>RC3D <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Xu_R-C3D_Region_Convolutional_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[2]</a>: an extension of RPN</p>
</li>
</ul>
<p>[1] Zhu, Wangjiang, et al. “A key volume mining deep framework for action recognition.” CVPR. 2016.</p>
<p>[2] Xu, Huijuan, Abir Das, and Kate Saenko. “R-c3d: Region convolutional 3d network for temporal activity detection.” ICCV, 2017.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3>]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>proposal</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph Neural Network</title>
    <url>/2022/06/16/machine_learning/Graph%20Neural%20Network/</url>
    <content><![CDATA[<h2 id="Representative-methods"><a href="#Representative-methods" class="headerlink" title="Representative methods"></a>Representative methods</h2><p><a href="http://bcmi.sjtu.edu.cn/~niuli/download/AAAI_tutorial_GNN.pdf" target="_blank" rel="noopener">Tutorial slides on AAAI2019</a> </p>
<ul>
<li>Graph Convolutional Network (GCN) <a href="https://arxiv.org/abs/1609.02907" target="_blank" rel="noopener">[6]</a></li>
<li>Graph Attention Network <a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener">[7]</a></li>
<li>GraphSAGE(SAmple and aggreGatE) <a href="https://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs.pdf" target="_blank" rel="noopener">[8]</a></li>
<li>Transformer <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">[9]</a>: transformer can be deemed as a type of GNN. </li>
</ul>
<h2 id="Avoid-oversmoothing-and-go-deeper"><a href="#Avoid-oversmoothing-and-go-deeper" class="headerlink" title="Avoid oversmoothing and go deeper"></a>Avoid oversmoothing and go deeper</h2><ul>
<li><p>Initial residual and Identity mapping <a href="http://xxx.itp.ac.cn/pdf/2007.02133v1" target="_blank" rel="noopener">[12]</a></p>
</li>
<li><p>GCN and PageRank <a href="http://xxx.itp.ac.cn/pdf/1810.05997v5" target="_blank" rel="noopener">[13]</a></p>
</li>
</ul>
<h2 id="Graph-similarity-matching"><a href="#Graph-similarity-matching" class="headerlink" title="Graph similarity/matching"></a>Graph similarity/matching</h2><p>A survey on graph similarity <a href="https://arxiv.org/pdf/1912.11615.pdf" target="_blank" rel="noopener">[4]</a></p>
<h2 id="Graph-transformation"><a href="#Graph-transformation" class="headerlink" title="Graph transformation:"></a>Graph transformation:</h2><p>pooling/unpooling <a href="https://arxiv.org/pdf/1905.05178.pdf" target="_blank" rel="noopener">[5]</a></p>
<h2 id="Dynamic-Graph"><a href="#Dynamic-Graph" class="headerlink" title="Dynamic Graph:"></a>Dynamic Graph:</h2><ul>
<li>Pointer Graph Network <a href="https://arxiv.org/pdf/2006.06380.pdf" target="_blank" rel="noopener">[11]</a></li>
</ul>
<h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><ul>
<li><p>GNN for zero-shot learning <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Zero-Shot_Recognition_via_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[1]</a><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2486.pdf" target="_blank" rel="noopener">[2]</a>: treat each category as a graph node</p>
</li>
<li><p>GNN for multi-view learning <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Dongang_Wang_Dividing_and_Aggregating_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[3]</a>: treat each view as a graph node</p>
</li>
<li><p>GNN for clustering <a href="https://arxiv.org/pdf/2002.01633.pdf" target="_blank" rel="noopener">[10]</a></p>
</li>
</ul>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h4><ol>
<li><p>Wang, Xiaolong, Yufei Ye, and Abhinav Gupta. “Zero-shot recognition via semantic embeddings and knowledge graphs.” CVPR, 2018.</p>
</li>
<li><p>Lee, Chung-Wei, et al. “Multi-label zero-shot learning with structured knowledge graphs.” CVPR, 2018.</p>
</li>
<li><p>Wang, Dongang, et al. “Dividing and aggregating network for multi-view action recognition.” ECCV, 2018.</p>
</li>
<li><p>Ma, Guixiang, et al. “Deep Graph Similarity Learning: A Survey.” arXiv preprint arXiv:1912.11615 (2019).</p>
</li>
<li><p>Hongyang Gao, Shuiwang Ji: Graph U-Nets. CoRR abs/1905.05178 (2019)</p>
</li>
<li><p>Kipf, Thomas N., and Max Welling. “Semi-supervised classification with graph convolutional networks.” arXiv preprint arXiv:1609.02907 (2016).</p>
</li>
<li><p>Veličković, Petar, et al. “Graph attention networks.” arXiv preprint arXiv:1710.10903 (2017).</p>
</li>
<li><p>Hamilton, Will, Zhitao Ying, and Jure Leskovec. “Inductive representation learning on large graphs.” NeurIPS, 2017.</p>
</li>
<li><p>Vaswani, Ashish, et al. “Attention is all you need.” NeurIPS, 2017.</p>
</li>
<li><p>Bo, Deyu, et al. “Structural Deep Clustering Network.” Proceedings of The Web Conference 2020. 2020.</p>
</li>
<li><p>Veličković, Petar, et al. “Pointer Graph Networks.” arXiv preprint arXiv:2006.06380 (2020).</p>
</li>
<li><p>Chen, Ming, et al. “Simple and deep graph convolutional networks.” arXiv preprint arXiv:2007.02133 (2020).</p>
</li>
<li><p>Klicpera, Johannes, Aleksandar Bojchevski, and Stephan Günnemann. “Predict then propagate: Graph neural networks meet personalized pagerank.” arXiv preprint arXiv:1810.05997 (2018).</p>
</li>
<li><p>Dosovitskiy, Alexey, et al. “An image is worth 16x16 words: Transformers for image recognition at scale.” arXiv preprint arXiv:2010.11929 (2020).</p>
</li>
<li><p>Carion, Nicolas, et al. “End-to-End Object Detection with Transformers.” arXiv preprint arXiv:2005.12872 (2020).</p>
</li>
<li><p>Chen, Hanting, et al. “Pre-Trained Image Processing Transformer.” arXiv preprint arXiv:2012.00364 (2020).</p>
</li>
<li><p>Chefer, Hila, Shir Gur, and Lior Wolf. “Transformer Interpretability Beyond Attention Visualization.” arXiv preprint arXiv:2012.09838 (2020).</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN Evaluation Metric</title>
    <url>/2022/06/16/machine_learning/GAN%20Evaluation%20Metric/</url>
    <content><![CDATA[<h2 id="Objective-Evaluation"><a href="#Objective-Evaluation" class="headerlink" title="Objective Evaluation:"></a>Objective Evaluation:</h2><p>An empirical study on evaluation metrics of generative adversarial networks <a href="https://arxiv.org/pdf/1806.07755.pdf" target="_blank" rel="noopener">[1]</a> with <a href="https://github.com/xuqiantong/GAN-Metrics" target="_blank" rel="noopener">code</a>.</p>
<ol>
<li><p>Inception Score (IS): classification score using the InceptionNet pretrained on ImageNet</p>
<script type="math/tex; mode=display">IS=\exp\{E_x[KL(p_M(y|x)||p_M(y))]\}</script><p> in which $p_M(y)$ is the marginal distribution of  $p_M(y|x)$. Expect $p_M(y)$ to be of low entropy while $p_M(y|x)$ to be of high entropy. The higher, the better.</p>
</li>
<li><p>Mode score: extension of Inception score</p>
</li>
<li><p>Kernel MMD: MMD distance between two data distributions</p>
</li>
<li><p>Wasserstein distance: Wasserstein distance (Earth mover’s distance) between two data distributions.</p>
</li>
<li><p>Fréchet Inception Distance (FID): extract InceptionNet features and measure the data distribution distance. The lower, the better.</p>
<script type="math/tex; mode=display">FID=\|\mu_r-\mu_g\|+trace(\Sigma_r+\Sigma_g-2(\Sigma_r\Sigma_g)^{\frac{1}{2}})</script></li>
<li><p>KNN score: treat true data as positive and generated data as negative. Calculate the leave-one-out (LOO) accuracy based on 1-NN classifier.</p>
</li>
<li><p>Learned Perceptual Image Patch Similarity (LPIPS): <script type="math/tex">d(x,x_0)=\sum_l \frac{1}{H_l W_l}\sum_{h,w}\|w_l\circ (\hat{y}_{hw}^l-\hat{y}^l_{0hw})\|^2</script> <a href="https://arxiv.org/pdf/1801.03924.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://github.com/richzhang/PerceptualSimilarity#a-basic-usage" target="_blank" rel="noopener">[code]</a> </p>
</li>
</ol>
<h2 id="Subjective-Evaluation"><a href="#Subjective-Evaluation" class="headerlink" title="Subjective Evaluation:"></a>Subjective Evaluation:</h2><ol>
<li>Each user sees two randomly selected results at a time and is asked to choose the one that looks more realistic.  After obtaining all the pairwise results, Bradley-Terry model (B-T model) is used to calculate the global ranking score for each method. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Tsai_Deep_Image_Harmonization_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[2]</a></li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>Xu, Qiantong, et al. “An empirical study on evaluation metrics of generative adversarial networks.” arXiv preprint arXiv:1806.07755 (2018).</li>
<li>Tsai, Yi-Hsuan, et al. “Deep image harmonization.” CVPR, 2017.</li>
<li>Zhang, Richard, et al. “The unreasonable effectiveness of deep features as a perceptual metric.” CVPR, 2018.</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>Finding Datasets</title>
    <url>/2022/06/16/machine_learning/Finding%20Datasets/</url>
    <content><![CDATA[<ol>
<li><p><a href="https://www.kaggle.com/datasets" target="_blank" rel="noopener">Kaggle datasets</a></p>
</li>
<li><p><a href="https://registry.opendata.aws/" target="_blank" rel="noopener">Amazon datasets</a></p>
</li>
<li><p><a href="https://archive.ics.uci.edu/ml/datasets.html" target="_blank" rel="noopener">UCI datasets</a></p>
</li>
<li><p><a href="https://toolbox.google.com/datasetsearch/" target="_blank" rel="noopener">Google dataset search</a></p>
</li>
<li><p><a href="https://msropendata.com/" target="_blank" rel="noopener">Microsoft datasets</a></p>
</li>
<li><p><a href="https://github.com/awesomedata/awesome-public-datasets" target="_blank" rel="noopener">Awesome public datasets</a></p>
</li>
<li><p><a href="https://www.visualdata.io/" target="_blank" rel="noopener">Computer vision datasets</a></p>
</li>
<li><p><a href="https://www.graviti.com/open-datasets/" target="_blank" rel="noopener">Graviti</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Euler Angle (yaw,pitch,roll)</title>
    <url>/2022/06/16/machine_learning/Euler%20Angle%20(yaw,pitch,roll)/</url>
    <content><![CDATA[<p>Borrowing aviation terminology, these rotations will be referred to as yaw, pitch, and roll:</p>
<p>A yaw is a counterclockwise rotation of \( \alpha\) about the \(z\)-axis. The rotation matrix is given by</p>
<script type="math/tex; mode=display">\displaystyle R_z(\alpha) = \begin{pmatrix}\cos\alpha & -\sin\alpha & 0 \\\\ \sin\alpha & \cos\alpha & 0 \\\\  0 & 0 & 1  \end{pmatrix} .</script><p>A pitch is a counterclockwise rotation of \( \beta\) about the \( y\)-axis. The rotation matrix is given by</p>
<script type="math/tex; mode=display">\displaystyle R_y(\beta) = \begin{pmatrix}\cos\beta & 0 & \sin\beta \\\\ 0 & 1 & 0 \\\\ -\sin\beta & 0 & \cos\beta  \end{pmatrix} .</script><p>A roll is a counterclockwise rotation of \( \gamma\) about the \( x\)-axis. The rotation matrix is given by</p>
<script type="math/tex; mode=display">\displaystyle R_x(\gamma) = \begin{pmatrix}1 & 0 & 0 \\\\ 0 & \cos\gamma & -\sin\gamma \\\\ 0 & \sin\gamma & \cos\gamma  \end{pmatrix} .</script><p>Note that \( R(\alpha,\beta,\gamma)\) performs the roll first, then the pitch, and finally the yaw. If the order of these operations is changed, a different rotation matrix would result.</p>
<p>For gaze direction, roll does not change gaze direction, so only yaw and pitch affect gaze direction. Given a normalized 3D vector (x,y,z), how to determine the yaw and pitch angles?<br>The problem should be discussed based on the order of doing yaw/pitch.</p>
<p>Consider an eye rigid model (bound with a head rigid model), aligned with original coordinate system, is facing x positive direction. Since roll has no effect on eye direction, we only perform yaw and pitch. For coordinate transformation, we consider the reverse process.</p>
<p>The eye direction in new coordinate system is \(c_1 = (1,0,0)\) but \(c_2 = (x_0,y_0,z_0)\) in the original coordinate system. </p>
<ul>
<li><p>If true rotation order is yaw-&gt;pitch, then <script type="math/tex">c\_2=R\_z(-\alpha)R\_y (-\beta)c\_1.</script>. Then, \(\beta=arsin(z_0),\alpha=-artan(y_0/x_0)\). </p>
</li>
<li><p>If true rotation order is pitch-&gt;yaw, then <script type="math/tex">c\_2=R\_y(-\beta)R\_z (-\alpha)c\_1.</script>. Then, \(\alpha=-arsin(y_0),\beta=artan(z_0/x_0)\). </p>
</li>
</ul>
<p>If we insert \(R_x(\gamma)\) before \(c_1\), the results won’t change, which demonstrates that roll will not influence eye direction. In other words, if the true rotation order is yaw-&gt;pitch-&gt;roll or pitch-&gt;yaw-&gt;roll, the above analysis still holds. </p>
<p>Notice:<br>-</p>
<ul>
<li>we used right-hand coordinate system, that is, thumb along the z-axis and fingers from x-axis to y-axis.</li>
<li>rotation \(\theta\) around some axis means rotating counter clockwise \(\theta\) when looking along the positive direction of that axis</li>
<li>when doing rotations in sequence, each rotation is based on the up-to-date coordinate system (x-axis, y-axis, z-axis).</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Endovascular Surgery</title>
    <url>/2022/06/16/machine_learning/Endovascular%20Surgery/</url>
    <content><![CDATA[<h4 id="Key-words"><a href="#Key-words" class="headerlink" title="Key words:"></a>Key words:</h4><p>catheter, cannulation, EM tracking (enhance visualization and provide objective metric)</p>
<h4 id="Endovascular-Technique"><a href="#Endovascular-Technique" class="headerlink" title="Endovascular Technique:"></a>Endovascular Technique:</h4><p>Early endovascular technique: real-time fluoroscopy and 2D angiography: ionizing radiation and repeated injection of a nephrotoxic contrast agent.</p>
<p>Image fusion techniques: project 3D CT and magnetic resonance imaging to real-time 2D fluoroscopic images, still require real-time fluroscopy.</p>
<p>Electromagnetic (EM) tracking: an EM field is generated by the Aurora Window Field Generator, and sensors on the tips measure and transmit the roll orientation and forward motion. The veracity of EM tracking is evaluated on the basis of target registration error (TRE).</p>
<h4 id="System"><a href="#System" class="headerlink" title="System:"></a>System:</h4><ol>
<li>manual simulator 2. virtual simulator (virtual reality)</li>
</ol>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/yQSOKk9.jpg" width="50%"></p>
<h3 id="Task"><a href="#Task" class="headerlink" title="Task:"></a>Task:</h3><p>Different locations of vessels correspond to the tasks with different difficult levels. </p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/iGbNTt1.jpg" width="50%"></p>
<h4 id="Metric"><a href="#Metric" class="headerlink" title="Metric:"></a>Metric:</h4><p>Certain metric is required to evaluate or augment the skills of surgeon (novice, intermediate, expert).</p>
<p>1.expert observation and subjective score</p>
<p>2.number of cases</p>
<p>3.kinematic metrics: mapping from kinematic data to skill (classification)</p>
<ul>
<li>3D path length</li>
<li>spectral arc length: change of acceleration in frequency domain</li>
<li>root mean dimensionless jerk: movement smoothness</li>
<li>submovement number and duration</li>
<li>catheter turn: measure the task difficulty</li>
</ul>
<h4 id="View"><a href="#View" class="headerlink" title="View:"></a>View:</h4><ul>
<li>2D or 3D</li>
<li>real-time or stored (using stored image can reduce fluoroscopy time and radiation exposure)</li>
<li>anteroposterior/lateral/endoluminal view</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>surgery</tag>
      </tags>
  </entry>
  <entry>
    <title>Distillation</title>
    <url>/2022/06/16/machine_learning/Distillation/</url>
    <content><![CDATA[<ol>
<li><p>knowledge/model distillation <a href="https://arxiv.org/pdf/1503.02531" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>data distillation <a href="https://arxiv.org/pdf/1712.04440.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://arxiv.org/pdf/2011.00050.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://proceedings.neurips.cc/paper/2021/file/299a23a2291e2126b91d54f3601ec162-Paper.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
</ol>
<p>A survey of knowledge distillation <a href="https://arxiv.org/abs/2004.05937" target="_blank" rel="noopener">[3]</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 (2015).</p>
<p>[2] Radosavovic, Ilija, et al. “Data distillation: Towards omni-supervised learning.” CVPR, 2018.</p>
<p>[3] Wang, Lin, and Kuk-Jin Yoon. “Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks.” arXiv preprint arXiv:2004.05937 (2020).</p>
<p>[4] Nguyen, Timothy, Zhourong Chen, and Jaehoon Lee. “Dataset Meta-Learning from Kernel Ridge-Regression.” arXiv preprint arXiv:2011.00050 (2020).</p>
<p>[5] Nguyen, Timothy, et al. “Dataset distillation with infinitely wide convolutional networks.” Advances in Neural Information Processing Systems 34 (2021).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>distillation</tag>
      </tags>
  </entry>
  <entry>
    <title>Data Augmentation</title>
    <url>/2022/06/16/machine_learning/Data%20Augmentation/</url>
    <content><![CDATA[<ol>
<li><p>Traditional data augmentation</p>
<ul>
<li><p>color, hue, illumination</p>
</li>
<li><p>flip, crop, shear, rotation, (piecewise) affine transformation, Cutout, RandErasing, HideAndSeek, GridMask</p>
</li>
</ul>
</li>
<li><p>Mixtures:  Mixup <a href="https://openreview.net/pdf?id=r1Ddp1-Rb" target="_blank" rel="noopener">[1]</a>, CutMix <a href="https://arxiv.org/pdf/1905.04899.pdf" target="_blank" rel="noopener">[2]</a> (Mixture in spatial domain), GridMask <a href="https://arxiv.org/pdf/2001.04086.pdf" target="_blank" rel="noopener">[6]</a>, FMix <a href="https://arxiv.org/pdf/2002.12047.pdf" target="_blank" rel="noopener">[3]</a> (Mixture in frequency)</p>
</li>
<li><p>Learn optimal data augmentation strategy: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Cubuk_AutoAugment_Learning_Augmentation_Strategies_From_Data_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://arxiv.org/pdf/1712.04621.pdf" target="_blank" rel="noopener">[5]</a>, AutoAugment, RandAugment, Fast AutoAugment, Faster AutoAugment, Greedy Augment.</p>
</li>
<li><p>Semantic augmentation: <a href="https://arxiv.org/abs/2007.10538" target="_blank" rel="noopener">[7]</a></p>
</li>
</ol>
<p>A summary of existing data augmentation methods <a href="https://github.com/AgaMiko/data-augmentation-review" target="_blank" rel="noopener">[link]</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] mixup: Beyond empirical risk minimization</p>
<p>[2] Cutmix: Regularization strategy to train strong classifiers with localizable features</p>
<p>[3] Understanding and Enhancing Mixed Sample Data Augmentation</p>
<p>[4] AutoAugment: Learning Augmentation Strategies from Data</p>
<p>[5] The Effectiveness of Data Augmentation in Image Classification using Deep Learning</p>
<p>[6] GridMask Data Augmentation</p>
<p>[7] Regularizing Deep Networks with Semantic Data Augmentation</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Clothes Dataset</title>
    <url>/2022/06/16/machine_learning/Clothes%20Dataset/</url>
    <content><![CDATA[<ol>
<li><p>deepfashion: <a href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html</a> (attribute, bounding box, landmark)</p>
</li>
<li><p>Colorful-Fashion: <a href="https://sites.google.com/site/fashionparsing/home" target="_blank" rel="noopener">https://sites.google.com/site/fashionparsing/home</a> (pixel-level color-category label)</p>
</li>
<li><p>CCP (Clothing Co-Parsing): <a href="https://github.com/bearpaw/clothing-co-parsing" target="_blank" rel="noopener">https://github.com/bearpaw/clothing-co-parsing</a> (parsing label)</p>
</li>
<li><p>fashionistas: <a href="http://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/(parsing" target="_blank" rel="noopener">http://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/(parsing</a> label)</p>
</li>
<li><p>HPW (Human Parsing in the Wild): <a href="https://github.com/lemondan/HumanParsing-Dataset" target="_blank" rel="noopener">https://github.com/lemondan/HumanParsing-Dataset</a> (parsing label)</p>
</li>
<li><p>modaNet: <a href="https://github.com/eBay/modanet" target="_blank" rel="noopener">https://github.com/eBay/modanet</a> (polygon annotations)</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Class Imbalance</title>
    <url>/2022/06/16/machine_learning/Class%20Imbalance/</url>
    <content><![CDATA[<ol>
<li>re-sampling</li>
<li>synthetic samples: generate more samples for minor classes</li>
<li>re-weighting</li>
<li>few-shot learning</li>
<li>decoupling representation and classifier learning: use normal sampling in the feature learning stage and use re-sampling in the classifier learning stage. </li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Causal Inference</title>
    <url>/2022/06/16/machine_learning/Causality/</url>
    <content><![CDATA[<p><strong>Big Names:</strong> Judy Pearl <a href="http://ftp.cs.ucla.edu/pub/stat_ser/r354-corrected-reprint.pdf" target="_blank" rel="noopener">[Tutorial]</a> <a href="http://bcmi.sjtu.edu.cn/home/niuli/download/Judy_Pearl_CausalityIntro.pdf" target="_blank" rel="noopener">[slides]</a> <a href="http://bcmi.sjtu.edu.cn/home/niuli/download/Causality_2nd_Edition.pdf" target="_blank" rel="noopener">[textbook]</a>, James Robin <a href="https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2018/08/hernanrobins_v1.10.36.pdf" target="_blank" rel="noopener">[Textbook]</a> <a href="https://grass.upc.edu/en/seminar/presentation-files/causal-inference/chapters-1-i-2" target="_blank" rel="noopener">[slides]</a></p>
<p><strong>Tutorial:</strong> </p>
<ul>
<li>Causality for machine learning <a href="https://arxiv.org/pdf/1911.10500.pdf" target="_blank" rel="noopener">[4]</a></li>
<li>Towards Causal Representation Learning <a href="https://arxiv.org/pdf/2102.11107.pdf" target="_blank" rel="noopener">[8]</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//bcmi.sjtu.edu.cn/home/niuli/download/A_briefing_on_causal_inference.pdf" target="_blank" rel="noopener">A briefing on causal inference</a> written by myself</li>
</ul>
<p><strong>Workshop:</strong> <a href="https://sites.google.com/view/nips2018causallearning/home" target="_blank" rel="noopener">NIPS2018 workshop on causal learning</a>, <a href="http://kdd2020tutorial.thumedialab.com/" target="_blank" rel="noopener">KDD2020 Tutorial on Causal Inference Meets Machine Learning</a></p>
<p><strong>Material:</strong> <a href="https://www.bradyneal.com/causal-inference-course" target="_blank" rel="noopener">MILA Course</a></p>
<p>Causality and disentanglement: <a href="https://arxiv.org/pdf/2011.11878" target="_blank" rel="noopener">[5]</a> <a href="https://arxiv.org/pdf/2010.02637.pdf" target="_blank" rel="noopener">[6]</a></p>
<p>Counterfactual and disentanglement: <a href="https://arxiv.org/pdf/2103.00887.pdf" target="_blank" rel="noopener">[7]</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Chalupka K, Perona P, Eberhardt F. Visual causal feature learning. arXiv preprint arXiv:1412.2309, 2014.</p>
<p>[2] Lopez-Paz D, Nishihara R, Chintala S, et al. Discovering causal signals in images. CVPR, 2017.</p>
<p>[3] Bau D, Zhu J Y, Strobelt H, et al. GAN Dissection: Visualizing and Understanding Generative Adversarial Networks. arXiv preprint arXiv:1811.10597, 2018.</p>
<p>[4] Bernhard Schölkopf: CAUSALITY FOR MACHINE LEARNING. arXiv preprint arXiv:1911.10500, 2019.</p>
<p>[5] Kim, Hyemi, et al. “Counterfactual Fairness with Disentangled Causal Effect Variational Autoencoder.” arXiv preprint arXiv:2011.11878 (2020).</p>
<p>[6] Shen, Xinwei, et al. “Disentangled Generative Causal Representation Learning.” arXiv preprint arXiv:2010.02637 (2020).</p>
<p>[7] Yue, Zhongqi, et al. “Counterfactual Zero-Shot and Open-Set Visual Recognition.” arXiv preprint arXiv:2103.00887 (2021).</p>
<p>[8] Schölkopf, Bernhard, et al. “Towards causal representation learning.” arXiv preprint arXiv:2102.11107 (2021).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>causality</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU Cuda and CuDNN</title>
    <url>/2022/06/16/hardware/GPU/GPU%20Cuda%20and%20CuDNN/</url>
    <content><![CDATA[<p><img src="https://www.ustcnewly.com/github_images/GPU_CUDA.png" style="width:600px;"></p>
<h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><ul>
<li><p>look up GPU information: <code>lspci</code> or <code>lshw -C display</code> </p>
</li>
<li><p>NVIDIA system management interface, monitor GPU usage: <code>nvidia-smi</code>  (GPU driver version and CUDA user-mode version)</p>
</li>
</ul>
<h3 id="GPU-Driver"><a href="#GPU-Driver" class="headerlink" title="GPU Driver"></a>GPU Driver</h3><ul>
<li><p>check the latest driver information on <a href="http://www.nvidia.com/Download/index.aspx" target="_blank" rel="noopener">http://www.nvidia.com/Download/index.aspx</a>. Then, look up driver information on local machine: <code>cat /proc/driver/nvidia/version</code></p>
</li>
<li><p>check the compatibility between CUDA runtime version and driver version: <a href="https://docs.nvidia.com/deploy/cuda-compatibility/" target="_blank" rel="noopener">https://docs.nvidia.com/deploy/cuda-compatibility/</a></p>
</li>
<li><p>Install NVIDIA GPU driver using GUI: Software &amp; Updates -&gt; Additional Drivers</p>
</li>
<li><p>Install NVIDIA GPU driver using apt-get</p>
  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:Ubuntu-x-swat/x-updates</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install nvidia-current nvidia-current-modaliases nvidia-settings</span><br></pre></td></tr></table></figure>
</li>
<li><p>Install NVIDIA GPU driver using *.run file downloaded from <a href="http://www.nvidia.com/Download/index.aspx" target="_blank" rel="noopener">http://www.nvidia.com/Download/index.aspx</a></p>
<ol>
<li>Hit CTRL+ALT+F1 and login using your credentials.</li>
<li>Stop your current X server session by typing <code>sudo service lightdm stop</code></li>
<li>Enter runlevel 3 by typing <code>sudo init 3</code> and install your *.run file.</li>
<li>You might be required to reboot when the installation finishes. If not, run <code>sudo service lightdm start</code> or <code>sudo start lightdm</code> to start your X server again.</li>
</ol>
</li>
</ul>
<h3 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h3><p>When using anaconda to install deep learning platform, sometimes it is unnecessary to install CUDA by yourself. </p>
<ol>
<li><p>Preprocessing</p>
<ul>
<li>uninstall the GPU driver first: <code>sudo /usr/bin/nvidia-uninstall</code> or <code>sudo apt-get remove --purge nvidia*</code> and <code>sudo apt-get autoremove</code>; <code>sudo reboot</code></li>
<li>blacklist nouveau: add “blacklist nouveau” and “options nouveau modeset=0” at the end of /etc/modprobe.d/blacklist.conf; <code>sudo update-initramfs -u</code>; <code>sudo reboot</code></li>
<li>Stop your current X server session: <code>sudo service lightdm stop</code></li>
</ul>
</li>
<li><p>Install Cuda</p>
<p> Download the *.run file from NVIDIA website</p>
<ul>
<li>The latest version: <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-downloads</a> </li>
<li><p>All versions: <a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a> </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo sh cuda_10.0.130_410.48_linux.run</span><br></pre></td></tr></table></figure>
<p>and then add into PATH and LD_LIBRARY_PATH</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo 'export PATH=/usr/local/cuda/bin:$PATH' &gt;&gt; ~/.bashrc</span><br><span class="line">echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>check Cuda version after installation: <code>nvcc -V</code>. Compile and run the cuda samples.</p>
</li>
</ol>
<h3 id="CuDNN"><a href="#CuDNN" class="headerlink" title="CuDNN"></a>CuDNN</h3><p>CuDNN is to accelerate Cuda, from <a href="https://developer.nvidia.com/rdp/form/cudnn-download-survey" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/form/cudnn-download-survey</a>, just download compressed package.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd $CUDNN_PATH	</span><br><span class="line">sudo cp include/* /usr/local/cuda/include/</span><br><span class="line">sudo cp -P lib64/* /usr/local/cuda/lib64/ #use -P to retain symbolic links</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>hardware</category>
        <category>GPU</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>CUDA</tag>
        <tag>CuDNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Illumination Model</title>
    <url>/2022/06/16/hardware/camera/Illumination%20Model/</url>
    <content><![CDATA[<ol>
<li><p>Dichromatic Reflection Model <a href="https://apps.dtic.mil/sti/pdfs/ADA150999.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-61864-3_17#enumeration" target="_blank" rel="noopener">[2]</a> <script type="math/tex">I_i=\gamma_b C_i L_i R_{b,i}+\gamma_s C_i L_i R_{s,i}</script>, in which <script type="math/tex">i</script> is the pixel index, <script type="math/tex">L</script> is the global illumination, <script type="math/tex">C_i</script> is the sensor sensitivity. The chromatic terms <script type="math/tex">R_b</script> and <script type="math/tex">R_s</script> account for body and surface reflection, which are only related to object material.  </p>
</li>
<li><p>gray pixels:  pixels with equal RGB values. detecting gray pixels in a color-biased image is not easy. <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Qian_On_Finding_Gray_Pixels_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[3]</a> </p>
</li>
<li><p>albedo, shading, gloss <a href="https://arxiv.org/pdf/2010.05907.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_InverseRenderNet_Learning_Single_Image_Inverse_Rendering_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Shafer, Steven A. “Using color to separate reflection components.” Color Research &amp; Application 10.4 (1985): 210-218.<br>[2] Song, Shuangbing, et al. “Illumination Harmonization with Gray Mean Scale.” Computer Graphics International Conference. Springer, Cham, 2020.<br>[3] Qian, Yanlin, et al. “On finding gray pixels.” CVPR, 2019.<br>[4] Bhattad, Anand, and David A. Forsyth. “Cut-and-Paste Neural Rendering.” arXiv preprint arXiv:2010.05907 (2020).<br>[5] Yu, Ye, and William AP Smith. “InverseRenderNet: Learning single image inverse rendering.” CVPR, 2019.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Camera Survey</title>
    <url>/2022/06/16/hardware/camera/Camera%20Survey/</url>
    <content><![CDATA[<h3 id="Interface-Type"><a href="#Interface-Type" class="headerlink" title="Interface Type:"></a>Interface Type:</h3><p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/L2j9XQP.jpg" width="900"></p>
<p>GigE and USB interfaces are commonly used. The advantage of GigE is long-distance transmission.</p>
<h3 id="Color-v-s-Monochrome"><a href="#Color-v-s-Monochrome" class="headerlink" title="Color v.s. Monochrome"></a>Color v.s. Monochrome</h3><p>When the exposure begins, each photosite is uncovered to collect incoming light. When the exposure ends, the occupancy of each photosite is read as an electrical signal, which is then quantified and stored as a numerical value in an image file.</p>
<p>Unlike color sensors, monochrome sensors capture all incoming light at each pixel regardless of color.<br>Unlike with color, monochrome sensors also do not require demosaicing to create the final image because the values recorded at each photosite effectively just become the values at each pixel. As a result, monochrome sensors are able to achieve a slightly higher resolution.</p>
<h3 id="Sensor-Type"><a href="#Sensor-Type" class="headerlink" title="Sensor Type:"></a>Sensor Type:</h3><ul>
<li><strong>CCD (Charged Coupling Devices)</strong>: special manufacturing process that allows the conversion to take place in the chip without distortion, which makes them more expensive. CCD can capture high-quality image with low noise and is sensitive to light.</li>
</ul>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/9NvqKXW.jpg" width="900"></p>
<ul>
<li><strong>CMOS (Complimentary Metal Oxide Semiconductor)</strong>: use transistors at each pixel to move the charge through traditional wires. Traditional manufacturing processes are used to make CMOS, which is the same as creating microchips. CMOS is cheaper and has low power consumption</li>
</ul>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/bozmHu7.jpg" width="900"></p>
<h3 id="Readout-Method"><a href="#Readout-Method" class="headerlink" title="Readout Method:"></a>Readout Method:</h3><p><strong>Global v.s. rolling shutter:</strong> originally, CCD uses global shutter while CMOS uses rolling shutter. Rolling shutter is always active and rolling through the pixels line by line from top to bottom. In contrast, global shutter stores their electrical charges and reads out when the shutter is closed and the pixel is reset for the next exposure, allowing the entire sensor area to be output simultaneously. Nowadays, CMOS can also have global shutter capabilities.</p>
<p><strong>Advantage of global shutter:</strong> global shutter can manage motions and pulsed light conditions rather well as the scene is viewed or exposed at one moment in time by enabling synchronous timing of the light or motion to the open shutter phase. However, rolling shutter can also manage motions and pulsed light conditions to an extent through a combination of fast shutter speeds and timing of the light source. </p>
<h3 id="Quantum-Efficiency"><a href="#Quantum-Efficiency" class="headerlink" title="Quantum Efficiency"></a>Quantum Efficiency</h3><p>The ability of a pixel to convert an incident photon to charge is specified by its quantum efficiency. For example, if for ten incident photons, four photo-electrons are produced, then the quantum efficiency is 40%. Typical values of quantum efficiency are in the range of 30 - 60%. The quantum efficiency depends on wavelength and is not necessarily uniform over the response to light intensity.</p>
<h3 id="Field-of-View"><a href="#Field-of-View" class="headerlink" title="Field of View"></a>Field of View</h3><p>FOV (Field of View) depends on the lens size. Generally, larger sensors yield greater FOV.</p>
<h3 id="Pixel-Size"><a href="#Pixel-Size" class="headerlink" title="Pixel Size"></a>Pixel Size</h3><p>A small pixel size is desirable because it results in a smaller die size and/or higher spatial resolution; a large pixel size is desirable because it results in higher dynamic range and signal-to-noise ratio. </p>
]]></content>
      <categories>
        <category>hardware</category>
        <category>camera</category>
      </categories>
      <tags>
        <tag>camera</tag>
      </tags>
  </entry>
  <entry>
    <title>Zoom in</title>
    <url>/2022/06/16/deep_learning/Zoom%20in/</url>
    <content><![CDATA[<p>(1) Zoom in a bounding box <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zheng_Learning_Multi-Attention_Convolutional_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
<p>(2) Zoom in salient region <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Adria_Recasens_Learning_to_Zoom_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://arxiv.org/pdf/1903.06150.pdf" target="_blank" rel="noopener">[4]</a></p>
<ul>
<li>relation to (1):  if the salience region is rectangle and salience value is infinity, this should be equivalent to zooming in a bounding box. </li>
<li>relation to pooling: weighted pooling with salience map as weight map</li>
<li>relation to deformable CNN: use salience map to calculate offset for each position</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Fu, Jianlong, Heliang Zheng, and Tao Mei. “Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition.” CVPR, 2017.</p>
<p>[2] Zheng, Heliang, et al. “Learning multi-attention convolutional neural network for fine-grained image recognition.” ICCV, 2017.</p>
<p>[3] Recasens, Adria, et al. “Learning to zoom: a saliency-based sampling layer for neural networks.” ECCV, 2018.</p>
<p>[4] Zheng, Heliang, et al. “Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-grained Image Recognition.” arXiv preprint arXiv:1903.06150 (2019).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Zero-Shot Semantic Segmentation</title>
    <url>/2022/06/16/classification_detection_segmentation/Zero-Shot%20Semantic%20Segmentation/</url>
    <content><![CDATA[<ul>
<li><p>standard semantic segmentation: <a href="https://github.com/RohanDoshi2018/ZeroshotSemanticSegmentation/blob/master/rohan_doshi_senior_thesis.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Xian_Semantic_Projection_Network_for_Zero-_and_Few-Label_Semantic_Segmentation_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://arxiv.org/pdf/1906.00817.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
<li><p>binary segmentation: <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/MDALC/Kato_Zero-Shot_Semantic_Segmentation_via_Variational_Mapping_ICCVW_2019_paper.pdf" target="_blank" rel="noopener">[4]</a></p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Rohan Doshi, Olga Russakovsky, “zero-shot semantic segmentation”, bachelor thesis.</p>
<p>[2] Y. Xian, S. Choudhury, Y. He, B. Schiele and Z. Akata , “SPNet: Semantic Projection Network for Zero- and Few-Label Semantic Segmentation”, CVPR, 2019.</p>
<p>[3] Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, Sorbonne, Patrick Pérez, “Zero-Shot Semantic Segmentation”, 2019</p>
<p>[4] Kato, Naoki, Toshihiko Yamasaki, and Kiyoharu Aizawa. “Zero-Shot Semantic Segmentation via Variational Mapping.” ICCV Workshops. 2019.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>semantic segmentation</tag>
        <tag>zero-shot</tag>
      </tags>
  </entry>
  <entry>
    <title>Zero-Shot Object Detection</title>
    <url>/2022/06/16/classification_detection_segmentation/Zero-Shot%20Object%20Detection/</url>
    <content><![CDATA[<ul>
<li><p>ZSL based on bounding box features</p>
<ul>
<li><a href="https://arxiv.org/pdf/1804.04340.pdf" target="_blank" rel="noopener">[1]</a>: use background bounding boxes from background classes</li>
<li><a href="https://arxiv.org/pdf/1803.06049.pdf" target="_blank" rel="noopener">[3]</a>: classification loss with semantic clustering</li>
</ul>
</li>
<li><p>End-to-end zero-shot object detection</p>
<ul>
<li><a href="https://arxiv.org/pdf/1803.07113.pdf" target="_blank" rel="noopener">[2]</a>: extend YOLO, concatenate three feature maps to predict confidence score.</li>
<li><a href="https://arxiv.org/pdf/1811.08982.pdf" target="_blank" rel="noopener">[4]</a>: use polarity loss similar to focal loss and vocabulary to enhance word vector</li>
<li><a href="https://arxiv.org/pdf/1805.06157.pdf" target="_blank" rel="noopener">[5]</a>: output both classification scores and semantic embeddings</li>
</ul>
</li>
<li><p>Feature generation</p>
</li>
<li><ul>
<li><p><a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Hayat_Synthesizing_the_Unseen_for_Zero-shot_Object_Detection_ACCV_2020_paper.pdf" target="_blank" rel="noopener">[6]</a>: synthesize<br>visual features for unseen classes</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/9153181" target="_blank" rel="noopener">[7]</a>: semantics-preserving graph propagation modules that enhance both category and region representations</p>
</li>
</ul>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, Ajay Divakaran, “Zero-Shot Object Detection”, ECCV, 2018.</p>
<p>[2] Pengkai Zhu, Hanxiao Wang, and Venkatesh Saligrama, “Zero Shot Detection”, T-CSVT, 2019.</p>
<p>[3] Rahman, Shafin, Salman Khan, and Fatih Porikli. “Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts.” arXiv preprint arXiv:1803.06049 (2018).</p>
<p>[4] Rahman, Shafin, Salman Khan, and Nick Barnes. “Polarity Loss for Zero-shot Object Detection.” arXiv preprint arXiv:1811.08982 (2018).</p>
<p>[5] Demirel, Berkan, Ramazan Gokberk Cinbis, and Nazli Ikizler-Cinbis. “Zero-Shot Object Detection by Hybrid Region Embedding.” arXiv preprint arXiv:1805.06157 (2018).</p>
<p>[6] Hayat, Nasir, et al. “Synthesizing the unseen for zero-shot object detection.” Proceedings of the Asian Conference on Computer Vision. 2020.</p>
<p>[7] Yan, Caixia, et al. “Semantics-preserving graph propagation for zero-shot object detection.” IEEE Transactions on Image Processing 29 (2020): 8163-8176.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>zero-shot</tag>
      </tags>
  </entry>
  <entry>
    <title>Zero-Shot Learning</title>
    <url>/2022/06/16/classification_detection_segmentation/Zero-Shot%20Learning/</url>
    <content><![CDATA[<p>Zero-shot learning focuses on the relation between visual features X, semantic embeddings A, and category labels Y. Based on the approach, existing zero-shot learning works can be roughly categorized into the following groups:</p>
<p>1) semantic relatedness: X-&gt;Y (semantic similarity; write classifier)</p>
<p>2) semantic embedding: X-&gt;A-&gt;Y (map from X to A; map from A to X; map between A and X into common space)</p>
<p>Based on the setting, existing zero-shot learning works can be roughly categorized into the following groups:</p>
<p>1) inductive ZSL (do not use unlabeled test images in the training stage) v.s. semi-supervised/transductive ZSL (use unlabeled test images in the training stage)</p>
<p>2) standard ZSL (test images only from unseen categories) v.s. generalized ZSL (test images from both seen and unseen categories) (<a href="http://papers.nips.cc/paper/5027-zero-shot-learning-through-cross-modal-transfer.pdf" target="_blank" rel="noopener">novelty detection</a>, <a href="https://arxiv.org/pdf/1605.04253.pdf" target="_blank" rel="noopener">calibrated stacking</a>)</p>
<p><strong>Ideas:</strong></p>
<ol>
<li><p>Mapping: dictionary learning, metric learning, etc</p>
</li>
<li><p>Embedding: multiple embedding <a href="https://yanweifu.github.io/embedding/embedding_paper_eccv14.pdf" target="_blank" rel="noopener">[1]</a>, free embedding <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Akata_Evaluation_of_Output_2015_CVPR_paper.pdf" target="_blank" rel="noopener">[1]</a>, self-defined embedding <a href="https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/6368/FuEtAl_PAMI2014_GREEN.pdf?sequence=2" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>Application: video-&gt;object(attribute)-&gt;action <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Jain_Objects2action_Classifying_and_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[1]</a>, image-&gt;object(attribute)-&gt;scene</p>
</li>
<li><p>Combination: with active learning <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wah_Attribute-Based_Detection_of_2013_CVPR_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Gavves_Active_Transfer_Learning_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[2]</a>, online learning <a href="https://pdfs.semanticscholar.org/8b72/4754d7c7d7e5f98c1982b144fddb66add843.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>External knowledge graph: WordNet-based <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2486.pdf" target="_blank" rel="noopener">[1]</a>, NELL-based <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Zero-Shot_Recognition_via_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>Deep learning: graph neural network <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Zero-Shot_Recognition_via_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[1]</a>, RNN <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2486.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>Generate synthetic exemplars for unseen categories: synthetic images <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[SP-AEN]</a> or synthetic features <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Verma_Generalized_Zero-Shot_Learning_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[SE-ZSL]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_A_Generative_Adversarial_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[GAZSL]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2709.pdf" target="_blank" rel="noopener">[f-xGAN]</a></p>
</li>
</ol>
<p><strong>Critical Issues:</strong></p>
<ol>
<li><p>generalized ZSL, why first predict seen or unseen?: As claimed in <a href="https://arxiv.org/pdf/1605.04253.pdf" target="_blank" rel="noopener">[1]</a>, since we only see labeled data from seen classes, during training, the scoring functions of seen classes tend to dominate those of unseen classes, leading to biased predictions in GZSL and aggressively classifying a new data point into the label space of S because classifiers for the seen classes do not get trained on negative examples from the unseen classes.</p>
</li>
<li><p>hubness problem <a href="https://arxiv.org/pdf/1412.6568.pdf" target="_blank" rel="noopener">[1]</a><a href="https://arxiv.org/pdf/1511.04458.pdf" target="_blank" rel="noopener">[2]</a>: As claimed in <a href="https://arxiv.org/pdf/1511.04458.pdf" target="_blank" rel="noopener">[2]</a>, one practical effect of the ZSL domain shift is the Hubness problem. Specifically, after the domain shift, there are a small set of hub test-class prototypes that become nearest or K nearest neighbours to the majority of testing samples in the semantic space, while others are NNs of no testing instances. This results in poor accuracy and highly biased predictions with the majority of testing examples being assigned to a small minority of classes. </p>
</li>
<li><p>projection domain shift: what is the impact on the decision values?</p>
</li>
</ol>
<p><strong>Datasets:</strong></p>
<ol>
<li><p>small-scale datasets:  <a href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html" target="_blank" rel="noopener">CUB</a>, <a href="https://cvml.ist.ac.at/AwA2" target="_blank" rel="noopener">AwA</a>, <a href="http://cs.brown.edu/~gmpatter/sunattributes.html" target="_blank" rel="noopener">SUN</a>, <a href="http://vision.cs.uiuc.edu/attributes/" target="_blank" rel="noopener">aPY</a>, <a href="http://vision.stanford.edu/aditya86/ImageNetDogs/" target="_blank" rel="noopener">Dogs</a>, <a href="http://www.robots.ox.ac.uk/~vgg/data/flowers/102/" target="_blank" rel="noopener">FLO</a></p>
</li>
<li><p>large-scale dataset: ImageNet</p>
</li>
</ol>
<p><strong>Survey and Resource:</strong></p>
<ol>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8253589" target="_blank" rel="noopener">Recent Advances in Zero-Shot Recognition</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1707.00600.pdf" target="_blank" rel="noopener">Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly</a> <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/zero-shot-learning/zero-shot-learning-the-good-the-bad-and-the-ugly/" target="_blank" rel="noopener">[code]</a></p>
</li>
<li><p><a href="https://github.com/chichilicious/awesome-zero-shot-learning" target="_blank" rel="noopener">List of paper and datasets</a></p>
</li>
</ol>
<p><strong>Other applications:</strong></p>
<ol>
<li>zero-shot object detection</li>
<li>zero-shot figure-ground segmentation <a href="http://www.cs.umanitoba.ca/~ywang/papers/icpr16_segment.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>zero-shot semantic segmentation</li>
<li>zero-shot retrieval</li>
<li>zero-shot domain adaptation</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>zero-shot</tag>
      </tags>
  </entry>
  <entry>
    <title>Word Vector</title>
    <url>/2022/06/16/deep_learning/Word%20Vector/</url>
    <content><![CDATA[<h2 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h2><p>For a brief survey summarizing <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">skip-gram</a>, <a href="https://arxiv.org/pdf/1301.3781.pdf?" target="_blank" rel="noopener">CBOW</a>, <a href="http://www.aclweb.org/anthology/D14-1162" target="_blank" rel="noopener">GloVe</a>, etc, please refer to <a href="http://bcmi.sjtu.edu.cn/home/niuli/download/word_vector_survey.pdf" target="_blank" rel="noopener">this</a>.</p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>word2vec: <a href="https://github.com/tensorflow/models/tree/master/tutorials/embedding" target="_blank" rel="noopener">TensorFlow</a></p>
<p>GloVe: <a href="https://github.com/stanfordnlp/GloVe" target="_blank" rel="noopener">C</a>, <a href="https://github.com/shashankg7/glove-tensorflow" target="_blank" rel="noopener">TensorFlow</a></p>
<h2 id="WikiCorpus"><a href="#WikiCorpus" class="headerlink" title="WikiCorpus"></a>WikiCorpus</h2><p>Download the <a href="https://dumps.wikimedia.org/enwiki/" target="_blank" rel="noopener">WikiCorpus</a> and use the <a href="\code\shellscript\get-wikimedia.sh">shellscript</a> to process (e.g., remove numbers, invalide chars, urls), leading to sequence of pure words.</p>
<h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><ul>
<li>English word vectors: <a href="https://github.com/3Top/word2vec-api" target="_blank" rel="noopener">https://github.com/3Top/word2vec-api</a></li>
<li>Non-English word vectors: <a href="https://github.com/Kyubyong/wordvectors" target="_blank" rel="noopener">https://github.com/Kyubyong/wordvectors</a></li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>word vector</tag>
      </tags>
  </entry>
  <entry>
    <title>Weakly-supervised Segmentation</title>
    <url>/2022/06/16/classification_detection_segmentation/Weakly-supervised%20Segmentation/</url>
    <content><![CDATA[<ul>
<li><p>Using web data for semantic segmentation:</p>
<ul>
<li><p><a href="http://zpascal.net/cvpr2017/Jin_Webly_Supervised_Semantic_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[1]</a>: crawl web images with white background and generate composite images to initialize segmentation network</p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Bootstrapping_the_Performance_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[2]</a>: train a segmentation network using web data to obtain rough segmentation mask</p>
</li>
</ul>
</li>
<li><p>image-level semantic/instance segmentation: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Learning_Instance_Activation_Maps_for_Weakly_Supervised_Instance_Segmentation_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[9]</a> <a href="https://arxiv.org/pdf/2002.08098.pdf" target="_blank" rel="noopener">[10]</a> <a href="https://arxiv.org/pdf/2101.11253.pdf" target="_blank" rel="noopener">[11]</a></p>
</li>
<li><p>box-level semantic/instance segmentation: <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[3]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Khoreva_Simple_Does_It_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[4]</a> <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Ahn_Weakly_Supervised_Learning_of_Instance_Segmentation_With_Inter-Pixel_Relations_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[5]</a> <a href="https://papers.nips.cc/paper/8885-weakly-supervised-instance-segmentation-using-the-bounding-box-tightness-prior.pdf" target="_blank" rel="noopener">[6]</a></p>
</li>
<li><p>scribble/point-level semantic segmentaiton: <a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Lin_ScribbleSup_Scribble-Supervised_Convolutional_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[7]</a> <a href="https://arxiv.org/pdf/1506.02106.pdf" target="_blank" rel="noopener">[8]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Tang_Normalized_Cut_Loss_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[12]</a> <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Meng_Tang_On_Regularized_Losses_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[13]</a> <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Marin_Beyond_Gradient_Descent_for_Regularized_Segmentation_Losses_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[14]</a> <a href="https://opus.lib.uts.edu.au/bitstream/10453/141475/2/0508.pdf" target="_blank" rel="noopener">[15]</a> <a href="http://openaccess.thecvf.com/content/ICCV2021/papers/Pan_Scribble-Supervised_Semantic_Segmentation_by_Uncertainty_Reduction_on_Neural_Representation_and_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[16]</a> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Scribble-Supervised_Semantic_Segmentation_Inference_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[17]</a> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Seminar_Learning_for_Click-Level_Weakly_Supervised_Semantic_Segmentation_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[18]</a> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Tree_Energy_Loss_Towards_Sparsely_Annotated_Semantic_Segmentation_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[19]</a></p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Bin Jin, Maria V. Ortiz Segovia, Sabine Süsstrunk:<br>Webly Supervised Semantic Segmentation. CVPR 2017.</p>
<p>[2] Tong Shen, Guosheng Lin, Chunhua Shen, Ian D. Reid:<br>Bootstrapping the Performance of Webly Supervised Semantic Segmentation. CVPR 2018.</p>
<p>[3] Dai, Jifeng, Kaiming He, and Jian Sun. “Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation.” ICCV, 2015.</p>
<p>[4] Khoreva, Anna, et al. “Simple does it: Weakly supervised instance and semantic segmentation.” CVPR, 2017.</p>
<p>[5] Ahn, Jiwoon, Sunghyun Cho, and Suha Kwak. “Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations.” CVPR, 2019.</p>
<p>[6] Hsu, Cheng-Chun, et al. “Weakly Supervised Instance Segmentation using the Bounding Box Tightness Prior.” NeurIPS. 2019.</p>
<p>[7] Lin, Di, et al. “Scribblesup: Scribble-supervised convolutional networks for semantic segmentation.” CVPR, 2016.</p>
<p>[8] Bearman, Amy, et al. “What’s the point: Semantic segmentation with point supervision.” ECCV, 2016.</p>
<p>[9] Zhu, Yi, et al. “Learning instance activation maps for weakly supervised instance segmentation.” CVPR, 2019.</p>
<p>[10] Wang, Xiang, et al. “Weakly-Supervised Semantic Segmentation by Iterative Affinity Learning.” International Journal of Computer Vision (2020): 1-14.</p>
<p>[11] Jo, Sanhyun, and In-Jae Yu. “Puzzle-CAM: Improved localization via matching partial and full features.” arXiv preprint arXiv:2101.11253 (2021).</p>
<p>[12] Tang, Meng, et al. “Normalized cut loss for weakly-supervised cnn segmentation.” CVPR, 2018.</p>
<p>[13] Tang, Meng, et al. “On regularized losses for weakly-supervised cnn segmentation.” ECCV, 2018.</p>
<p>[14] Marin, Dmitrii, et al. “Beyond gradient descent for regularized segmentation losses.” CVPR, 2019.</p>
<p>[15] Wang, Bin, et al. “Boundary perception guidance: A scribble-supervised semantic segmentation approach.” IJCAI, 2019.</p>
<p>[16] Pan, Zhiyi, et al. “Scribble-supervised semantic segmentation by uncertainty reduction on neural representation and self-supervision on neural eigenspace.” ICCV, 2021.</p>
<p>[17] Xu, Jingshan, et al. “Scribble-supervised semantic segmentation inference.” ICCV, 2021.</p>
<p>[18] Chen, Hongjun, et al. “Seminar learning for click-level weakly supervised semantic segmentation.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.</p>
<p>[19] Liang, Zhiyuan, et al. “Tree energy loss: Towards sparsely annotated semantic segmentation.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>weakly-supervised</tag>
        <tag>webly-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title>Weakly-supervised Object Detection</title>
    <url>/2022/06/16/classification_detection_segmentation/Weakly-supervised%20Object%20Detection/</url>
    <content><![CDATA[<ol>
<li><p>webly supervised object detection <a href="https://arxiv.org/pdf/1707.08721.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>use a few bounding box annotations and a large number of image label annotations <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Ramanathan_DLWL_Improving_Detection_for_Lowshot_Classes_With_Weakly_Labelled_Data_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Exploiting Web Images for Weakly Supervised Object Detection. IEEE Trans. Multimedia 21(5): 1135-1146 (2019)</p>
<p>[2] DLWL: Improving Detection for Lowshot classes with Weakly Labelled data, Vignesh Ramanathan, Rui Wang, Dhruv Mahajan, CVPR2020</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>weakly-supervised</tag>
        <tag>webly-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title>Weakly-supervised Localization</title>
    <url>/2022/06/16/classification_detection_segmentation/Weakly-supervised%20Localization/</url>
    <content><![CDATA[<p>Closely related to <a href="https://ustcnewly.github.io/2019/09/26/deep_learning/Weakly-supervised%20Segmentation/">weakly-supervised segmentation</a>.</p>
<ul>
<li>attention based: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaolin_Zhang_Self-produced_Guidance_for_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[2]</a></li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Zhang, Xiaolin, et al. “Adversarial complementary learning for weakly supervised object localization.” CVPR, 2018.</p>
<p>[2] Zhang, Xiaolin, et al. “Self-produced guidance for weakly-supervised object localization.” ECCV, 2018.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>weakly-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title>Weakly-supervised Classification</title>
    <url>/2022/06/16/classification_detection_segmentation/Weakly-supervised%20Classification/</url>
    <content><![CDATA[<p><strong>Two problems:</strong></p>
<ul>
<li><u>Label noise</u>: label flip noise (belong to other training categories) and outlier noise (does not belong to any training category).</li>
<li><u>Domain shift</u>: domain distribution mismatch between web data and consumer data.</li>
</ul>
<p><strong>Solutions:</strong></p>
<ol>
<li><p>label flip layer: <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Chen_Webly_Supervised_Learning_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1406.2080.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Xiao_Learning_From_Massive_2015_CVPR_paper.pdf" target="_blank" rel="noopener">[3]</a> </p>
</li>
<li><p>multi-instance learning: <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhuang_Attend_in_Groups_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[4]</a> (pixel-level attention) <a href="https://jiajunwu.com/papers/dmil_cvpr.pdf" target="_blank" rel="noopener">[5]</a> <a href="https://arxiv.org/pdf/1802.04712.pdf" target="_blank" rel="noopener">[6]</a> <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_MetaCleaner_Learning_to_Hallucinate_Clean_Representations_for_Noisy-Labeled_Visual_Recognition_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[19]</a>(image-level attention)</p>
</li>
<li><p>reweight training samples: <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3435.pdf" target="_blank" rel="noopener">[7]</a> <a href="https://arxiv.org/pdf/1411.7718.pdf" target="_blank" rel="noopener">[8]</a> <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Misra_Seeing_Through_the_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[9]</a></p>
</li>
<li><p>curriculumn learning: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Sheng_Guo_CurriculumNet_Learning_from_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[10]</a> <a href="https://arxiv.org/pdf/1712.05055.pdf" target="_blank" rel="noopener">[11]</a></p>
</li>
<li><p>bootstrapping: <a href="https://arxiv.org/pdf/1412.6596.pdf" target="_blank" rel="noopener">[12]</a></p>
</li>
<li><p>negative learning: <a href="https://arxiv.org/pdf/1908.07387.pdf" target="_blank" rel="noopener">[18]</a></p>
</li>
<li><p>Cyclical Training: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_O2U-Net_A_Simple_Noisy_Label_Detection_Approach_for_Deep_Neural_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[20]</a></p>
</li>
</ol>
<p><strong>Use auxiliary clean data</strong>:</p>
<ol>
<li><p>active learning (select training samples to annotate): <a href="https://arxiv.org/pdf/1511.06789.pdf" target="_blank" rel="noopener">[13]</a> </p>
</li>
<li><p>reinforcement learning (learn labeling policies): <a href="https://arxiv.org/pdf/1706.02884.pdf" target="_blank" rel="noopener">[14]</a></p>
</li>
<li><p>analogous to semi-supervised learning </p>
<ul>
<li>partial data with both noisy labels and clean labels as well as partial data with only noisy labels <a href="https://vision.cornell.edu/se3/wp-content/uploads/2017/04/DeepLabelCleaning_CVPR.pdf" target="_blank" rel="noopener">[15]</a> <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Xiao_Learning_From_Massive_2015_CVPR_paper.pdf" target="_blank" rel="noopener">[3]</a>  <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3435.pdf" target="_blank" rel="noopener">[7]</a></li>
<li>partial data with noisy labels and partial data with clean labels <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7778168" target="_blank" rel="noopener">[16]</a> <a href="https://arxiv.org/pdf/1703.02391.pdf" target="_blank" rel="noopener">[17]</a></li>
</ul>
</li>
</ol>
<p><strong>Datasets:</strong></p>
<p>There are two types of label noise: synthetic label noise and web label noise.</p>
<ul>
<li><p>large-scale web datasets: <a href="https://www.vision.ee.ethz.ch/webvision/" target="_blank" rel="noopener">webvision v1</a>, <a href="https://data.vision.ee.ethz.ch/cvl/webvision/" target="_blank" rel="noopener">webvision v2</a></p>
</li>
<li><p>fine-grained web datasets: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Xiao_Learning_From_Massive_2015_CVPR_paper.pdf" target="_blank" rel="noopener">clothing</a>, <a href="https://bitbucket.org/jingruixiaozhuang/attend-in-groups-a-weakly-supervised-deep-learning-framework/src/master/" target="_blank" rel="noopener">car</a>, <a href="https://github.com/sxzrt/learning-from-web-data" target="_blank" rel="noopener">Stanford Dogs</a>, <a href="https://kuanghuei.github.io/Food-101N/" target="_blank" rel="noopener">Food101N</a>, <a href="https://github.com/sxzrt/learning-from-web-data" target="_blank" rel="noopener">MIT indoor67</a>, <a href="https://github.com/sxzrt/learning-from-web-data" target="_blank" rel="noopener">skin disease-198</a></p>
</li>
<li><p>synthetic noisy datasets via label flipping: <a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">CIFAR-10/100</a></p>
</li>
</ul>
<p><strong>Surveys:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2011.04406.pdf" target="_blank" rel="noopener">A Survey of Label-noise Representation Learning: Past, Present and Future</a></li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Chen, Xinlei, and Abhinav Gupta. “Webly supervised learning of convolutional networks.” ICCV, 2015.</p>
<p>[2] Sukhbaatar, Sainbayar, et al. “Training convolutional networks with noisy labels.” arXiv preprint arXiv:1406.2080 (2014).</p>
<p>[3] Xiao, Tong, et al. “Learning from massive noisy labeled data for image classification.” CVPR, 2015.</p>
<p>[4] Zhuang, Bohan, et al. “Attend in groups: a weakly-supervised deep learning framework for learning from web data.” CVPR, 2017.</p>
<p>[5] Wu, Jiajun, et al. “Deep multiple instance learning for image classification and auto-annotation.” CVPR, 2015.</p>
<p>[6] Ilse, Maximilian, Jakub M. Tomczak, and Max Welling. “Attention-based deep multiple instance learning.” arXiv preprint arXiv:1802.04712 (2018).</p>
<p>[7] Lee, Kuang-Huei, et al. “Cleannet: Transfer learning for scalable image classifier training with label noise.” CVPR, 2018.</p>
<p>[8] Liu, Tongliang, and Dacheng Tao. “Classification with noisy labels by importance reweighting.” T-PAMI, 2015.</p>
<p>[9] Misra, Ishan, et al. “Seeing through the human reporting bias: Visual classifiers from noisy human-centric labels.” CVPR, 2016.</p>
<p>[10] Guo, Sheng, et al. “Curriculumnet: Weakly supervised learning from large-scale web images.” ECCV, 2018.</p>
<p>[11] Jiang, Lu, et al. “Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels.” arXiv preprint arXiv:1712.05055 (2017).</p>
<p>[12] Reed, Scott, et al. “Training deep neural networks on noisy labels with bootstrapping.” arXiv preprint arXiv:1412.6596 (2014).</p>
<p>[13] Krause, Jonathan, et al. “The unreasonable effectiveness of noisy data for fine-grained recognition.” ECCV, 2016. </p>
<p>[14] Yeung, Serena, et al. “Learning to learn from noisy web videos.” CVPR, 2017.</p>
<p>[15] Veit, Andreas, et al. “Learning from noisy large-scale datasets with minimal supervision.” CVPR, 2017.</p>
<p>[16] Xu, Zhe, et al. “Webly-supervised fine-grained visual categorization via deep domain adaptation.” T-PAMI, 2016.</p>
<p>[17] Li, Yuncheng, et al. “Learning from noisy labels with distillation.” ICCV, 2017.</p>
<p>[18] Kim, Youngdong, et al. “Nlnl: Negative learning for noisy labels.” ICCV, 2019.</p>
<p>[19] “MetaCleaner: Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition”, CVPR, 2019.</p>
<p>[20] Huang, Jinchi, et al. “O2u-net: A simple noisy label detection approach for deep neural networks.” ICCV, 2019.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>weakly-supervised</tag>
        <tag>webly-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title>Weak-shot Object Detection</title>
    <url>/2022/06/16/classification_detection_segmentation/Weak-shot%20Object%20Detection/</url>
    <content><![CDATA[<p>Weak-shot object detection is also called cross-supervised or mixed-supervised object detection. Specifically, all categories are splitted into base categories and novel categories. Base categories have box-level annotation while novel categories only have image-level annotations. </p>
<ul>
<li><p>Transfer common objectness: <a href="https://arxiv.org/pdf/2007.07986.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1802.09778" target="_blank" rel="noopener">[3]</a></p>
</li>
<li><p>Transfer the mapping from inaccurate bounding boxes to accurate bounding boxes: <a href="https://arxiv.org/pdf/2006.15056.pdf" target="_blank" rel="noopener">[2]</a> </p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Zhong, Yuanyi, et al. “Boosting weakly supervised object detection with progressive knowledge transfer.” European Conference on Computer Vision. Springer, Cham, 2020.</p>
<p>[2] Chen, Zitian, et al. “Cross-Supervised Object Detection.” arXiv preprint arXiv:2006.15056 (2020).</p>
<p>[3] Li, Yan, et al. “Mixed supervised object detection with robust objectness transfer.” IEEE transactions on pattern analysis and machine intelligence 41.3 (2018): 639-653.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>weakly-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title>Watermark Removal</title>
    <url>/2022/06/16/image_video_synthesis/Watermark%20Removal/</url>
    <content><![CDATA[<ul>
<li><p>watermark removal: ICA <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4014106" target="_blank" rel="noopener">[4]</a>, inpainting <a href="https://scholars.lib.ntu.edu.tw/bitstream/123456789/123926/1/12.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
<li><p>watermarks consistent across a collection of images: multi-image matting and reconstruction <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Dekel_On_the_Effectiveness_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
</ul>
<ul>
<li>Survey papers on watermarking: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=939835" target="_blank" rel="noopener">[1]</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1560462" target="_blank" rel="noopener">[2]</a></li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>Podilchuk, Christine I., and Edward J. Delp. “Digital watermarking: algorithms and applications.” IEEE signal processing Magazine 18.4 (2001): 33-46.</li>
<li>Potdar, Vidyasagar M., Song Han, and Elizabeth Chang. “A survey of digital image watermarking techniques.” INDIN’05. 2005 3rd IEEE International Conference on Industrial Informatics, 2005.. IEEE, 2005.</li>
<li>Dekel, Tali, et al. “On the effectiveness of visible watermarks.” CVPR, 2017.</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>watermark</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual Sentiment Analysis</title>
    <url>/2022/06/16/deep_learning/Visual%20Sentiment%20Analysis/</url>
    <content><![CDATA[<ol>
<li><p><a href="https://www.ijcai.org/proceedings/2017/0456.pdf" target="_blank" rel="noopener">Joint Image Emotion Classification and Distribution Learning via Deep Convolutional Neural Network</a></p>
</li>
<li><p><a href="http://faculty.ucmerced.edu/mhyang/papers/cvpr2018_visual_sentiment.pdf" target="_blank" rel="noopener">Weakly Supervised Coupled Networks for Visual Sentiment Analysis</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>visual sentiment</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual Object Tracking</title>
    <url>/2022/06/16/deep_learning/Visual%20Object%20Tracking/</url>
    <content><![CDATA[<h3 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h3><p>Tracking is challenging due to the following factors: deformation, illumination variation, blur&amp;fast motion, background clutter, rotation, scale, boundary effect</p>
<h3 id="History"><a href="#History" class="headerlink" title="History"></a>History</h3><p>Tracking methods can be roughly categorized into generative methods and discriminative methods(feature+machine learning). Recently, correlation filter based methods and deep learning methods are dominant. </p>
<ul>
<li>Meanshift: density based, ASMS <a href="https://github.com/vojirt/asms" target="_blank" rel="noopener">https://github.com/vojirt/asms</a></li>
<li>Particle filter: particle based statistical method</li>
<li>Optical flow: match feature points between neighboring frames</li>
<li>correlation filter: KCF, DCF, CSK, CN, DSST, SRDCF, ECO. Basic CF methods are sensitive to deformation, fast motion, and boundary effect.</li>
<li>deep learning: GOTURN, MDNet, TCNN, SiamFC</li>
</ul>
<p>Two research groups contribute to CF methods most:</p>
<ul>
<li>Oxford: <a href="https://www.robots.ox.ac.uk/~luca/" target="_blank" rel="noopener">https://www.robots.ox.ac.uk/~luca/</a>, </li>
<li>Linkoping: <a href="http://users.isy.liu.se/en/cvl/marda26/" target="_blank" rel="noopener">http://users.isy.liu.se/en/cvl/marda26/</a></li>
</ul>
<p><strong>Comparison of Speed and Performance</strong><br><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/LsTWGvk.jpg" alt=""></p>
<h3 id="Survey-papers"><a href="#Survey-papers" class="headerlink" title="Survey papers"></a>Survey papers</h3><ul>
<li>Object tracking: A survey, 2006</li>
<li>Object tracking benchmark, 2015</li>
</ul>
<h3 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h3><ul>
<li>OTB50/100: <a href="http://cvlab.hanyang.ac.kr/tracker_benchmark/" target="_blank" rel="noopener">http://cvlab.hanyang.ac.kr/tracker_benchmark/</a></li>
<li>VOT2016: <a href="http://www.votchallenge.net/vot2016/dataset.html" target="_blank" rel="noopener">http://www.votchallenge.net/vot2016/dataset.html</a> </li>
</ul>
<h3 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h3><ul>
<li>Visual Object Tracking (VOT) challenge:<br><a href="http://www.votchallenge.net/challenges.html" target="_blank" rel="noopener">http://www.votchallenge.net/challenges.html</a><br>VOT2016 has released the code of many trackers: <a href="http://votchallenge.net/vot2016/trackers.html" target="_blank" rel="noopener">http://votchallenge.net/vot2016/trackers.html</a></li>
<li>Multiple Object Tracking Challenge (MOT) challenge:<br><a href="https://motchallenge.net/" target="_blank" rel="noopener">https://motchallenge.net/</a></li>
</ul>
<h3 id="Detection-based-Tracking"><a href="#Detection-based-Tracking" class="headerlink" title="Detection based Tracking"></a>Detection based Tracking</h3><p>Detection based tracking is also named as  tracking by detection or multiple object tracking. (MOT Challenge)</p>
<p>TLD (tracking-learning-detection): update tracker and detector during learning<br><a href="http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/tld.html" target="_blank" rel="noopener">http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/tld.html</a></p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>object tracking</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual Dialogue</title>
    <url>/2022/06/16/deep_learning/Visual%20Dialogue/</url>
    <content><![CDATA[<p>Visual dialogue <a href="https://arxiv.org/pdf/1611.08669.pdf" target="_blank" rel="noopener">[1]</a>: a dialogue with one image</p>
<p>Multimodal Dialogue <a href="https://arxiv.org/pdf/1704.00200.pdf" target="_blank" rel="noopener">[2]</a><a href="https://dl.acm.org/citation.cfm?id=3240605" target="_blank" rel="noopener">[3]</a>: a dialogue with multiple images</p>
<p>[1] Visual Dialog</p>
<p>[2] Towards Building Large Scale Multimodal Domain-Aware Conversation Systems</p>
<p>[3] Knowledge-aware Multimodal Dialogue Systems</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>VQA</tag>
      </tags>
  </entry>
  <entry>
    <title>Video Object Segmentation</title>
    <url>/2022/06/16/classification_detection_segmentation/Video%20Object%20Segmentation/</url>
    <content><![CDATA[<p>Given the segmentation mask of the first frame of a video clip, predict the segmentation masks in the subsequent frames. </p>
<ol>
<li><p>Davis challenge <a href="https://davischallenge.org/" target="_blank" rel="noopener">https://davischallenge.org/</a> held since 2017, related papers <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Perazzi_A_Benchmark_Dataset_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1704.00675.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>YouTube-VOS: A Large-Scale Benchmark for Video Object Segmentation <a href="https://youtube-vos.org/home" target="_blank" rel="noopener">https://youtube-vos.org/home</a></p>
</li>
<li><p>GyGO: an E-commerce Video Object Segmentation Dataset by Visualead <a href="https://github.com/ilchemla/gygo-dataset" target="_blank" rel="noopener">https://github.com/ilchemla/gygo-dataset</a></p>
</li>
</ol>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h4><ol>
<li><p>Perazzi, Federico, et al. “A benchmark dataset and evaluation methodology for video object segmentation.” CVPR, 2016.</p>
</li>
<li><p>Pont-Tuset, Jordi, et al. “The 2017 davis challenge on video object segmentation.” arXiv preprint arXiv:1704.00675 (2017).</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>video object segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>Video Instance Segmentation</title>
    <url>/2022/06/16/classification_detection_segmentation/Video%20Instance%20Segmentation/</url>
    <content><![CDATA[<ul>
<li><p>Track-by-Detect: MaskTrack R-CNN <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Video_Instance_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>Clip-Match: Vistr <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_End-to-End_Video_Instance_Segmentation_With_Transformers_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>Propose-Reduce: <a href="https://arxiv.org/pdf/2103.13746.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Yang, Linjie, Yuchen Fan, and Ning Xu. “Video instance segmentation.” ICCV, 2019.</p>
<p>[2] Wang, Yuqing, et al. “End-to-end video instance segmentation with transformers.” CVPR, 2021.</p>
<p>[3] Lin, Huaijia, et al. “Video instance segmentation with a propose-reduce paradigm.” ICCV, 2021.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>instance segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>Video Harmonization</title>
    <url>/2022/06/16/image_video_synthesis/Video%20Harmonization/</url>
    <content><![CDATA[<p>First deep learning approach for video harmonization <a href="https://arxiv.org/pdf/1809.01372.pdf" target="_blank" rel="noopener">[1]</a></p>
<p>[1] Haozhi Huang, Senzhe Xu, Junxiong Cai, Wei Liu, Shimin Hu, “Temporally Coherent Video Harmonization Using<br>Adversarial Networks”, arxiv, 2018.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>harmonization</tag>
      </tags>
  </entry>
  <entry>
    <title>Variants of Convolution in Deep Learning</title>
    <url>/2022/06/16/deep_learning/Variants%20of%20Convolution%20in%20Deep%20Learning/</url>
    <content><![CDATA[<ol>
<li><p>Depthwise convolution: do not sum over all the channels. So when the number of input channels is $n_{in}$ and  the number of filters is $n_{filter}$, the number of output channels is $n_{in}\times n_{filter}$.</p>
</li>
<li><p>Pointwise convolution: pointwise fully connected across all the channels.</p>
</li>
<li><p>Group convolution: divide channels into several groups and perform pointwise convolution within each group. Note that Pointwise convolution is the special case of group convolution when there is only one group. Group convolution is used in <a href="https://arxiv.org/pdf/1707.01083.pdf" target="_blank" rel="noopener">ShuffleNet</a>.</p>
</li>
<li><p>Depthwise separable convolution = depthwise convolution + pointwise convolution</p>
</li>
<li><p>Dilation convolution or atrous convolution: increase the receptive field without increasing the number of parameters, typically used for <a href="https://arxiv.org/pdf/1411.4038.pdf" target="_blank" rel="noopener">segmentation</a>. </p>
</li>
<li><p><a href="https://arxiv.org/pdf/1703.06211.pdf" target="_blank" rel="noopener">Deformable convolution</a> (left) and <a href="https://arxiv.org/pdf/1506.02025.pdf" target="_blank" rel="noopener">spatial transformation network</a> (right): these two methods both belong to irregular convolution and tweak the coordinates on the input feature map. Deformable convolution learn sthe offset while spatial transformation network learns the affine transformation.<br> <img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/ht5DXxp.jpg" width="100%" height="100%"></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1709.01507.pdf" target="_blank" rel="noopener">Squeeze-and-Excitation</a>: learn different weights for each channel.<br> <img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/CMk5i17.jpg" width="80%" height="80%"></p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>network structure</tag>
      </tags>
  </entry>
  <entry>
    <title>VAE</title>
    <url>/2022/06/16/image_video_synthesis/VAE/</url>
    <content><![CDATA[<h2 id="Advanced-VAE"><a href="#Advanced-VAE" class="headerlink" title="Advanced VAE"></a>Advanced VAE</h2><ol>
<li><p>VQVAE <a href="https://arxiv.org/pdf/1711.00937.pdf" target="_blank" rel="noopener">[1]</a>,VQVAE2 <a href="https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf" target="_blank" rel="noopener">[2]</a>. Accelerate auto-regression: <a href="https://arxiv.org/pdf/2111.12701.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://arxiv.org/pdf/2202.04200.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
<li><p>NVAE <a href="https://arxiv.org/pdf/2007.03898.pdf" target="_blank" rel="noopener">[3]</a> </p>
</li>
</ol>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Oord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu. “Neural discrete representation learning.” arXiv preprint arXiv:1711.00937 (2017).<br>[2] Razavi, Ali, Aaron van den Oord, and Oriol Vinyals. “Generating diverse high-fidelity images with vq-vae-2.” Advances in neural information processing systems. 2019.<br>[3] Vahdat, Arash, and Jan Kautz. “Nvae: A deep hierarchical variational autoencoder.” arXiv preprint arXiv:2007.03898 (2020).<br>[4] Bond-Taylor, Sam, et al. “Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes.” arXiv preprint arXiv:2111.12701 (2021).<br>[5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman, “MaskGIT: Masked Generative Image Transformer”, arXiv preprint arXiv:2202.04200.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>Deconvolution Layer</title>
    <url>/2022/06/16/deep_learning/Upsampling%20Layer/</url>
    <content><![CDATA[<ol>
<li><p>unpooling</p>
<p> <img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/Ot61UEE.jpg" widt="80%" width="50%"></p>
</li>
<li><p>transpose convolution (learnable upsampling)</p>
<p> <img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/h7GQbIH.jpg" widt="90%" width="50%"></p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>network structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Tricky Back Propagation</title>
    <url>/2022/06/16/deep_learning/Tricky%20Back%20Propagation/</url>
    <content><![CDATA[<p>Some operations are indifferentiable, which causes difficulties for back propagation.</p>
<ol>
<li><p>sample from distribution: reparameterization trick<br><a href="https://arxiv.org/pdf/1312.6114.pdf" target="_blank" rel="noopener">Auto-Encoding Variational Bayes</a> </p>
</li>
<li><p>argmax: soft argmax<br><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.4107&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Gradient Descent Optimization of Smoothed Information Retrieval Metrics</a> </p>
</li>
<li><p>crop: two-dimension boxcar function<br><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition</a> </p>
</li>
<li><p>delta function: lookup table+blur kernel<br><a href="https://arxiv.org/pdf/1801.05117.pdf" target="_blank" rel="noopener">Reblur2Deblur: Deblurring Videos via Self-Supervised Learning</a> </p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2022/06/16/deep_learning/Transformer/</url>
    <content><![CDATA[<p>Transformer <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">[1]</a> is multi-head self-attention, which can be used for sequence-to-sequence or sequence-to-label applications.</p>
<h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><ul>
<li>classification: ViT<a href="https://arxiv.org/pdf/2010.11929.pdf" target="_blank" rel="noopener">[2]</a> RVT <a href="https://arxiv.org/pdf/2105.07926.pdf" target="_blank" rel="noopener">[12]</a></li>
<li>object detection: DETR<a href="https://arxiv.org/pdf/2005.12872.pdf" target="_blank" rel="noopener">[3]</a></li>
<li>person reID: <a href="https://arxiv.org/pdf/2102.04378.pdf" target="_blank" rel="noopener">[7]</a></li>
<li>general low-level vision (super-resolution, denoising, deraining) <a href="https://arxiv.org/pdf/2012.00364.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://arxiv.org/pdf/2106.03106.pdf" target="_blank" rel="noopener">[16]</a></li>
<li>unconditional GAN: <a href="https://arxiv.org/pdf/2102.07074.pdf" target="_blank" rel="noopener">[10]</a></li>
<li>video understanding: <a href="https://arxiv.org/pdf/2102.05095.pdf" target="_blank" rel="noopener">[11]</a></li>
<li>image colorization: <a href="https://openreview.net/pdf?id=5NA1PinlGFu" target="_blank" rel="noopener">[6]</a> <a href="https://ci.idm.pku.edu.cn/Weng_ECCV22b.pdf" target="_blank" rel="noopener">[17]</a></li>
<li>style transfer: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_StyTr2_Image_Style_Transfer_With_Transformers_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[15]</a></li>
</ul>
<h3 id="Combination-of-transformer-and-CNN"><a href="#Combination-of-transformer-and-CNN" class="headerlink" title="Combination of transformer and CNN:"></a>Combination of transformer and CNN:</h3><ul>
<li>T2T-Vit <a href="https://arxiv.org/pdf/2101.11986.pdf" target="_blank" rel="noopener">[9]</a></li>
<li>TransUNet <a href="https://arxiv.org/pdf/2102.04306.pdf" target="_blank" rel="noopener">[8]</a></li>
<li>Next-ViT <a href="https://arxiv.org/pdf/2207.05501.pdf" target="_blank" rel="noopener">[14]</a></li>
</ul>
<h3 id="Interpretability"><a href="#Interpretability" class="headerlink" title="Interpretability"></a>Interpretability</h3><ul>
<li>beyond attention visualization <a href="https://arxiv.org/pdf/2012.09838" target="_blank" rel="noopener">[5]</a></li>
</ul>
<h3 id="Position-embedding"><a href="#Position-embedding" class="headerlink" title="Position embedding"></a>Position embedding</h3><ul>
<li><a href="https://mp.weixin.qq.com/s/QlR528MYCioEuYwJIs20Pg" target="_blank" rel="noopener">A summary</a></li>
</ul>
<h3 id="Lightweight-Transformer"><a href="#Lightweight-Transformer" class="headerlink" title="Lightweight Transformer"></a>Lightweight Transformer</h3><ul>
<li>XFormer <a href="https://arxiv.org/pdf/2207.07268.pdf" target="_blank" rel="noopener">[13]</a></li>
</ul>
<p><a href="https://mp.weixin.qq.com/s/HRRS5Piy_SsZy41WYg-H8Q" target="_blank" rel="noopener">A summary of efficient Transformers</a></p>
<h3 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h3><ul>
<li><a href="https://arxiv.org/pdf/2012.12556.pdf" target="_blank" rel="noopener">A survey on visual transformer</a></li>
<li><a href="https://arxiv.org/pdf/2101.01169.pdf" target="_blank" rel="noopener">Transformers in Vision: A Survey</a></li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Vaswani, Ashish, et al. “Attention is all you need.” NeurIPS, 2017.</p>
<p>[2] Dosovitskiy, Alexey, et al. “An image is worth 16x16 words: Transformers for image recognition at scale.” arXiv preprint arXiv:2010.11929 (2020).</p>
<p>[3] Carion, Nicolas, et al. “End-to-End Object Detection with Transformers.” arXiv preprint arXiv:2005.12872 (2020).</p>
<p>[4] Chen, Hanting, et al. “Pre-Trained Image Processing Transformer.” arXiv preprint arXiv:2012.00364 (2020).</p>
<p>[5] Chefer, Hila, Shir Gur, and Lior Wolf. “Transformer Interpretability Beyond Attention Visualization.” arXiv preprint arXiv:2012.09838 (2020).</p>
<p>[6] Manoj Kumar, Dirk Weissenborn &amp; Nal Kalchbrenner, “COLORIZATION TRANSFORMER”, ICLR, 2021.</p>
<p>[7] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, Wei Jiang, “TransReID: Transformer-based Object Re-Identification”, arXiv preprint arXiv:2102.04378 (2021).</p>
<p>[8] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L. Yuille, Yuyin Zhou, “TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation”, arXiv preprint arXiv:2102.04306 (2021).</p>
<p>[9] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, Shuicheng Yan, “Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet”, arXiv preprint arXiv:2101.11986 (2021).</p>
<p>[10] Yifan Jiang, Shiyu Chang, Zhangyang Wang, “TransGAN: Two Transformers Can Make One Strong GAN”, arXiv preprint arXiv:2102.07074 (2021).</p>
<p>[11] Is Space-Time Attention All You Need for Video Understanding.</p>
<p>[12] Towards Robust Vision Transformer.</p>
<p>[13] Zhao, Youpeng, et al. “Lightweight Vision Transformer with Cross Feature Attention.” arXiv preprint arXiv:2207.07268 (2022).</p>
<p>[14] Li, Jiashi, et al. “Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios.” arXiv preprint arXiv:2207.05501 (2022).</p>
<p>[15] Deng, Yingying, et al. “StyTr2: Image Style Transfer with Transformers.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</p>
<p>[16] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, Houqiang Li. “Uformer: A General U-Shaped Transformer for Image Restoration.”</p>
<p>[17] Shuchen Weng, Jimeng Sun, Yu Li, Si Li, and Boxin Shi. “CT2: Colorization Transformer via Color Tokens”, ECCV, 2022.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Texture Bias</title>
    <url>/2022/06/16/deep_learning/Texture%20Bias/</url>
    <content><![CDATA[<p>It is claimed in <a href="https://arxiv.org/pdf/1811.12231.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1907.12892.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://arxiv.org/pdf/1909.08245.pdf" target="_blank" rel="noopener">[3]</a> that CNN is biased towards texture, that is, CNN tends to classify an object based on its texture intead of its shape.</p>
<p>In <a href="https://proceedings.neurips.cc/paper/2020/file/db5f9f42a7157abe65bb145000b5871a-Paper.pdf" target="_blank" rel="noopener">[4]</a>, it is claimed that texture-bias is caused by data augmentation approach. Using different data augmentation approaches can introduce either texture-bias or shape-bias. Similarly, <a href="https://arxiv.org/pdf/2010.05981.pdf" target="_blank" rel="noopener">[5]</a> debiases shape and texture.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><p>Geirhos, Robert, et al. “ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.” ICLR, 2019. <a href="https://github.com/rgeirhos/texture-vs-shape" target="_blank" rel="noopener">[code]</a></p>
</li>
<li><p>Brochu, Francis. “Increasing Shape Bias in ImageNet-Trained Networks Using Transfer Learning and Domain-Adversarial Methods.” arXiv preprint arXiv:1907.12892 (2019). </p>
</li>
<li><p>Asadi, Nader, Mehrdad Hosseinzadeh, and Mahdi Eftekhari. “Towards Shape Biased Unsupervised Representation Learning for Domain Generalization.” arXiv preprint arXiv:1909.08245 (2019).</p>
</li>
<li><p>Hermann, Katherine, Ting Chen, and Simon Kornblith. “The origins and prevalence of texture bias in convolutional neural networks.” NeurIPS, (2020).</p>
</li>
<li><p>Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, Cihang Xie, “SHAPE-TEXTURE DEBIASED NEURAL NETWORK TRAINING”， ICLR, 2021.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>StyleGAN</title>
    <url>/2022/06/16/image_video_synthesis/StyleGAN/</url>
    <content><![CDATA[<ul>
<li>StyleGAN of all trades <a href="https://arxiv.org/pdf/2111.01619.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>StyleGANv1<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[5]</a></li>
<li>StyleGANv2<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[6]</a>: remove blob-shaped artifacts that resemble water droplets.</li>
<li>StyleGANv3<a href="https://papers.nips.cc/paper/2021/file/076ccd93ad68be51f23707988e934906-Paper.pdf" target="_blank" rel="noopener">[2]</a>: solve alias (texture sticking) issue, that is, detail appearing to glued to image coordinates instead of the surface of depicted objects.</li>
<li>StyleGAN-XL <a href="https://arxiv.org/pdf/2202.00273.pdf" target="_blank" rel="noopener">[3]</a>: extend to large dataset</li>
<li>3D styleGAN <a href="https://arxiv.org/pdf/2207.10642.pdf" target="_blank" rel="noopener">[4]</a></li>
</ul>
<h4 id="Image-editing-using-styleGA"><a href="#Image-editing-using-styleGA" class="headerlink" title="Image editing using styleGA"></a>Image editing using styleGA</h4><p>InsetGAN <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fruhstuck_InsetGAN_for_Full-Body_Image_Generation_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[7]</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Chong, Min Jin, Hsin-Ying Lee, and David Forsyth. “StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN.” arXiv preprint arXiv:2111.01619 (2021).</p>
<p>[2] Karras, Tero, et al. “Alias-free generative adversarial networks.” Thirty-Fifth Conference on Neural Information Processing Systems. 2021.</p>
<p>[3] Sauer, Axel, Katja Schwarz, and Andreas Geiger. “Stylegan-xl: Scaling stylegan to large diverse datasets.” arXiv preprint arXiv:2202.00273 (2022).</p>
<p>[4] Xiaoming Zhao, Fangchang Ma, David Güera, Zhile Ren, Alexander G. Schwing, Alex Colburn. “Generative Multiplane Images: Making a 2D GAN 3D-Aware”.</p>
<p>[5] Karras, Tero, Samuli Laine, and Timo Aila. “A style-based generator architecture for generative adversarial networks.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.</p>
<p>[6] Karras, Tero, et al. “Analyzing and improving the image quality of stylegan.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.</p>
<p>[7] Frühstück, Anna, et al. “Insetgan for full-body image generation.” CVPR, 2022.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Structural Reparameterization</title>
    <url>/2022/06/16/deep_learning/Structural%20Reparameterization/</url>
    <content><![CDATA[<ul>
<li>DiracNet <a href="https://arxiv.org/pdf/1706.00388.pdf" target="_blank" rel="noopener">[1]</a>, </li>
<li>Acnet <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ACNet_Strengthening_the_Kernel_Skeletons_for_Powerful_CNN_via_Asymmetric_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[2]</a></li>
<li>RepVGG <a href="https://arxiv.org/pdf/2101.03697.pdf" target="_blank" rel="noopener">[3]</a></li>
</ul>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Zagoruyko, Sergey, and Nikos Komodakis. “Diracnets: Training very deep neural networks without skip-connections.” arXiv preprint arXiv:1706.00388 (2017).</p>
<p>[2] Ding, Xiaohan, et al. “Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks.” Proceedings of the IEEE International Conference on Computer Vision. 2019.</p>
<p>[3] “RepVGG: Making VGG-style ConvNets Great Again”, arXiv, 2021.  </p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Statistics in Deep Learning</title>
    <url>/2022/06/16/deep_learning/Statistics%20in%20Deep%20Learning/</url>
    <content><![CDATA[<ol>
<li>correlation and covariance <a href="https://arxiv.org/pdf/1810.11730.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1607.01719.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://arxiv.org/pdf/1711.00848.pdf" target="_blank" rel="noopener">[3]</a></li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Gao, Hang, et al. “Low-shot learning via covariance-preserving adversarial augmentation networks.” NIPS, 2018.</p>
<p>[2] Sun, Baochen, and Kate Saenko. “Deep coral: Correlation alignment for deep domain adaptation.” ECCV, 2016.</p>
<p>[3] Kumar, Abhishek, Prasanna Sattigeri, and Avinash Balakrishnan. “Variational inference of disentangled latent concepts from unlabeled observations.” ICLR, 2018.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Spatial Transformation</title>
    <url>/2022/06/16/deep_learning/Spatial%20Transformation/</url>
    <content><![CDATA[<ul>
<li><p>parametric transform (affine transformation, thin-plate translation, etc): STN <a href="http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf" target="_blank" rel="noopener">[2]</a>, hierarchical STN <a href="https://arxiv.org/pdf/1801.09467.pdf" target="_blank" rel="noopener">[5]</a>, deformable style transfer <a href="https://arxiv.org/pdf/2003.11038.pdf" target="_blank" rel="noopener">[10]</a></p>
</li>
<li><p>learn distortion grid <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Adria_Recasens_Learning_to_Zoom_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Looking_for_the_Devil_in_the_Details_Learning_Trilinear_Attention_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[6]</a> <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Marin_Efficient_Segmentation_Learning_Downsampling_Near_Semantic_Boundaries_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[7]</a></p>
</li>
<li><p>learn conv offset: Deformable CNN v1<a href="https://arxiv.org/pdf/1703.06211.pdf" target="_blank" rel="noopener">[3]</a>, v2<a href="https://arxiv.org/pdf/1811.11168.pdf" target="_blank" rel="noopener">[4]</a>, deformable kernel <a href="https://arxiv.org/pdf/1910.02940.pdf" target="_blank" rel="noopener">[9]</a></p>
</li>
<li><p>optical flow: <a href="https://arxiv.org/pdf/2003.00696.pdf" target="_blank" rel="noopener">[8]</a> </p>
</li>
<li><p>swap disentangled <a href="https://ustcnewly.github.io/2020/03/30/deep_learning/Geometry-aware%20Deep%20Feature/">geometry-relevant feature</a></p>
</li>
<li><p>move keypoints: transGAGA <a href="https://arxiv.org/pdf/1904.09571.pdf" target="_blank" rel="noopener">[11]</a></p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Recasens, Adria, et al. “Learning to zoom: a saliency-based sampling layer for neural networks.” ECCV, 2018.</p>
<p>[2] Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. “Spatial transformer networks.” NIPS, 2015.</p>
<p>[3] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei:<br>Deformable Convolutional Networks. ICCV 2017.</p>
<p>[4] Xizhou Zhu, Han Hu, Stephen Lin, Jifeng Dai: Deformable ConvNets v2: More Deformable, Better Results. CoRR abs/1811.11168 (2018)</p>
<p>[5] Shu, Chang, et al. “Hierarchical Spatial Transformer Network.” arXiv preprint arXiv:1801.09467 (2018).</p>
<p>[6] Zheng, Heliang, et al. “Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-grained Image Recognition.” CVPR, 2019.</p>
<p>[7] Marin, Dmitrii, et al. “Efficient segmentation: Learning downsampling near semantic boundaries.” ICCV, 2019.</p>
<p>[8] Ren, Yurui, et al. “Deep Image Spatial Transformation for Person Image Generation.”, CVPR, 2020.</p>
<p>[9] Gao, Hang, et al. “Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation.” arXiv preprint arXiv:1910.02940 (2019).</p>
<p>[10] Kim, Sunnie SY, et al. “Deformable Style Transfer.” arXiv preprint arXiv:2003.11038 (2020).</p>
<p>[11] Wayne Wu, Kaidi Cao, Cheng Li, Chen Qian, Chen Change Loy: TransGaGa: Geometry-Aware Unsupervised Image-To-Image Translation. CVPR 2019</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>semi-supervised Learning</title>
    <url>/2022/06/16/deep_learning/Semi-supervised%20Learning/</url>
    <content><![CDATA[<h3 id="Surveys"><a href="#Surveys" class="headerlink" title="Surveys"></a>Surveys</h3><p><a href="https://arxiv.org/pdf/2006.05278.pdf" target="_blank" rel="noopener">An Overview of Deep Semi-Supervised Learning</a> </p>
<h3 id="Semi-supervised-learning"><a href="#Semi-supervised-learning" class="headerlink" title="Semi-supervised learning"></a>Semi-supervised learning</h3><ol>
<li><p>mixmatch <a href="https://arxiv.org/pdf/1905.02249" target="_blank" rel="noopener">[4]</a>: gracefully unify data augmentation, sharpening (low entropy), mixup.</p>
</li>
<li><p>unsupervised data augmentation <a href="https://arxiv.org/abs/1904.12848" target="_blank" rel="noopener">[5]</a> <a href="https://github.com/google-research/uda" target="_blank" rel="noopener">[code]</a></p>
</li>
</ol>
<h3 id="co-training-for-semi-supervised-Learning"><a href="#co-training-for-semi-supervised-Learning" class="headerlink" title="co-training for semi-supervised Learning"></a>co-training for semi-supervised Learning</h3><ol>
<li><p>multi-view: co-training <a href="https://arxiv.org/pdf/1803.05984.pdf" target="_blank" rel="noopener">[1]</a>, tri-net <a href="https://www.ijcai.org/proceedings/2018/0278.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>multi-graph: label propagation <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaohang_Zhan_Consensus-Driven_Propagation_in_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Deep Co-Training for Semi-Supervised Image Recognition</p>
<p>[2] Tri-net for Semi-Supervised Deep Learning</p>
<p>[3] Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition</p>
<p>[4] Berthelot, David, et al. “Mixmatch: A holistic approach to semi-supervised learning.” arXiv preprint arXiv:1905.02249 (2019).</p>
<p>[5] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le, “Unsupervised Data Augmentation for Consistency Training.” arXiv preprint arXiv:1904.12848 (2019).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>semi-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title>Self-supervised Learning</title>
    <url>/2022/06/16/deep_learning/Self-supervised%20Learning/</url>
    <content><![CDATA[<p>Design a proxy task using unlabeled or weakly-labeled data to help the original task. Essentially, self-supervised learning is multi-task learning with the proxy task not relying on heavy human annotation. The problem is which proxy task without human annotation is the most effective one.</p>
<p>Please refer to the tutorial slides <a href="http://bcmi.sjtu.edu.cn/home/niuli/download/self_supervised_learning_tutorial_naiyanwang.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://bcmi.sjtu.edu.cn/home/niuli/download/self_supervised_learning_tutorial_zisserman.pdf" target="_blank" rel="noopener">[2]</a>, the <a href="https://arxiv.org/pdf/1902.06162.pdf" target="_blank" rel="noopener">survey</a>, and the <a href="https://github.com/jason718/awesome-self-supervised-learning" target="_blank" rel="noopener">paper list</a>.</p>
<ol>
<li><p>image-to-image</p>
<ul>
<li>image-to-image translation: colorization <a href="https://arxiv.org/pdf/1603.08511.pdf" target="_blank" rel="noopener">[1]</a>, inpainting <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[2]</a>, cross-channel generation <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Split-Brain_Autoencoders_Unsupervised_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[3]</a></li>
<li>spatial location: relative location <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[1]</a>, jigsaw <a href="https://arxiv.org/pdf/1603.09246.pdf" target="_blank" rel="noopener">[2]</a>, predicting rotation <a href="https://arxiv.org/pdf/1803.07728.pdf" target="_blank" rel="noopener">[3]</a></li>
<li>contrastive learning: instance-wise contrastive learning (e.g., MOCO), prototypical contrastive learning (clustering) <a href="https://arxiv.org/pdf/1406.6909.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/2005.04966" target="_blank" rel="noopener">[2]</a> </li>
<li>MAE: <a href="https://siam-mae-video.github.io/resources/paper.pdf" target="_blank" rel="noopener">Siamese MAE</a></li>
</ul>
</li>
</ol>
<ol>
<li><p>video-to-image</p>
<ul>
<li>temporal coherence: <a href="http://people.csail.mit.edu/hmobahi/pubs/embedvideo.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.386.1586&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[2]</a> <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Jayaraman_Slow_and_Steady_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[3]</a></li>
<li>temporal order:  <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Unsupervised_Learning_of_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1603.08561.pdf" target="_blank" rel="noopener">[2]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Fernando_Self-Supervised_Video_Representation_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[3]</a></li>
<li>unsupervised image tasks with video clues: clustering <a href="https://arxiv.org/pdf/1406.6909.pdf" target="_blank" rel="noopener">[1]</a>, optical flow prediction <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Walker_Dense_Optical_Flow_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[1]</a>, unsupervised segmentation based on optical flow <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Pathak_Learning_Features_by_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Pathak_Learning_Features_by_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[2]</a>,unsupervised depth estimation based on optical flow <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Huaizu_Jiang_Self-Supervised_Relative_Depth_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[2]</a></li>
<li>video generation <a href="https://arxiv.org/pdf/1502.04681.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>cross-modal consistency: consistency between visual kernel and optical flow kernel <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Mahendran18/mahendran18.pdf" target="_blank" rel="noopener">[1]</a></li>
</ul>
</li>
<li><p>video-to-video: all video-to-image methods can be used for video-to-video by averaging frame features.</p>
<ul>
<li>3D rotation <a href="https://arxiv.org/pdf/1811.11387.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>Cubic puzzle <a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-KimD.1806.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>video localization and classification <a href="https://arxiv.org/pdf/1904.03597.pdf" target="_blank" rel="noopener">[1]</a></li>
</ul>
</li>
</ol>
<p><strong>Muti-task self-supervised learning:</strong> integrate multiple proxy tasks <a href="https://arxiv.org/pdf/1804.10069.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/abs/1708.07860" target="_blank" rel="noopener">[2]</a></p>
<p><strong>Combined with other frameworks:</strong> self-supervised GAN <a href="https://arxiv.org/pdf/1811.11212.pdf" target="_blank" rel="noopener">[1]</a></p>
<p>  A recent paper <a href="https://arxiv.org/pdf/1901.09005.pdf" target="_blank" rel="noopener">[1*]</a> claims that the best self-supervised learning method is still the earliest image inpainting model. The design of network architecture has a significant impact on the performance of self-supevivsed learning methods.</p>
<p>SimCLR <a href="https://arxiv.org/pdf/2002.05709.pdf" target="_blank" rel="noopener">[2*]</a> is a SOTA self-supervised learning method with performance approaching supervised learning. </p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1*] Alexander Kolesnikov, Xiaohua Zhai, Lucas Beyer: Revisiting Self-Supervised Visual Representation Learning. CVPR 2019.</p>
<p>[2*] Chen, Ting, et al. “A simple framework for contrastive learning of visual representations.” arXiv preprint arXiv:2002.05709 (2020).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>self-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title>Scene Text Recognition</title>
    <url>/2022/06/16/classification_detection_segmentation/Scene%20Text%20Recognition/</url>
    <content><![CDATA[<p>Scene text detection and recognition are challenging due to the following issues: scattered and sparse, blur, illumination, partial occlusion, multi-oriented, multi-lingual. </p>
<h2 id="Scene-text-detection"><a href="#Scene-text-detection" class="headerlink" title="Scene text detection:"></a>Scene text detection:</h2><p>The detection methods can be grouped into proposal-based method and part-based method.</p>
<p><strong>Paper list (in chronological order):</strong></p>
<ol>
<li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.188.187&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Detecting Text in Natural Scenes with<br>Stroke Width Transform</a>, CVPR 2010: assume consistent stroke width within each character</p>
</li>
<li><p><a href="http://www.iapr-tc11.org/dataset/MSRA-TD500/Detecting_Texts_of_Arbitrary_Orientations_in_Natural_Images.pdf" target="_blank" rel="noopener">Detecting Texts of Arbitrary Orientations in Natural Images</a>, CVPR 2012: design rotation-invariant features</p>
</li>
<li><p><a href="http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/jaderberg14deep.pdf" target="_blank" rel="noopener">Deep Features for Text Spotting</a>, ECCV 2014: add three branches for prediction</p>
</li>
<li><p><a href="http://www.whuang.org/papers/whuang2014_eccv.pdf" target="_blank" rel="noopener">Robust scene text detection with convolution neural network induced mser trees</a>, ECCV 2014</p>
</li>
<li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.717.4947&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Real-time Lexicon-free Scene Text<br>Localization and Recognition</a>, T-PAMI 2016</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1412.1842.pdf" target="_blank" rel="noopener">Reading Text in the Wild with Convolutional Neural Networks</a>, IJCV 2016</p>
</li>
<li><p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gupta_Synthetic_Data_for_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Synthetic Data for Text Localisation in Natural Images</a>, CVPR 2016: directly predict the bounding boxes, generate synthetic dataset</p>
</li>
<li><p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Multi-Oriented_Text_Detection_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Multi-oriented text detection with fully convolutional networks</a>, CVPR 2016</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1609.03605.pdf" target="_blank" rel="noopener">Detecting Text in Natural Image with Connectionist Text Proposal Network</a>, ECCV 2016: look for text lines an fine vertical text pieces. sliding windows fed to Bi-LSTM.</p>
</li>
<li><p><a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=SSD%3A+single+shot+multibox+detector&amp;btnG=" target="_blank" rel="noopener">SSD: single shot multibox detector</a>, ECCV 2016</p>
</li>
<li><p><a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12256/12121" target="_blank" rel="noopener">Reading Scene Text in Deep Convolutional Sequences</a>, AAAI 2016</p>
</li>
<li><p><a href="Scene text detection via holistic, multi-channel prediction">Scene text detection via holistic, multi-channel prediction</a>, arxiv 2016: holistic and pixel-wise predictions on text region map, character map, and linking<br>orientation map</p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Deep_Direct_Regression_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Deep Direct Regression for Multi-Oriented Scene Text Detection</a>, ICCV 2017</p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Hu_WordSup_Exploiting_Word_ICCV_2017_paper.pdf" target="_blank" rel="noopener">WordSup: Exploiting Word Annotations for Character based Text Detection</a>, ICCV 2017: a weakly supervised framework that can utilize word annotations for character detector training</p>
</li>
<li><p><a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14202/14295" target="_blank" rel="noopener">TextBoxes: A Fast Text Detector with a Single Deep Neural Network</a>, AAAI 2017</p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Shi_Detecting_Oriented_Text_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Detecting Oriented Text in Natural Images by Linking Segments</a>, CVPR 2017: detect text with segments and links</p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf" target="_blank" rel="noopener">EAST: An Efficient and Accurate Scene Text Detector</a>, CVPR 2017: use <a href="https://arxiv.org/pdf/1509.04874.pdf" target="_blank" rel="noopener">DenseBox</a> to generate quadrangle proposals</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1801.02765.pdf" target="_blank" rel="noopener">TextBoxes++: A Single-Shot Oriented Scene Text Detector</a>, TIP 2018:  extension of TextBoxes</p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liao_Rotation-Sensitive_Regression_for_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Rotation-sensitive Regression for Oriented Scene Text Detection</a>, CVPR 2018: rotation-sensitive feature maps for regression and rotation-invariant features for classification </p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1688.pdf" target="_blank" rel="noopener">Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation</a>, CVPR 2018: combine corner localization and region segmentation</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1801.01315.pdf" target="_blank" rel="noopener">PixelLink: Detecting Scene Text via Instance Segmentation</a>, AAAI 2018: rectangle enclosing instance segmentation mask, which is obtained based on text/non-text prediction and link prediction.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1703.01086.pdf" target="_blank" rel="noopener">Arbitrary-Oriented Scene Text Detection via Rotation Proposals</a>, TMM 2018: generate rotated proposals</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1807.01544.pdf" target="_blank" rel="noopener">TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes</a>, ECCV 2018: infer the center line area (TCL) and associated circle radius/rotation</p>
</li>
</ol>
<h2 id="Scene-text-recognition"><a href="#Scene-text-recognition" class="headerlink" title="Scene text recognition:"></a>Scene text recognition:</h2><p>The recognition methods can be grouped into character-level, word-level, and sequence-level.</p>
<p><strong>Paper list (in chronological order):</strong></p>
<ol>
<li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.648.354&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">End-to-End Scene Text Recognition</a>, ICCV 2011: detection using Random Ferns and recognition via Pictorial Structure with a Lexicon</p>
</li>
<li><p><a href="https://hal.inria.fr/hal-00818178/document" target="_blank" rel="noopener">Top-down and bottom-up cues for scene text recognition</a>, CVPR 2012: construct a CRF model to impose both bottom-up (i.e. character detections) and top-down (i.e. language statistics) cues</p>
</li>
<li><p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Shi_Scene_Text_Recognition_2013_CVPR_paper.pdf" target="_blank" rel="noopener">Scene text recognition using part-based tree-structured character detection</a>, CVPR 2013:  build a CRF model to incorporate the detection scores, spatial constraints and linguistic knowledge into one framework</p>
</li>
<li><p><a href="https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Bissacco_PhotoOCR_Reading_Text_2013_ICCV_paper.pdf" target="_blank" rel="noopener">PhotoOCR: Reading text in uncontrolled conditions</a>, ICCV 2013: automatically generate training data and perform OCR on web images</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s11263-014-0793-6" target="_blank" rel="noopener">Label embedding: A frugal baseline for text recognition</a>, IJCV 2015: learn a common space for image and word</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1412.1842.pdf" target="_blank" rel="noopener">Reading Text in the Wild with Convolutional Neural Networks</a>, IJCV 2016 </p>
</li>
<li><p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shi_Robust_Scene_Text_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Robust Scene Text Recognition with Automatic Rectification</a>, CVPR 2016</p>
</li>
<li><p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lee_Recursive_Recurrent_Nets_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Recursive Recurrent Nets with Attention Modeling for OCR in the Wild</a>, CVPR 2016: character-level language model embodied in a recurrent neural network</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/7801919" target="_blank" rel="noopener">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</a>, T-PAMI 2017</p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Cheng_Focusing_Attention_Towards_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Focusing Attention: Towards Accurate Text Recognition in Natural Images</a>, ICCV 2017: Focusing Network to handle the attention drift</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1706.01487.pdf" target="_blank" rel="noopener">Visual attention models for scene text recognition</a>, 2017 arxiv</p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper.pdf" target="_blank" rel="noopener">AON: Towards Arbitrarily-Oriented Text Recognition
</a>, CVPR 2018</p>
</li>
<li><p>(recommended by Guo)<a href="https://arxiv.org/pdf/1507.05717.pdf" target="_blank" rel="noopener">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</a>, T-PAMI, 2017</p>
</li>
</ol>
<h2 id="End-to-end"><a href="#End-to-end" class="headerlink" title="End-to-end"></a>End-to-end</h2><p>Integrate scene text detection and recognition in an end-to-end system.</p>
<p><strong>Paper list (in chronological order):</strong></p>
<ol>
<li><p><a href="http://cmp.felk.cvut.cz/~matas/papers/neumann-text-accv10.pdf" target="_blank" rel="noopener">A method for text localization and recognition in real-world images</a>, ACCV 2010</p>
</li>
<li><p><a href="http://fadaei.semnan.ac.ir/uploads/MV7.pdf" target="_blank" rel="noopener">Real-Time Scene Text Localization and Recognition</a>, CVPR 2012</p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Li_Towards_End-To-End_Text_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Towards End-to-end Text Spotting with Convolutional<br>Recurrent Neural Networks</a>, ICCV 2017: designed for horizontal scene text</p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Busta_Deep_TextSpotter_An_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Deep TextSpotter: An End-To-End Trainable Scene Text Localization and Recognition Framework</a>, ICCV 2017:  detect and recognize horizontal and multioriented<br>scene text</p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1699.pdf" target="_blank" rel="noopener">FOTS: Fast Oriented Text Spotting with a Unified Network</a>, CVPR 2018: using EAST as text detector and CRNN as text recognizer</p>
</li>
</ol>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><ul>
<li><a href="http://mclab.eic.hust.edu.cn/icdar2017chinese/" target="_blank" rel="noopener">RCTW-17</a></li>
<li><a href="http://rrc.cvc.uab.es/?ch=8" target="_blank" rel="noopener">MLT</a> </li>
<li><a href="https://github.com/Yuliang-Liu/Curve-Text-Detector" target="_blank" rel="noopener">SCUT-CTW1500</a></li>
<li><a href="https://github.com/cs-chan/Total-Text-Dataset" target="_blank" rel="noopener">Total-Text</a></li>
<li><a href="http://rrc.cvc.uab.es/?ch=4&amp;com=introduction" target="_blank" rel="noopener">ICDAR 2015</a></li>
<li><a href="http://www.iapr-tc11.org/mediawiki/index.php/MSRA_Text_Detection_500_Database_(MSRA-TD500" target="_blank" rel="noopener">MSRA-TD500</a>)</li>
<li><a href="http://cvit.iiit.ac.in/projects/SceneTextUnderstanding/IIIT5K.html" target="_blank" rel="noopener">IIIT 5K-Word</a></li>
<li><a href="https://vision.cornell.edu/se3/coco-text-2/" target="_blank" rel="noopener">COCO-Text</a></li>
</ul>
<h2 id="Surveys"><a href="#Surveys" class="headerlink" title="Surveys"></a>Surveys</h2><ul>
<li><a href="http://www.vlrlab.net/admin/uploads/avatars/FCS_TextSurvey_2015.pdf" target="_blank" rel="noopener">Scene text detection and recognition: Recent advances and future trends</a>, FCS 2015</li>
<li><a href="https://ieeexplore.ieee.org/abstract/document/6945320" target="_blank" rel="noopener">Text detection and recognition in imagery: A survey</a>, T-PAMI 2015</li>
</ul>
<h2 id="Special-Sessions"><a href="#Special-Sessions" class="headerlink" title="Special Sessions"></a>Special Sessions</h2><ol>
<li><p>Use Spatial Transformation Network (STN) <a href="https://arxiv.org/pdf/1509.05329.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1611.04298.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://arxiv.org/pdf/1707.08831.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://arxiv.org/pdf/1603.03915.pdf" target="_blank" rel="noopener">[4]</a></p>
</li>
<li><p>Use Deformable Convolution Network (DCN) <a href="https://arxiv.org/pdf/1805.01167" target="_blank" rel="noopener">[1]</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>scene text</tag>
      </tags>
  </entry>
  <entry>
    <title>Scale Variation for Object Detection</title>
    <url>/2022/06/16/classification_detection_segmentation/Scale%20Variation%20for%20Object%20Detection/</url>
    <content><![CDATA[<p>This problem is well discussed in <a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01497.pdf</a>. Different schemes for addressing multiple scales and sizes: (a) multi-scale input images (b) multi-scale feature maps (c) multi-scale anchor boxes on one feature map.</p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/HlFdOUN.jpg" width="100%"></p>
<ol>
<li><p>The first way is based on image/feature pyramids, e.g., in DPM and CNN-based methods. The images are resized at multiple scales, and feature maps (HOG or deep convolutional features) are computed for each scale. This way is often useful but is time-consuming. </p>
</li>
<li><p>The second way is to use sliding windows of multiple scales (and/or aspect ratios) of the feature maps. For example, in DPM, models of different aspect ratios are trained separately using different filter sizes. If this way is used to address multiple scales, it can be thought of as a “pyramid of filters”. The second way is usually adopted jointly with the first way. </p>
</li>
<li><p>As a comparison, our anchor-based method is built on comparison, our anchor-based method is built on a pyramid of anchors, which is more cost-efficient. Our method classifies and regresses bounding boxes with reference to anchor boxes of multiple scales and aspect ratios. It only relies on images and feature maps of a single scale, and uses filters (sliding windows on the feature map) of a single size. We show by experiments the effects of this scheme for addressing multiple scales and sizes. Because of this multi-scale design based on anchors, we can simply use the convolutional features computed on a single-scale image, as is also done by the Fast R-CNN detector. The design of multi- scale anchors is a key component for sharing features without extra cost for addressing scales.</p>
</li>
<li><p>use different dilation rates to vary receptive fields<br><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/ew7IgsX.jpg" width="80%"></p>
</li>
<li><p>use feature pyramid <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[1]</a>  </p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Lin, Tsung-Yi, et al. “Feature pyramid networks for object detection.” CVPR, 2017.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title>ROI Feature</title>
    <url>/2022/06/16/deep_learning/ROI%20feature/</url>
    <content><![CDATA[<ol>
<li>ROI pooling</li>
<li>ROI alignment <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>Precise RoI Pooling <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[2]</a></li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>He, Kaiming, et al. “Mask R-CNN.” ICCV, 2017.</li>
<li>Jiang, Borui, et al. “Acquisition of localization confidence for accurate object detection.” ECCV, 2018.</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>ROI</tag>
      </tags>
  </entry>
  <entry>
    <title>Repeated Patterns</title>
    <url>/2022/06/16/deep_learning/Repeated%20Patterns/</url>
    <content><![CDATA[<ol>
<li><p>detect repeated patterns <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/176177/1/paper.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>inpaint corrupted images with repeated patterns <a href="https://arxiv.org/pdf/2109.07161.pdf" target="_blank" rel="noopener">[2]</a>: use frequency convolution</p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Louis Lettry, Michal Perdoch, Kenneth Vanhoey, and Luc Van Gool. Repeated pattern detection using cnn activations. In WACV, 2017</p>
<p>[2] Suvorov, Roman, et al. “Resolution-robust Large Mask Inpainting with Fourier Convolutions.” arXiv preprint arXiv:2109.07161 (2021).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>blending</tag>
      </tags>
  </entry>
  <entry>
    <title>Receptive Field</title>
    <url>/2022/06/16/deep_learning/Receptive%20Field/</url>
    <content><![CDATA[<center><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/5oklFvo.png" width="40%" border="0"></center>

<ol>
<li><p>Real receptive field is smaller than theorical receptive field, and shrinks by $\frac{1}{\sqrt{n}}$ with $n$ being the number of layers.</p>
</li>
<li><p>Advanced networks (e.g., ResNet) have larger receptive field than old networks (e.g., AlexNet). In latest networks, the receptive field of each pixel in the last layer is as large as the whole image. Generally, larger receptive field leads to higher accuracy, but is not the only factor that influences the accuracy.</p>
</li>
</ol>
<p><a href="https://fomoro.com/research/article/receptive-field-calculator" target="_blank" rel="noopener">Fomoro</a>: a website to calculate receptive field.</p>
<p><a href="https://distill.pub/2019/computing-receptive-fields/" target="_blank" rel="noopener">Distill</a>: mathematical derivations and open-source library to compute receptive field.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>Wenjie Luo, Yujia Li, Raquel Urtasun, Richard S. Zemel:<br>Understanding the Effective Receptive Field in Deep Convolutional Neural Networks. NIPS, 2016.</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>network structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Privileged Information</title>
    <url>/2022/06/16/deep_learning/Privileged%20Information/</url>
    <content><![CDATA[<p>Learning Using Privileged Information (LUPI) or SVM+ was proposed by Vapnik in <a href="http://www.jmlr.org/papers/volume16/vapnik15b/vapnik15b.pdf" target="_blank" rel="noopener">[the first paper]</a>.</p>
<h3 id="High-level-ideas"><a href="#High-level-ideas" class="headerlink" title="High-level ideas:"></a>High-level ideas:</h3><ul>
<li>Use privileged information in the same way as for multi-view learning</li>
<li>Transfer between privileged information and primary information</li>
<li>Use privileged information to control the training process like training uncertainty or training difficulty (e.g., training loss, noise). </li>
</ul>
<h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications:"></a>Applications:</h3><ul>
<li><p>SVM for binary classification</p>
<ul>
<li>model the slack variable : SVM+ <a href="http://www.jmlr.org/papers/volume16/vapnik15b/vapnik15b.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>model the margin: <a href="https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Sharmanska_Learning_to_Rank_2013_ICCV_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1410.0389.pdf" target="_blank" rel="noopener">[2]</a></li>
<li>structural SVM: <a href="http://papers.nips.cc/paper/5561-object-localization-based-on-structural-svm-using-privileged-information.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>theoretical analysis: <a href="http://papers.nips.cc/paper/3960-on-the-theory-of-learnining-with-privileged-information.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.765.6609&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[2]</a></li>
</ul>
</li>
<li><p>Gaussian process classification</p>
<ul>
<li>GPC <a href="http://papers.nips.cc/paper/5373-mind-the-nuisance-gaussian-process-classification-using-privileged-noise.pdf" target="_blank" rel="noopener">[1]</a></li>
</ul>
</li>
<li><p>L2 loss for classification/Hash</p>
<ul>
<li>multi-labeling <a href="https://arxiv.org/pdf/1702.08681.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>Hash ITQ <a href="https://arxiv.org/pdf/1605.04034.pdf" target="_blank" rel="noopener">[1]</a></li>
</ul>
</li>
<li><p>clustering</p>
<ul>
<li>clustering <a href="https://s3.amazonaws.com/academia.edu.documents/37761842/15.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&amp;Expires=1511467863&amp;Signature=vuLos7m4wT%2Bh%2BLPqPFAOJwB3nu8%3D&amp;response-content-disposition=inline%3B%20filename%3DPrivileged_Information_for_Data_Clusteri.pdf" target="_blank" rel="noopener">[1]</a></li>
</ul>
</li>
<li><p>metric learning for verification/classification</p>
<ul>
<li>ITML+ <a href="http://www.vision.ee.ethz.ch/~liwenw/papers/Xu_TNNLS2015.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://pdfs.semanticscholar.org/2412/9967f9e6c4fca95165a7b9e6745c9debe0dd.pdf" target="_blank" rel="noopener">[2]</a></li>
<li>DML+ <a href="http://ieeexplore.ieee.org.ezproxy.rice.edu/stamp/stamp.jsp?arnumber=8080246" target="_blank" rel="noopener">[1]</a></li>
<li>OITML <a href="https://pdfs.semanticscholar.org/eb43/d52f2f707115acf59fd9c744aa32c8e8de29.pdf" target="_blank" rel="noopener">[1]</a>: ordinal-based ITML</li>
</ul>
</li>
<li><p>CRF</p>
<ul>
<li>probilistic inference <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w38/Vrigkas_Inferring_Human_Activities_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[1]</a>: similar with multi-view, but integral over the latent privileged information space during testing</li>
</ul>
</li>
<li><p>random forest</p>
<ul>
<li>conditional regression forest <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6553766" target="_blank" rel="noopener">[1]</a>: design node splitting criterion</li>
</ul>
</li>
<li><p>matrix factorization for collaborative filtering</p>
<ul>
<li>PriMF <a href="https://www.ijcai.org/proceedings/2017/0223.pdf" target="_blank" rel="noopener">[1]</a></li>
</ul>
</li>
<li><p>Maximum Entropy Discrimination</p>
<ul>
<li>MED <a href="https://www.ijcai.org/Proceedings/16/Papers/263.pdf" target="_blank" rel="noopener">[1]</a></li>
</ul>
</li>
<li><p>Deep Learning</p>
<ul>
<li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Hoffman_Learning_With_Side_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Hallucination network</a></li>
<li>classification loss <a href="https://arxiv.org/pdf/1702.08681.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>model drop-out <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Lambert_Deep_Learning_Under_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[1]</a></li>
</ul>
</li>
</ul>
<h3 id="Settings"><a href="#Settings" class="headerlink" title="Settings:"></a>Settings:</h3><ul>
<li>multi-view + LUPI <a href="http://ieeexplore.ieee.org.ezproxy.rice.edu/stamp/stamp.jsp?arnumber=8008811" target="_blank" rel="noopener">[1]</a></li>
<li>multi-task multi-class LUPI <a href="http://f4k.dieei.unict.it/proceedings/ICPR2012/media/files/1223.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>multi-instance LUPI <a href="http://bcmi.sjtu.edu.cn/home/niuli/paper/Exploiting%20Privileged%20Information%20from%20Web%20Data%20for%20Image%20Categorization.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>active learning + LUPI <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552533" target="_blank" rel="noopener">[1]</a></li>
<li>distillation + LUPI <a href="https://arxiv.org/pdf/1511.03643.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>domain adaptation + LUPI <a href="https://arxiv.org/pdf/1810.03756.pdf" target="_blank" rel="noopener">[1]</a></li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>privileged information</tag>
      </tags>
  </entry>
  <entry>
    <title>Object Detection</title>
    <url>/2022/06/16/classification_detection_segmentation/Object%20Detection/</url>
    <content><![CDATA[<p><strong>two-stage:</strong> use region proposal network (RPN) to generate proposals</p>
<ol>
<li><p><a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" target="_blank" rel="noopener">faster-RCNN</a> </p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/1otto8I.png" width="50%"></p>
</li>
</ol>
<p><strong>one-stage:</strong> remove RPN and use anchors with associated fixed proposals based on predefined scales/aspect-ratios. </p>
<ol>
<li>YOLO <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" target="_blank" rel="noopener">v1</a> <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf" target="_blank" rel="noopener">v2</a> <a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="noopener">v3</a></li>
<li><p><a href="https://arxiv.org/pdf/1512.02325.pdf" target="_blank" rel="noopener">SSD</a> </p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/1jX0kAI.jpg" width="50%"></p>
</li>
</ol>
<p><strong>Corner Points:</strong> remove anchors and directly predict corner points</p>
<ol>
<li><p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Hei_Law_CornerNet_Detecting_Objects_ECCV_2018_paper.pdf" target="_blank" rel="noopener">CornerNet</a></p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/pqK8zQM.jpg" width="90%"></p>
</li>
</ol>
<p><strong>No anchor:</strong> actually use aach cell as an anchor</p>
<ol>
<li><p><a href="https://arxiv.org/pdf/1904.11490.pdf" target="_blank" rel="noopener">RPDet</a>: use object centers as positive cells; paired with deformable CNN</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1904.03797.pdf" target="_blank" rel="noopener">FoveaBox</a>: use the cells in fovea area (object bounding box) as positive cells</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1901.03278.pdf" target="_blank" rel="noopener">Guided Anchoring</a>:  use deformable CNN to obtain adapted feature map</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Object Detection Loss</title>
    <url>/2022/06/16/classification_detection_segmentation/Object%20Detection%20Loss/</url>
    <content><![CDATA[<h3 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h3><script type="math/tex; mode=display">L(p,u,t^u,v)  L_{cls} (p,u) + \lambda[u\geq 1]L_{loc}(t^u,v),</script><p>   where $p$ is $(K+1)$-dim class probability vector with 0 being the background class, $u$ is the groundtruth class, $v$ is the ground-truth regression tuple, and $t^u$ is the predicted regression tuple for class $u$. $L_{cls}$ is a multi-class softmax loss and $L_{loc}$ is  a smooth L1 loss.</p>
<h3 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h3><script type="math/tex; mode=display">L(p_i,t_i)=L_{cls} (p_i,p_i^*) + \lambda p_i^*L_{reg}(t_i,t_i^*),</script><p> where $L_{cls}$ is a two-class (e.g., obj or not obg) (resp., multi-class) softmax loss for RPN (resp., gen)  and $L_{reg}$ is a smooth L1 loss. So the loss of faster RCNN is basically the same as fast RCNN.</p>
<p> fast and faster RCNN generate proposals, so they have the pos/neg labels for anchor boxes. However, the following SSD and YOLO do not generate proposals, so they need to match anchor boxes with ground-truth boxes.</p>
<h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><p> By using $x_{ij}^p$ as a binary indicator for matching the i-th default box to the j-th ground-truth box of category p. Multiple detection boxes can be matched to the same ground-truth box.</p>
<script type="math/tex; mode=display">l(x,c,l,g)=L_{conf}(x,c) + \alpha L_{loc}(x,l,g),</script><p> where $L_{conf}$ is a (K+1)-class softmax loss, and </p>
<script type="math/tex; mode=display">L_{loc} (x,l,g)=\sum_i \sum_j x_{ij}^k |l_i-g_j|.</script><h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><script type="math/tex; mode=display">\sum_{i=0}^{S^2}\sum_{j=0}^B \mathcal{1^{obj}}[(x_i-\hat{x}_i)^2+(y_i-\hat{y}_i)^2] + \sum_{i=0}^{S^2}\sum_{j=0}^B \mathcal{1^{obj}}[(\sqrt{w_i}-\sqrt{\hat{w}_i)}^2+(\sqrt{h_i}-\sqrt{\hat{y}_i})^2] + \sum_{i=0}^{S^2}\sum_{j=0}^B  \mathcal{1^{obj}} (C_i-\hat{C}_i)^2+ \sum_{i=0}^{S^2}\sum_{j=0}^B \mathcal{1^{noobj}}(C_i-\hat{C}_i)^2  + \sum_{i=0}^{S^2}\sum_{j=0}^B  \mathcal{1^{obj}}\sum_{c} (p_i(c)-\hat{p}_i(c))^2</script><p>Note that for the noobj anchorboxes, there is only one loss term involved. </p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Non-local Network</title>
    <url>/2022/06/16/deep_learning/Non-local%20Network/</url>
    <content><![CDATA[<p>Extensions of non-local network <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[1]</a>: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_Asymmetric_Non-Local_Neural_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Expectation-Maximization_Attention_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[3]</a> <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_CCNet_Criss-Cross_Attention_for_Semantic_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Dynamic_Graph_Message_Passing_Networks_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[5]</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Wang, Xiaolong, et al. “Non-local neural networks.” CVPR, 2018.</p>
<p>[2] Zhu, Zhen, et al. “Asymmetric non-local neural networks for semantic segmentation.” ICCV, 2019.</p>
<p>[3] Li, Xia, et al. “Expectation-maximization attention networks for semantic segmentation.” ICCV, 2019.</p>
<p>[4] Huang, Zilong, et al. “Ccnet: Criss-cross attention for semantic segmentation.” ICCV, 2019.</p>
<p>[5] Zhang, Li, et al. “Dynamic graph message passing networks.” CVPR, 2020.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>network structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Multi-modality Fusion</title>
    <url>/2022/06/16/deep_learning/Multi-modality%20Fusion/</url>
    <content><![CDATA[<ol>
<li>Concatenation/summation, or weighted (attention mechanism) concatenation/summation.</li>
<li>P(y|x1,x2)=P(y|x1)P(y|x2), with Gaussian distribution assumption <a href="https://arxiv.org/pdf/2112.05130.pdf" target="_blank" rel="noopener">[1]</a></li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>Huang, Xun, et al. “Multimodal Conditional Image Synthesis with Product-of-Experts GANs.” arXiv preprint arXiv:2112.05130 (2021).</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Multi-modal Problem</title>
    <url>/2022/06/16/deep_learning/Multi-modal%20Problem/</url>
    <content><![CDATA[<p>Multi-modal problem means that given an input, there exist multiple possible outputs instead of a single deterministic output. The key problem is the mode collapse problem.</p>
<ol>
<li><p>The ground-truth output belongs to one of K generated possibilities <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Vondrick_Anticipating_Visual_Representations_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[1]</a>. K is set beforehand.</p>
</li>
<li><p>Ensure bijection between random vector and output: Associate random factor (e.g., random vector z) with specific information <a href="https://arxiv.org/pdf/1711.11586.pdf" target="_blank" rel="noopener">[2]</a>. Either random factor is conditioned on specific information, or the generated output can recognize random factor. If the mapping from random vector to output is invertible (e.g., glow), there is a natural bijection between random vector and output <a href="http://de.arxiv.org/pdf/2006.14200?gitT" target="_blank" rel="noopener">[6]</a>. </p>
<center><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/gAVVos7.jpg" width="50%" border="0"></center>
</li>
<li><p>Enforce different random vectors to produce different outputs: push apart the outputs generated from different random vectors z with diversity loss or mode seeking loss <a href="https://arxiv.org/pdf/1903.05628.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://arxiv.org/pdf/1901.09024.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Normalized_Diversification_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Vondrick, Carl, Hamed Pirsiavash, and Antonio Torralba. “Anticipating visual representations from unlabeled video.” CVPR, 2016.</p>
<p>[2] Zhu, Jun-Yan, et al. “Toward multimodal image-to-image translation.” Advances in Neural Information Processing Systems. 2017.</p>
<p>[3] Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and Ming-Hsuan Yang. Mode seeking generative adversarial networks for diverse image synthesis. In CVPR, 2019.</p>
<p>[4] Dingdong Yang, Seunghoon Hong, Yunseok Jang, Tianchen Zhao, and Honglak Lee. Diversity-sensitive<br>conditional generative adversarial networks. arXiv preprint arXiv:1901.09024, 2019.</p>
<p>[5] Shaohui Liu, Xiao Zhang, Jianqiao Wangni, Jianbo Shi: Normalized Diversification. CVPR 2019: 10306-10315</p>
<p>[6] Lugmayr, Andreas, et al. “SRFlow: Learning the Super-Resolution Space with Normalizing Flow.” arXiv preprint arXiv:2006.14200 (2020).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>MLP</title>
    <url>/2022/06/16/deep_learning/MLP/</url>
    <content><![CDATA[<ul>
<li>classification <a href="https://proceedings.neurips.cc/paper/2021/file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/2105.02723.pdf" target="_blank" rel="noopener">[2]</a></li>
<li>detection, segmentation <a href="https://arxiv.org/pdf/2107.08391.pdf" target="_blank" rel="noopener">[3]</a></li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Tolstikhin, Ilya O., et al. “Mlp-mixer: An all-mlp architecture for vision.” Advances in Neural Information Processing Systems 34 (2021).</p>
<p>[2] Melas-Kyriazi, Luke. “Do you even need attention? a stack of feed-forward layers does surprisingly well on imagenet.” arXiv preprint arXiv:2105.02723 (2021).</p>
<p>[3] Lian, Dongze, et al. “As-mlp: An axial shifted mlp architecture for vision.” arXiv preprint arXiv:2107.08391 (2021).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Memory Network</title>
    <url>/2022/06/16/deep_learning/Memory%20Network/</url>
    <content><![CDATA[<ul>
<li><p>First paper of <a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="noopener">memory network</a>: I(input feature map), G(generalization), O(output feature map), R(response), use the following objective function to optimize the variables in I,G,O,R.</p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/MX43xxZ.jpg" width="50%"></p>
</li>
<li><p><a href="https://arxiv.org/abs/1503.08895" target="_blank" rel="noopener">end-to-end memory network</a>: easy back-propagation</p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/mAulfoo.jpg" width="50%"></p>
</li>
</ul>
<ul>
<li><p>semi-supervised learning with memory module <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yanbei_Chen_Semi-Supervised_Deep_Learning_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>few-shot learning with memory module <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Linchao_Zhu_Compound_Memory_Networks_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Memory_Matching_Networks_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://arxiv.org/pdf/1605.06065.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
<li><p>global memory <a href="https://arxiv.org/abs/2101.11939" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>short-term memory and long-term memory <a href="https://arxiv.org/pdf/2207.07115.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>Mask-aided Object Detection</title>
    <url>/2022/06/16/deep_learning/Mask-aided%20Object%20Detection/</url>
    <content><![CDATA[<h2 id="Help-generate-proposal"><a href="#Help-generate-proposal" class="headerlink" title="Help generate proposal:"></a>Help generate proposal:</h2><h3 id="Help-generate-proposal-1"><a href="#Help-generate-proposal-1" class="headerlink" title="Help generate proposal:"></a>Help generate proposal:</h3><ol>
<li><p>Combine semantic mask with feature map (e.g., concatenation, summation) to help predict bounding boxes: <a href="https://papers.nips.cc/paper/2021/file/20885c72ca35d75619d6a378edea9f76-Paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/2006.15056.pdf" target="_blank" rel="noopener">[2]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Pseudo_Mask_Augmented_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
<li><p>Generate proposals from semantic mask: <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Diba_Weakly_Supervised_Cascaded_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[4]</a> </p>
</li>
</ol>
<h3 id="Help-select-proposal"><a href="#Help-select-proposal" class="headerlink" title="Help select proposal:"></a>Help select proposal:</h3><ol>
<li><p>Assign weights to proposals based on semantic mask: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Weakly_Supervised_Object_Detection_With_Segmentation_Collaboration_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
<li><p>Use semantic mask surrounding each proposal as auxilary feature: <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Yunchao_Wei_TS2C_Tight_Box_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[6]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Yan Liu, Zhijie Zhang, Li Niu, Junjie Chen, Liqing Zhang, “Mixed Supervised Object Detection by Transferring Mask Prior and Semantic Similarity”, NeurIPS, 2021. </p>
<p>[2] Zitian Chen, Zhiqiang Shen, Jiahui Yu, Erik Learned-Miller: “Cross-Supervised Object Detection.” arXiv preprint arXiv:2006.15056 (2020)</p>
<p>[3] Zhao, Xiangyun, Shuang Liang, and Yichen Wei. “Pseudo mask augmented object detection.” CVPR, 2018.</p>
<p>[4] Diba, Ali, et al. “Weakly supervised cascaded convolutional networks.” CVPR, 2017.</p>
<p>[5] Li, Xiaoyan, et al. “Weakly supervised object detection with segmentation collaboration.” ICCV, 2019.</p>
<p>[6] Wei, Yunchao, et al. “Ts2c: Tight box mining with surrounding segmentation context for weakly supervised object detection.” ECCV, 2018.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Layout Generation</title>
    <url>/2022/06/16/deep_learning/Layout%20Generation/</url>
    <content><![CDATA[<ol>
<li>VAE/GAN: <a href="https://www.cs.cityu.edu.hk/~rynson/papers/sigg19.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/2104.02416.pdf" target="_blank" rel="noopener">[6]</a> <a href="https://arxiv.org/pdf/1909.00302.pdf" target="_blank" rel="noopener">[7]</a>(hierarchical encoder/decoder)</li>
<li>GNN: <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480494.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://arxiv.org/pdf/1901.06767.pdf" target="_blank" rel="noopener">[5]</a></li>
<li>autoregressive: <a href="https://arxiv.org/pdf/2006.14615.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://arxiv.org/pdf/1907.10719.pdf" target="_blank" rel="noopener">[4]</a></li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><p>Zheng, Xinru, et al. “Content-aware generative modeling of graphic design layouts.” ACM Transactions on Graphics (TOG) 38.4 (2019): 1-15.</p>
</li>
<li><p>Lee, Hsin-Ying, et al. “Neural design network: Graphic layout generation with constraints.” ECCV, 2020.</p>
</li>
<li><p>Gupta, Kamal, et al. “Layout Generation and Completion with Self-attention.” arXiv preprint arXiv:2006.14615 (2020).</p>
</li>
<li><p>Jyothi, Akash Abdu, et al. “Layoutvae: Stochastic scene layout generation from a label set.” ICCV, 2019.</p>
</li>
<li><p>Li, Jianan, et al. “Layoutgan: Generating graphic layouts with wireframe discriminators.” ICLR, 2019.</p>
</li>
<li><p>Arroyo, Diego Martin, Janis Postels, and Federico Tombari. “Variational Transformer Networks for Layout Generation.” CVPR, 2021.</p>
</li>
<li><p>Patil, Akshay Gadi, et al. “Read: Recursive autoencoders for document layout generation.” CVPR Workshops. 2020.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Jigsaw Puzzle</title>
    <url>/2022/06/16/deep_learning/Jigsaw%20Puzzle/</url>
    <content><![CDATA[<ol>
<li><p>Reorganize patches <a href="https://arxiv.org/pdf/1603.09246.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>Reorganize pixels <a href="https://www.researchgate.net/profile/Wan-Shen-4/publication/358259068_AggMapNet_enhanced_and_explainable_low-sample_omics_deep_learning_with_feature-aggregated_multi-channel_networks/links/61fba4891e98d168d7e97fa7/AggMapNet-enhanced-and-explainable-low-sample-omics-deep-learning-with-feature-aggregated-multi-channel-networks.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Noroozi, Mehdi, and Paolo Favaro. “Unsupervised learning of visual representations by solving jigsaw puzzles.” ECCV, 2016.</p>
<p>[2] Shen, Wan Xiang, et al. “AggMapNet: enhanced and explainable low-sample omics deep learning with feature-aggregated multi-channel networks.” Nucleic Acids Research (2022).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Interpretable Machine Learning</title>
    <url>/2022/06/16/deep_learning/Interpretable%20Machine%20Learning/</url>
    <content><![CDATA[<ol>
<li><p>Manipulate each layer/neuron, and observe the change of network parameters/activations.</p>
</li>
<li><p>Saliency map</p>
</li>
<li><p>Adversarial attack</p>
</li>
<li><p>Correlation</p>
</li>
<li><p>Information gain/loss</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Instance Image-to-Image Translation</title>
    <url>/2022/06/16/image_video_synthesis/Instance%20Image%20to%20Image%20Translation/</url>
    <content><![CDATA[<p>Translate one or multiple instances in an image: <a href="https://arxiv.org/pdf/1812.10889.pdf&amp;xid=17259,15700023,15700186,15700191,15700256,15700259,15700262,15700264.pdf" target="_blank" rel="noopener">[1]</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Mo, Sangwoo, Minsu Cho, and Jinwoo Shin. “Instagan: Instance-aware image-to-image translation.” arXiv preprint arXiv:1812.10889 (2018).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>Image-text Retrieval</title>
    <url>/2022/06/16/deep_learning/Image-text%20Retrieval/</url>
    <content><![CDATA[<h2 id="Webly-supervised-image-text-retrieval"><a href="#Webly-supervised-image-text-retrieval" class="headerlink" title="Webly supervised image-text retrieval"></a>Webly supervised image-text retrieval</h2><p>The first work <a href="https://arxiv.org/pdf/1808.07793.pdf" target="_blank" rel="noopener">[1]</a> on using web images and their tags to augment image-sentence pairs. We try to reproduce it, but it does not work at all. </p>
<p>The text associated with a web image generally consists of tags, title, and description.<br>The tags are very noisy, but they are acceptable for webly supervised image classification. The titles and descriptions are noisier. Only a few descriptions are complete sentences and match the corresponding images.</p>
<p><a href="https://github.com/google-research-datasets/conceptual-captions" target="_blank" rel="noopener">Conceptual caption dataset</a> <a href="https://www.aclweb.org/anthology/P18-1238.pdf" target="_blank" rel="noopener">[2]</a> crawled web images and their alt text, and developed an automatic pipeline that extracts, filters, and transforms candidate image-caption pairs, resulting in relatively clean image-text pairs. This large corpus of web image-text pairs can be used for pretraining image-text retrieval model or image captioning model. </p>
<h2 id="Image-text-Chinse-Datasets"><a href="#Image-text-Chinse-Datasets" class="headerlink" title="Image-text (Chinse) Datasets"></a>Image-text (Chinse) Datasets</h2><ul>
<li>AI challenger 2017: <a href="https://pan.baidu.com/s/1YziBPLiU2WmE0j35oaXeKw" target="_blank" rel="noopener">training set</a> code:asix <a href="https://pan.baidu.com/s/1p_0V89d4wfxk-7f7QsU9rg" target="_blank" rel="noopener">validation set</a> code:dcnn</li>
<li><a href="http://lixirong.net/datasets/flickr8kcn" target="_blank" rel="noopener">f30k-cn</a> </li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Niluthpol Chowdhury Mithun, Rameswar Panda, Evangelos E. Papalexakis, Amit K. Roy-Chowdhury:<br>Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval. ACM MM, 2018.</p>
<p>[2] Sharma, Piyush, et al. “Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.” ACL, 2018.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>weakly-supervised</tag>
        <tag>webly-supervised</tag>
        <tag>cross-modal retrieval</tag>
      </tags>
  </entry>
  <entry>
    <title>Image to Image Translation</title>
    <url>/2022/06/16/image_video_synthesis/Image%20to%20Image%20Translation/</url>
    <content><![CDATA[<h3 id="Optimization-based"><a href="#Optimization-based" class="headerlink" title="Optimization-based:"></a>Optimization-based:</h3><ul>
<li><p>texture synthesis</p>
<ul>
<li>Texture synthesis using convolutional neural networks. <a href="http://papers.nips.cc/paper/5633-texture-synthesis-using-convolutional-neural-networks.pdf" target="_blank" rel="noopener">[pdf]</a></li>
</ul>
</li>
<li><p>feature inversion</p>
<ul>
<li>Understanding Deep Image Representations by Inverting Them. </li>
</ul>
</li>
<li><p>style transfer = feature inversion + texture synthesis</p>
<ul>
<li><p>Image style transfer using convolutional neural networks. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/hwalsuklee/tensorflow-style-transfer" target="_blank" rel="noopener">[code]</a> (no training, test is slow)</p>
</li>
<li><p>Perceptual Losses for Real-Time Style Transfer and Super-Resolution. <a href="https://arxiv.org/pdf/1603.08155.pdf%7C" target="_blank" rel="noopener">[pdf]</a> (train a network for each style using style image and content image as inputs, real-time test, belong to one-to-one image mapping)</p>
</li>
<li><p>Texture Networks: Feed-forward Synthesis of Textures and Stylized Image. <a href="http://proceedings.mlr.press/v48/ulyanov16.pdf" target="_blank" rel="noopener">[pdf]</a></p>
</li>
<li><p>A learned representation for artistic style. <a href="https://arxiv.org/pdf/1610.07629.pdf" target="_blank" rel="noopener">[pdf]</a> (train a unified network for multiple styles)</p>
</li>
</ul>
</li>
</ul>
<h3 id="Feedforward-based"><a href="#Feedforward-based" class="headerlink" title="Feedforward-based:"></a>Feedforward-based:</h3><ul>
<li><p>super-resolution</p>
<ul>
<li><p>Learning a deep convolutional network for image super-resolution. <a href="http://ai2-s2-pdfs.s3.amazonaws.com/5763/c2c62463c61926c7e192dcc340c4691ee3aa.pdf" target="_blank" rel="noopener">[pdf]</a></p>
</li>
<li><p>Accurate Image Super-Resolution Using Very Deep Convolutional Networks <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/Jongchan/tensorflow-vdsr" target="_blank" rel="noopener">[code]</a> (VGG learns residual)</p>
</li>
<li><p>Accelerating the Super-Resolution Convolutional Neural Network. <a href="https://arxiv.org/pdf/1608.00367.pdf" target="_blank" rel="noopener">[pdf]</a> (hourglass structure, deconv)</p>
</li>
<li><p>Deeply-recursive convolutional network for image super-resolution. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[pdf]</a></p>
</li>
<li><p>Photo-realistic single image super-resolution using a generative adversarial network. <a href="https://arxiv.org/pdf/1609.04802.pdf" target="_blank" rel="noopener">[pdf]</a> (content_loss, adversarial loss)</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>inpainting or hole-filling</p>
<ul>
<li>Deep Image Inpainting. <a href="http://cs231n.stanford.edu/reports/2017/pdfs/328.pdf" target="_blank" rel="noopener">[pdf]</a> </li>
<li>Context Encoders: Feature Learning by Inpainting <a href="http://people.eecs.berkeley.edu/~pathak/papers/cvpr16.pdf" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/pathak22/context-encoder" target="_blank" rel="noopener">[code]</a></li>
</ul>
</li>
<li><p>colorization</p>
<ul>
<li><p>Colorful image colorization. <a href="https://arxiv.org/pdf/1603.08511.pdf" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/richzhang/colorization" target="_blank" rel="noopener">[code]</a></p>
</li>
<li><p>Learning Representations for Automatic Colorization. <a href="https://arxiv.org/pdf/1603.06668.pdf" target="_blank" rel="noopener">[pdf]</a> [<a href="https://github.com/gustavla/autocolorize" target="_blank" rel="noopener">code</a></p>
</li>
</ul>
</li>
<li><p>denoising</p>
<ul>
<li>Image Restoration Using Very Deep Convolutional EncoderDecoder Networks with Symmetric Skip Connections <a href="https://arxiv.org/pdf/1603.09056.pdf" target="_blank" rel="noopener">[pdf]</a> <a href="https://bitbucket.org/chhshen/image-denoising/" target="_blank" rel="noopener">[code]</a>:(conv and deconv)</li>
</ul>
</li>
<li><p>decompression</p>
<ul>
<li>Compression Artifacts Reduction by a Deep Convolutional Network <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dong_Compression_Artifacts_Reduction_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[pdf]</a></li>
</ul>
</li>
<li><p>dehaze/deraining</p>
<ul>
<li>Dehazenet: An end-to-end system for single image haze removal <a href="https://arxiv.org/pdf/1601.07661.pdf" target="_blank" rel="noopener">[pdf]</a></li>
</ul>
</li>
<li><p>demosaicking</p>
<ul>
<li>Deep joint demosaicking and denoising <a href="http://delivery.acm.org.ezproxy.rice.edu/10.1145/2990000/2982399/a191-gharbi.pdf?ip=128.42.202.150&amp;id=2982399&amp;acc=ACTIVE%20SERVICE&amp;key=B63ACEF81C6334F5%2EBB994C37505169BD%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;CFID=1002713420&amp;CFTOKEN=54421672&amp;__acm__=1509998099_5a382ee2ab7d300c852e5055dcf99b99" target="_blank" rel="noopener">[pdf]</a></li>
</ul>
</li>
<li><p><a href="https://ustcnewly.github.io/2018/11/27/deep_learning/Image%20Harmonization/">image harmonization</a></p>
</li>
</ul>
<ul>
<li>domain adaptation<ul>
<li>Unsupervised Pixel–Level Domain Adaptation with Generative Adversarial Network. <a href="Unsupervised Pixel–Level Domain Adaptation with Generative Adversarial Network">[pdf]</a> <a href="https://github.com/tensorflow/models/tree/master/research/domain_adaptation/pixel_domain_adaptation" target="_blank" rel="noopener">[code]</a></li>
</ul>
</li>
</ul>
<ul>
<li><p>general image-to-image translation</p>
<ul>
<li><p>paired training data</p>
<ul>
<li><p>Image-to-Image Translation with Conditional Adversarial Nets. <a href="https://arxiv.org/pdf/1611.07004v1.pdf" target="_blank" rel="noopener">[pdf]</a> <a href="https://phillipi.github.io/pix2pix/" target="_blank" rel="noopener">[code]</a>  (pixelGAN)</p>
</li>
<li><p>High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs. <a href="https://arxiv.org/pdf/1711.11585.pdf" target="_blank" rel="noopener">[pdf]</a>: extend pixel2pixel GAN with coarse-to-fine strategy.</p>
</li>
</ul>
</li>
<li><p>unpaired training data</p>
<ul>
<li><p>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. <a href="https://arxiv.org/pdf/1703.10593.pdf" target="_blank" rel="noopener">[pdf]</a><a href="https://github.com/junyanz/CycleGAN" target="_blank" rel="noopener">[code]</a> (CycleGAN)</p>
<ul>
<li>DualGAN: Unsupervised Dual Learning for Image-to-Image Translation <a href="https://arxiv.org/pdf/1704.02510.pdf" target="_blank" rel="noopener">[pdf]</a></li>
</ul>
</li>
<li><p>Learning to Discover Cross-Domain Relations with Generative Adversarial Networks. <a href="https://arxiv.org/pdf/1703.05192.pdf" target="_blank" rel="noopener">[pdf]</a> (discoGAN)</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Surveys"><a href="#Surveys" class="headerlink" title="Surveys"></a>Surveys</h3><ul>
<li><a href="https://arxiv.org/pdf/2101.08629.pdf" target="_blank" rel="noopener">Image-to-Image Translation: Methods and Applications</a></li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>image translation</tag>
      </tags>
  </entry>
  <entry>
    <title>Image Manipulation Detection</title>
    <url>/2022/06/16/deep_learning/Image%20Manipulation%20Detection/</url>
    <content><![CDATA[<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><ul>
<li><p>Casia V1.0/2.0: <a href="https://ieeexplore.ieee.org/document/6625374" target="_blank" rel="noopener">[paper]</a> <a href="https://github.com/namtpham/casia1groundtruth" target="_blank" rel="noopener">[v1]</a> <a href="https://github.com/namtpham/casia2groundtruth" target="_blank" rel="noopener">[v2]</a></p>
</li>
<li><p>Columbia Uncompressed Image Splicing Detection: <a href="https://www.ee.columbia.edu/ln/dvmm/downloads/authsplcuncmp/" target="_blank" rel="noopener">[dataset]</a></p>
</li>
<li><p>Pawel korus-Realistic Tampering Dataset: <a href="https://pkorus.pl/downloads" target="_blank" rel="noopener">[dataset]</a></p>
</li>
<li><p>Coverage: <a href="https://stefan.winklerbros.net/Publications/icip2016b.pdf" target="_blank" rel="noopener">[dataset]</a></p>
</li>
<li><p>NIST16: <a href="https://mfc.nist.gov/users/sign_in" target="_blank" rel="noopener">[dataset]</a></p>
</li>
<li><p>DEFACTO: <a href="https://defactodataset.github.io/" target="_blank" rel="noopener">[dataset]</a></p>
</li>
<li><p>self-made dataset with <a href="https://github.com/pengzhou1108/RGB-N" target="_blank" rel="noopener">[code]</a></p>
</li>
</ul>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><ul>
<li><p>MantraNet <a href="https://github.com/RonyAbecidan/ManTraNet-pytorch" target="_blank" rel="noopener">[code]</a>: compare each pixel with neighboring pixels</p>
</li>
<li><p>MAGritte <a href="https://github.com/vlkniaz/MAGritte" target="_blank" rel="noopener">[code]</a>: a combination of generation and discrimination</p>
</li>
<li><p>H-LSTM <a href="https://arxiv.org/pdf/1903.02495.pdf" target="_blank" rel="noopener">[paper]</a> <a href="https://github.com/jawadbappy/forgery_localization_HLED" target="_blank" rel="noopener">[code]</a>: 1. resampling features 2. use Hilbert curve to determine the patch order </p>
</li>
<li><p>Constrained-RCNN <a href="https://github.com/HuizhouLi/Constrained-R-CNN" target="_blank" rel="noopener">[code]</a>: constrained convolution</p>
</li>
<li><p>GSRNet <a href="https://arxiv.org/pdf/1811.09729.pdf" target="_blank" rel="noopener">[paper]</a> <a href="https://github.com/pengzhou1108/GSRNet" target="_blank" rel="noopener">[code]</a>: data augmentation</p>
</li>
<li><p>SPAN <a href="https://github.com/ZhiHanZ/IRIS0-SPAN" target="_blank" rel="noopener">[code]</a>: pyramid self-attention</p>
</li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>image manipulation</tag>
      </tags>
  </entry>
  <entry>
    <title>Image Loss</title>
    <url>/2022/06/16/deep_learning/Image%20Loss/</url>
    <content><![CDATA[<ol>
<li><p>perceptual loss <a href="https://arxiv.org/pdf/1603.08155.pdf" target="_blank" rel="noopener">[1]</a>: two images have similar semantic information</p>
<script type="math/tex; mode=display">\frac{1}{C_j H_j W_j}||\phi_j(\hat{x})-\phi_j(x)||^2</script></li>
<li><p>style loss <a href="https://arxiv.org/pdf/1603.08155.pdf" target="_blank" rel="noopener">[2]</a>: two images have similar channel correlation; related to bilinear pooling <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Lin_Bilinear_CNN_Models_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[6]</a></p>
<script type="math/tex; mode=display">||G_j^{\phi}(\hat{x})-G_j^{\phi}(x)||_F^2</script><p> with <script type="math/tex">G_j^{\phi}(x)_{c,c'}=\frac{1}{C_j H_j W_j}\sum_{h=1}^{H_j}\sum_{w=1}^{W_j}\phi_j(x)_{h,w,c}\phi_j(x)_{h,w,c'}</script></p>
</li>
<li><p>pairwise mean squared error (PMSE) <a href="https://papers.nips.cc/paper/5539-depth-map-prediction-from-a-single-image-using-a-multi-scale-deep-network.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://arxiv.org/pdf/1612.05424.pdf" target="_blank" rel="noopener">[4]</a>: scale-invariant mean squared error (in log space) </p>
<script type="math/tex; mode=display">\frac{1}{n}\sum_i d_i^2 - \frac{1}{n^2}(\sum_i d_i)^2</script></li>
<li><p>total variation (TV) loss <a href="https://arxiv.org/pdf/1603.08155.pdf" target="_blank" rel="noopener">[1]</a>: smoothness</p>
<script type="math/tex; mode=display">\sum_{(i,j)} ||x_{i,j+1}-x_{i,j}||_1 +||x_{i+1,j}-x_{i,j}||_1</script></li>
<li><p>alignment loss <a href="https://arxiv.org/pdf/1812.06145.pdf" target="_blank" rel="noopener">[5]</a>: two images have similar spatial correlation, complementary to style loss</p>
<script type="math/tex; mode=display">||F_j^{\phi}(\hat{x})-F_j^{\phi}(x)||_F^2</script><p> with <script type="math/tex">F_j^{\phi}(x)_{d,d'}=\frac{1}{C_j H_j W_j}\sum_{c=1}^{C}\phi_j(x)_{d,c}\phi_j(x)_{d',c}</script></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. “Perceptual losses for real-time style transfer and super-resolution.” ECCV, 2016.</p>
<p>[2] Gatys, Leon, Alexander S. Ecker, and Matthias Bethge. “Texture synthesis using convolutional neural networks.” NIPS, 2015.</p>
<p>[3] Eigen, David, Christian Puhrsch, and Rob Fergus. “Depth map prediction from a single image using a multi-scale deep network.” NIPS, 2014.</p>
<p>[4] Bousmalis, Konstantinos, et al. “Unsupervised pixel-level domain adaptation with generative adversarial networks.” CVPR, 2017.</p>
<p>[5] Abavisani, Mahdi, Hamid Reza Vaezi Joze, and Vishal M. Patel. “Improving the performance of unimodal dynamic hand-gesture recognition with multimodal training.” CVPR, 2019.</p>
<p>[6] Lin, Tsung-Yu, Aruni RoyChowdhury, and Subhransu Maji. “Bilinear cnn models for fine-grained visual recognition.” ICCV, 2015.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Image Inpainting</title>
    <url>/2022/06/16/image_video_synthesis/Image%20Inpainting/</url>
    <content><![CDATA[<h3 id="Partial-and-Gated-Convolution"><a href="#Partial-and-Gated-Convolution" class="headerlink" title="Partial and Gated Convolution"></a>Partial and Gated Convolution</h3><ul>
<li><p>partial convolution <a href="https://arxiv.org/pdf/1804.07723.pdf" target="_blank" rel="noopener">[1]</a>: hard-gating single-channel unlearnable layer</p>
<p>  <img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/B20xmYt.jpg" width="40%"></p>
</li>
<li><p>gated convolution <a href="https://arxiv.org/pdf/1806.03589.pdf" target="_blank" rel="noopener">[2]</a>: soft-gating multi-channel learnable layer</p>
<p> <img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/GOC8MsV.jpg" width="40%"></p>
<p> <img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/eG7JmBi.jpg" width="70%"></p>
</li>
</ul>
<h3 id="Filling-Priority"><a href="#Filling-Priority" class="headerlink" title="Filling Priority"></a>Filling Priority</h3><p>filling priority <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/criminisi_tip2004.pdf" target="_blank" rel="noopener">[3]</a>: Priority is the product of confidence term (a measure of the amount of reliable information surrounding the pixel) and data term (a function of the strength of isophotes hitting the front). Select the patch to be filled based on the priority, similar to patch-based texture synthesis. </p>
<pre><code>&lt;img src=&quot;http://bcmi.sjtu.edu.cn/~niuli/github_images/bO5YXEQ.jpg&quot; width=&quot;40%&quot;&gt; 
</code></pre><h3 id="Diverse-image-inpainting"><a href="#Diverse-image-inpainting" class="headerlink" title="Diverse image inpainting"></a>Diverse image inpainting</h3><ul>
<li><p>random vector: use random vector to generate diverse and plausible outputs <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Pluralistic_Image_Completion_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[6]</a></p>
</li>
<li><p>attribute vector: use target attribute values to guide image inpainting <a href="https://arxiv.org/abs/1801.07632" target="_blank" rel="noopener">[7]</a></p>
</li>
<li><p>use autoregressive model: <a href="https://arxiv.org/pdf/2103.10022.pdf" target="_blank" rel="noopener">[11]</a> <a href="https://arxiv.org/pdf/2103.14031.pdf" target="_blank" rel="noopener">[12]</a></p>
</li>
</ul>
<h3 id="Auxiliary-Information"><a href="#Auxiliary-Information" class="headerlink" title="Auxiliary Information"></a>Auxiliary Information</h3><ul>
<li><p>Semantics</p>
<ul>
<li>enforce inpainted result to have expected semantics <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Generative_Face_Completion_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[8]</a></li>
<li>first inpaint semantic map and then use complete semantic map as guidance <a href="https://arxiv.org/pdf/1805.03356v3.pdf" target="_blank" rel="noopener">[9]</a></li>
<li>guide feature learning in the decoder <a href="https://arxiv.org/pdf/2003.06877.pdf" target="_blank" rel="noopener">[10]</a></li>
<li>semantic-aware attention <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liao_Image_Inpainting_Guided_by_Coherence_Priors_of_Semantics_and_Textures_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[13]</a></li>
</ul>
</li>
<li><p>Edges</p>
<ul>
<li>Inpaint edge map and use complete edge map to help image inpainting <a href="https://arxiv.org/pdf/1901.00212" target="_blank" rel="noopener">[4]</a> <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Xiong_Foreground-Aware_Image_Inpainting_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[5]</a></li>
</ul>
</li>
</ul>
<h3 id="Frequency-Domain"><a href="#Frequency-Domain" class="headerlink" title="Frequency Domain"></a>Frequency Domain</h3><ul>
<li>using frequency map as network input <a href="https://arxiv.org/pdf/2012.01832.pdf" target="_blank" rel="noopener">[14]</a>  </li>
<li>fourier convolution: LAMA<a href="(https://arxiv.org/pdf/2109.07161.pdf">[15]</a>)</li>
<li>wavelet <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_WaveFill_A_Wavelet-Based_Generation_Network_for_Image_Inpainting_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[16]</a></li>
</ul>
<h3 id="Bridging-Inpainting-and-Generation"><a href="#Bridging-Inpainting-and-Generation" class="headerlink" title="Bridging Inpainting and Generation"></a>Bridging Inpainting and Generation</h3><ul>
<li>Co-Mod <a href="https://openreview.net/pdf?id=sSjqmfsk95O" target="_blank" rel="noopener">[17]</a></li>
</ul>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p><a href="https://arxiv.org/pdf/2103.14031.pdf" target="_blank" rel="noopener">[12]</a> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Bridging_Global_Context_Interactions_for_High-Fidelity_Image_Completion_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[18]</a> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MAT_Mask-Aware_Transformer_for_Large_Hole_Image_Inpainting_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[19]</a></p>
<h3 id="Diffusion-Model"><a href="#Diffusion-Model" class="headerlink" title="Diffusion Model"></a>Diffusion Model</h3><p><a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[20]</a>  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[21]</a> <a href="https://arxiv.org/pdf/2212.02963.pdf" target="_blank" rel="noopener">[22]</a> <a href="https://arxiv.org/pdf/2212.06909.pdf" target="_blank" rel="noopener">[23]</a></p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol>
<li>Liu, Guilin, et al. “Image inpainting for irregular holes using partial convolutions.” ECCV, 2018.</li>
<li>Yu, Jiahui, et al. “Free-form image inpainting with gated convolution.” ICCV, 2019.</li>
<li>Criminisi, Antonio, Patrick Pérez, and Kentaro Toyama. “Region filling and object removal by exemplar-based image inpainting.” TIP, 2004.</li>
<li>Nazeri, Kamyar, et al. “Edgeconnect: Generative image inpainting with adversarial edge learning.” arXiv preprint arXiv:1901.00212 (2019).</li>
<li>Xiong, Wei, et al. “Foreground-aware image inpainting.” CVPR, 2019.</li>
<li>Zheng, Chuanxia, Tat-Jen Cham, and Jianfei Cai. “Pluralistic image completion.” CVPR, 2019.</li>
<li>Chen, Zeyuan, et al. “High resolution face completion with multiple controllable attributes via fully end-to-end progressive generative adversarial networks.” arXiv preprint arXiv:1801.07632 (2018).</li>
<li>Li, Yijun, et al. “Generative face completion.” CVPR, 2017.</li>
<li>Song, Yuhang, et al. “Spg-net: Segmentation prediction and guidance network for image inpainting.” arXiv preprint arXiv:1805.03356 (2018).</li>
<li>Liao, Liang, et al. “Guidance and evaluation: Semantic-aware image inpainting for mixed scenes.” arXiv preprint arXiv:2003.06877 (2020).</li>
<li>Peng, Jialun, et al. “Generating Diverse Structure for Image Inpainting With Hierarchical VQ-VAE.” CVPR, 2021.</li>
<li>Wan, Ziyu, et al. “High-Fidelity Pluralistic Image Completion with Transformers.” arXiv preprint arXiv:2103.14031 (2021).</li>
<li>Liao, Liang, et al. “Image inpainting guided by coherence priors of semantics and textures.” CVPR, 2021.</li>
<li>Roy, Hiya, et al. “Image inpainting using frequency domain priors.” arXiv preprint arXiv:2012.01832 (2020).</li>
<li>Suvorov, Roman, et al. “Resolution-robust Large Mask Inpainting with Fourier Convolutions.” WACV (2021).</li>
<li>Yu, Yingchen, et al. “WaveFill: A Wavelet-based Generation Network for Image Inpainting.” ICCV, 2021.</li>
<li>Zhao, Shengyu, et al. “Large scale image completion via co-modulated generative adversarial networks.” ICLR (2021).</li>
<li>Zheng, Chuanxia, et al. “Bridging global context interactions for high-fidelity image completion.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</li>
<li>Li, Wenbo, et al. “Mat: Mask-aware transformer for large hole image inpainting.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.</li>
<li>Lugmayr, Andreas, et al. “Repaint: Inpainting using denoising diffusion probabilistic models.” CVPR, 2022.</li>
<li>Rombach, Robin, et al. “High-resolution image synthesis with latent diffusion models.” CVPR, 2022.</li>
<li>Li, Wenbo, et al. “SDM: Spatial Diffusion Model for Large Hole Image Inpainting.” arXiv preprint arXiv:2212.02963 (2022).</li>
<li>Wang, Su, et al. “Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting.” arXiv preprint arXiv:2212.06909 (2022).</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>inpainting</tag>
      </tags>
  </entry>
  <entry>
    <title>Image Harmonization</title>
    <url>/2022/06/16/image_video_synthesis/Image%20Harmonization/</url>
    <content><![CDATA[<h2 id="Fundamental"><a href="#Fundamental" class="headerlink" title="Fundamental"></a>Fundamental</h2><p><u>Image Statistics</u>: illuminance, color temperature, saturation, local contrast, hue, texture, tone</p>
<p><u>Color spaces</u>: RGB color space, CIELab color space (saturation/chrominance, hue, luminance).</p>
<h2 id="Image-realism"><a href="#Image-realism" class="headerlink" title="Image realism"></a>Image realism</h2><ol>
<li><p>Predict the realism using the discriminator learnt based on real images and fake images <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zhu_Learning_a_Discriminative_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[a]</a></p>
</li>
<li><p>Predict the realism based on global and local statistics: distance to neighboring realistic image, similarity between foreground and background <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.220.2272&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[a]</a></p>
</li>
</ol>
<h2 id="Image-harmonization"><a href="#Image-harmonization" class="headerlink" title="Image harmonization"></a>Image harmonization</h2><p>After pasting the foreground on the background, harmonize the foreground. </p>
<ul>
<li><strong>Traditional methods:</strong> match the foreground with the background; match the foreground with other semantically or statistically close realistic images.<ul>
<li>histogram matching: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5620893" target="_blank" rel="noopener">[a]</a> <a href="https://people.csail.mit.edu/wojciech/Harmonization/Harmonization.pdf" target="_blank" rel="noopener">[b]</a> <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lee_Automatic_Content-Aware_Color_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[c]</a></li>
<li>Gaussian model matching: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lee_Automatic_Content-Aware_Color_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[a]</a> <a href="http://www.cs.northwestern.edu/~bgooch/PDFs/ColorTransfer.pdf" target="_blank" rel="noopener">[b]</a> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.491.4250&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[c]</a></li>
<li>pixel-level classification: MRF<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5620893" target="_blank" rel="noopener">[a]</a></li>
<li>gamma correction <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lee_Automatic_Content-Aware_Color_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[a]</a> <a href="http://www.cs.northwestern.edu/~bgooch/PDFs/ColorTransfer.pdf" target="_blank" rel="noopener">[b]</a> </li>
</ul>
</li>
</ul>
<ul>
<li><strong>Deep learning methods:</strong> <a href="https://github.com/bcmi/Awesome-Image-Harmonization" target="_blank" rel="noopener">https://github.com/bcmi/Awesome-Image-Harmonization</a></li>
</ul>
<p>One interesting problem in image harmonization is whether the decomposition of reflectance and illumination is unique. If we have strong prior knowledge for the object reflectance (e.g., black-and-white zebra), the decomposition may be unique. Or if the object color is complex enough, which is equivalent to adding enough constraints, the decomposition may be unique. Otherwise, if we do not have strong prior knowledge for the object reflectance (e.g., a vase of arbitrary color) and the object color is simple (e.g., a single color), the decomposition is not unique. </p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/source_to_target_color.png" width="50%"></p>
<p>Given a source image and an obtained target image after applying color transfer, we hope to know whether there exists a valid path between source image and target image and whether there exist multiple valid paths between them. </p>
<h2 id="Deep-painterly-harmonization"><a href="#Deep-painterly-harmonization" class="headerlink" title="Deep painterly harmonization"></a>Deep painterly harmonization</h2><ul>
<li>deep painterly harmonization <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13478" target="_blank" rel="noopener">[1]</a> </li>
<li>style harmonization <a href="https://bmvc2019.org/wp-content/uploads/papers/0425-paper.pdf" target="_blank" rel="noopener">[2]</a></li>
<li>image blending <a href="https://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Deep_Image_Blending_WACV_2020_paper.pdf" target="_blank" rel="noopener">[3]</a></li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Luan, Fujun, et al. “Deep painterly harmonization.” Computer graphics forum. Vol. 37. No. 4. 2018.</p>
<p>[2] Peng, Hwai-Jin, Chia-Ming Wang, and Yu-Chiang Frank Wang. “Element-Embedded Style Transfer Networks for Style Harmonization.” BMVC. 2019.</p>
<p>[3] Zhang, Lingzhi, Tarmily Wen, and Jianbo Shi. “Deep image blending.” WACV. 2020.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>harmonization</tag>
      </tags>
  </entry>
  <entry>
    <title>Image Composition</title>
    <url>/2022/06/16/image_video_synthesis/Image%20Composition/</url>
    <content><![CDATA[<p>Simply speaking, image composition means cut-and-paste, that is, cutting one piece from one image and paste it on another image. The obtained composite image may be unrealistic due to the following reasons:</p>
<ul>
<li>The foreground is not well segmented, so there is an evident and unnatural boundary between foreground and background. </li>
<li>The foreground and background may look incompatible due to different color and illumination statistics. For example, the foreground is captured in the daytime while the background is captured at night.</li>
<li>The foreground is placed at an unreasonable location. For example, a horse is placed in the sky. </li>
<li>The foreground needs to be geometrically transformed. For example, when pasting eye glasses on a face, the eye glasses should fit the eyes and ears on the face.</li>
<li>The pasted foreground may also affect the background. For example, the foreground may cast a shadow on the background.</li>
</ul>
<p>Therefore, image composition is actually a combination of multiple subtasks.<br>Previously, some works only focus on one subtask such as harmonization or geometric transformation <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0380.pdf" target="_blank" rel="noopener">[1]</a>. Some other works attempt to solve all subtasks in a single package <a href="https://arxiv.org/pdf/1706.01021.pdf" target="_blank" rel="noopener">[2]</a> <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Toward_Realistic_Image_Compositing_With_Adversarial_Learning_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://arxiv.org/pdf/1910.11495.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Weng_MISC_Multi-Condition_Injection_and_Spatially-Adaptive_Compositing_for_Conditional_Person_Image_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[5]</a> <a href="https://arxiv.org/pdf/2009.08255.pdf" target="_blank" rel="noopener">[6]</a>.</p>
<p>Human matting+composition: <a href="https://arxiv.org/pdf/2011.02146.pdf" target="_blank" rel="noopener">[7]</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Lin, Chen-Hsuan, et al. “St-gan: Spatial transformer generative adversarial networks for image compositing.”, CVPR, 2018.</p>
<p>[2] Tan, Fuwen, et al. “Where and who? automatic semantic-aware person composition.” WACV, 2018.</p>
<p>[3] Chen, Bor-Chun, and Andrew Kae. “Toward Realistic Image Compositing with Adversarial Learning.” CVPR, 2019.</p>
<p>[4] Lingzhi Zhang, Tarmily Wen, Jianbo Shi: Deep Image Blending. WACV 2020: 231-240</p>
<p>[5] Weng, Shuchen, et al. “MISC: Multi-Condition Injection and Spatially-Adaptive Compositing for Conditional Person Image Synthesis.” CVPR, 2020.</p>
<p>[6] Zhan, Fangneng, et al. “Adversarial Image Composition with Auxiliary Illumination.” arXiv preprint arXiv:2009.08255 (2020).</p>
<p>[7] Zhang, He, et al. “Deep Image Compositing.” arXiv preprint arXiv:2011.02146 (2020).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>composition</tag>
      </tags>
  </entry>
  <entry>
    <title>Image Blending</title>
    <url>/2022/06/16/image_video_synthesis/Image%20Blending/</url>
    <content><![CDATA[<p>The target is to cut the foreground from one image and paste it on another image, followed by adjusting the foreground. The prevalent technique <a href="http://eric-yuan.me/poisson-blending/" target="_blank" rel="noopener">Poisson blending</a> <a href="https://www.cs.jhu.edu/~misha/Fall07/Papers/Perez03.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://people.csail.mit.edu/sparis/publi/2013/ijcv/Tao_IP_Error-tolerant_Image_Compositing.pdf" target="_blank" rel="noopener">[2]</a>, also called seamless cloning, is matching the gradient with boundary conditions via solving Poisson equation. In image harmonization, the original image containing the foreground may be unavailable.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>blending</tag>
      </tags>
  </entry>
  <entry>
    <title>Image Action Recognition with Unlabeled Videos</title>
    <url>/2022/06/16/deep_learning/Image%20Action%20Recognition%20with%20Unlabeled%20Videos/</url>
    <content><![CDATA[<ol>
<li><p>Self-supervised learning: see video-to-image in this <a href="https://ustcnewly.github.io/2018/09/07/deep_learning/Self-supervised%20Learning/">blog</a>. </p>
</li>
<li><p>predict optical flow and use two-stream network <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Im2Flow_Motion_Hallucination_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[1]</a> </p>
</li>
<li><p>Predicting pose information (use poselet detector) <a href="http://openaccess.thecvf.com/content_cvpr_2013/papers/Chen_Watching_Unlabeled_Video_2013_CVPR_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h3><p>[1] Gao, Ruohan, Bo Xiong, and Kristen Grauman. “Im2flow: Motion hallucination from static images for action recognition.” CVPR, 2018.</p>
<p>[2] Chen, Chao-Yeh, and Kristen Grauman. “Watching unlabeled video helps learn new human actions from very few labeled snapshots.” CVPR, 2013.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>action recognition</tag>
      </tags>
  </entry>
  <entry>
    <title>High-resolution Image Generation</title>
    <url>/2022/06/16/image_video_synthesis/High-resolution%20Image%20Generation/</url>
    <content><![CDATA[<ul>
<li><p>stacked generators from low-resolution to high-resolution: <a href="http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[5]</a> <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_StackGAN_Text_to_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[6]</a> <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Photographic_Image_Synthesis_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[10]</a></p>
</li>
<li><p>low-resolution generator embedded in high-resolution generator, upsample low-resolution result and add residual:  <a href="https://arxiv.org/pdf/1711.11585.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1907.12296" target="_blank" rel="noopener">[7]</a> <a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Hamada_Full-body_High-resolution_Anime_Generation_with_Progressive_Structure-conditional_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf" target="_blank" rel="noopener">[8]</a> <a href="https://arxiv.org/pdf/1710.10196.pdf?__hstc=200028081.1bb630f9cde2cb5f07430159d50a3c91.1524009600081.1524009600082.1524009600083.1&amp;__hssc=200028081.1.1524009600084&amp;__hsfp=1773666937" target="_blank" rel="noopener">[9]</a> <a href="https://arxiv.org/pdf/2005.09704.pdf" target="_blank" rel="noopener">[12]</a></p>
</li>
<li><p>fuse low-resolution outputs: <a href="https://arxiv.org/pdf/2009.06613.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://arxiv.org/pdf/2003.08791.pdf" target="_blank" rel="noopener">[11]</a></p>
</li>
<li><p>shallow mapping from large-scale input to large-scale output: <a href="https://www4.comp.polyu.edu.hk/~cslzhang/paper/PAMI_LUT.pdf" target="_blank" rel="noopener">[2]</a>(look-up table) <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cong_High-Resolution_Image_Harmonization_via_Collaborative_Dual_Transformations_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[15]</a> <a href="https://arxiv.org/pdf/2109.05750.pdf" target="_blank" rel="noopener">[16]</a></p>
</li>
<li><p>joint upsampling: given high-resolution input and low-resolution output, get high-resolution output. 1) append high-resolution input <a href="https://arxiv.org/pdf/1711.11585.pdf" target="_blank" rel="noopener">[1]</a>  or the feature of high-resolution input <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Photographic_Image_Synthesis_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[10]</a> to refinement network. 2) guided filter <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Fast_End-to-End_Trainable_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[13]</a>, use high-resolution input as guidance and coarse high-resolution output as filter input. 3) attentional upsampling <a href="https://arxiv.org/pdf/2012.09904" target="_blank" rel="noopener">[14]</a></p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Wang, Ting-Chun, et al. “High-resolution image synthesis and semantic manipulation with conditional gans.” CVPR, 2018.</p>
<p>[2] Zeng, Hui, et al. “Learning Image-adaptive 3D Lookup Tables for High Performance Photo Enhancement in Real-time.” PAMI, 2020.</p>
<p>[3] Yu, Haichao, et al. “High-Resolution Deep Image Matting.” arXiv preprint arXiv:2009.06613 (2020).</p>
<p>[4] Denton, Emily L., Soumith Chintala, and Rob Fergus. “Deep generative image models using a￼ laplacian pyramid of adversarial networks.” NIPS, 2015.</p>
<p>[5] Huang, Xun, et al. “Stacked generative adversarial networks.” CVPR, 2017.</p>
<p>[6] Zhang, Han, et al. “Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks.” ICCV, 2017.</p>
<p>[7] Andreini, Paolo, et al. “A two stage gan for high resolution retinal image generation and segmentation.” arXiv preprint arXiv:1907.12296 (2019).</p>
<p>[8] Hamada, K., Tachibana, K., Li, T., Honda, H., &amp; Uchida, Y. (2018). Full-body high-resolution anime generation with progressive structure-conditional generative adversarial networks. ECCV, 2018.</p>
<p>[9] Karras, Tero, et al. “Progressive growing of gans for improved quality, stability, and variation.” arXiv preprint arXiv:1710.10196 (2017).</p>
<p>[10] Chen, Qifeng, and Vladlen Koltun. “Photographic image synthesis with cascaded refinement networks.” ICCV, 2017.</p>
<p>[11] Anokhin, Ivan, et al. “High-Resolution Daytime Translation Without Domain Labels.” CVPR, 2020.</p>
<p>[12] Yi, Zili, et al. “Contextual residual aggregation for ultra high-resolution image inpainting.” CVPR, 2020.</p>
<p>[13] Wu, Huikai, et al. “Fast end-to-end trainable guided filter.” CVPR, 2018.</p>
<p>[14] Kundu, Souvik, et al. “Attention-based Image Upsampling.” arXiv preprint arXiv:2012.09904 (2020).</p>
<p>[15] Cong, Wenyan, et al. “High-Resolution Image Harmonization via Collaborative Dual Transformations.” CVPR, 2022.</p>
<p>[16] Liang, Jingtang, Xiaodong Cun, and Chi-Man Pun. “Spatial-Separated Curve Rendering Network for Efficient and High-Resolution Image Harmonization.” ECCV, 2022.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>image generation</tag>
      </tags>
  </entry>
  <entry>
    <title>Gradient Regularization</title>
    <url>/2022/06/16/deep_learning/Gradient%20Regularization/</url>
    <content><![CDATA[<ul>
<li>Gradient harmonization: <a href="https://arxiv.org/pdf/1811.05181.pdf" target="_blank" rel="noopener">[1]</a></li>
</ul>
<p>[1] Gradient Harmonized Single-stage Detector, AAAI, 2019</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>GNN for Segmentation</title>
    <url>/2022/06/16/classification_detection_segmentation/GNN%20for%20Segmentation/</url>
    <content><![CDATA[<ol>
<li>row-wise and column-wise LSTM on feature map: <a href="https://arxiv.org/pdf/1604.05000.pdf" target="_blank" rel="noopener">[1]</a> </li>
<li>graph LSTM on superpixels: <a href="https://arxiv.org/pdf/1603.07063.pdf" target="_blank" rel="noopener">[2]</a></li>
<li>3D graph: <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Qi_3D_Graph_Neural_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[3]</a></li>
<li>DAG on feature map: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_Boundary-Aware_Feature_Propagation_for_Scene_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[4]</a></li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>Li, Zhen, et al. “Lstm-cf: Unifying context modeling and fusion with lstms for rgb-d scene labeling.” ECCV, 2016.</li>
<li>Liang, Xiaodan, et al. “Semantic object parsing with graph lstm.” ECCV, 2016.</li>
<li>Qi, Xiaojuan, et al. “3d graph neural networks for rgbd semantic segmentation. ICCV, 2017.</li>
<li>Ding, Henghui, et al. “Boundary-aware feature propagation for scene segmentation.” ICCV, 2019.</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>semantic segmentation</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Geometry-aware Deep Feature</title>
    <url>/2022/06/16/deep_learning/Geometry-aware%20Deep%20Feature/</url>
    <content><![CDATA[<ol>
<li><p>Geometry feature generation based on unsupervisely detected landmarks. <a href="https://arxiv.org/pdf/1904.09571.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>Disentangle bottleneck features into category-invariant features and category-specific features. Category-invariant features encode the pose information. </p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>Wayne Wu, Kaidi Cao, Cheng Li, Chen Qian, Chen Change Loy: TransGaGa: Geometry-Aware Unsupervised Image-To-Image Translation. CVPR 2019</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Geometry Transformation for Image Composition</title>
    <url>/2022/06/16/image_video_synthesis/Geometry%20Transformation%20for%20Image%20Composition/</url>
    <content><![CDATA[<ul>
<li><p>Only geometry: <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[1]</a>, <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/CVFAD/Kikuchi_Regularized_Adversarial_Training_for_Single-Shot_Virtual_Try-On_ICCVW_2019_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>geometry+appearance <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhan_Spatial_Fusion_GAN_for_Image_Synthesis_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
<li><p>geometry+occlusion+appearance: <a href="https://arxiv.org/pdf/1807.07560" target="_blank" rel="noopener">[4]</a> <a href="https://arxiv.org/pdf/1905.04693.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><p>Lin, Chen-Hsuan, et al. “St-gan: Spatial transformer generative adversarial networks for image compositing.” CVPR, 2018.</p>
</li>
<li><p>Kikuchi, Kotaro, et al. “Regularized Adversarial Training for Single-shot Virtual Try-On.” ICCV Workshops. 2019.</p>
</li>
<li><p>Zhan, Fangneng, Hongyuan Zhu, and Shijian Lu. “Spatial fusion gan for image synthesis.” CVPR, 2019.</p>
</li>
<li><p>Azadi, Samaneh, et al. “Compositional gan: Learning image-conditional binary composition.” International Journal of Computer Vision 128.10 (2020): 2570-2585.</p>
</li>
<li><p>Fangneng Zhan, Jiaxing Huang, Shijian Lu, “Hierarchy Composition GAN for High-fidelity<br>Image Synthesis.” Transactions on cybernetics, 2021. </p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Generative Model</title>
    <url>/2022/06/16/image_video_synthesis/Generative%20Model/</url>
    <content><![CDATA[<ul>
<li><a href="https://ustcnewly.github.io/2018/09/07/deep_learning/GAN/">GAN</a></li>
<li><a href="https://ustcnewly.github.io/2021/10/08/deep_learning/VAE/">VAE</a></li>
<li>diffusion model: <a href="https://arxiv.org/pdf/2105.05233.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/2112.10741v1.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://arxiv.org/pdf/2206.00364.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://arxiv.org/pdf/2205.12952.pdf" target="_blank" rel="noopener">[4]</a></li>
</ul>
<p>Tutorial of generative models:</p>
<ul>
<li><p><a href="https://arxiv.org/pdf/2208.11970.pdf" target="_blank" rel="noopener">Understanding Diffusion Models: A Unified Perspective</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.02606" target="_blank" rel="noopener">Unifying Generative Models with GFlowNets</a></p>
</li>
</ul>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Dhariwal, Prafulla, and Alex Nichol. “Diffusion models beat gans on image synthesis.” arXiv preprint arXiv:2105.05233 (2021).</p>
<p>[2] GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models </p>
<p>[3] Elucidating the Design Space of Diffusion-Based Generative Models</p>
<p>[4] Wang, Tengfei, et al. “Pretraining is All You Need for Image-to-Image Translation.” arXiv preprint arXiv:2205.12952 (2022).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>Gaze Estimation</title>
    <url>/2022/06/16/deep_learning/Gaze%20Estimation/</url>
    <content><![CDATA[<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/VDw1UW6.jpg" width="50%" height="50%"></p>
<h3 id="Approaches"><a href="#Approaches" class="headerlink" title="Approaches"></a>Approaches</h3><ol>
<li><p>Corneal reflection-based methods</p>
<ul>
<li>NIR or LED illumination, learning the mapping (e.g., regression, ) between glint vector and gaze direction.</li>
</ul>
</li>
<li><p>Appearance based methods</p>
<ul>
<li>Limbus model <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.486.489&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[pdf]</a>: fit a limbus model (a fixed-diameter disc) to detected iris edges.</li>
</ul>
</li>
</ol>
<h3 id="Auxiliary-Tools"><a href="#Auxiliary-Tools" class="headerlink" title="Auxiliary Tools"></a>Auxiliary Tools</h3><ol>
<li><p>Calibration: obtain the visual axis and kappa angle for each person.</p>
</li>
<li><p>Facial landmarks detection</p>
<ul>
<li>One Millisecond Face Alignment with an Ensemble of Regression Trees <a href="http://openaccess.thecvf.com/content_cvpr_2014/papers/Kazemi_One_Millisecond_Face_2014_CVPR_paper.pdf" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/cheind/dest" target="_blank" rel="noopener">[code]</a></li>
<li>Continuous Conditional Neural Fields for Structured Regression <a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=1133&amp;context=lti" target="_blank" rel="noopener">[pdf]</a></li>
</ul>
</li>
<li><p>Head Pose Estimation</p>
<ul>
<li>EPnP algorithm <a href="https://infoscience.epfl.ch/record/160138/files/top.pdf" target="_blank" rel="noopener">[pdf]</a></li>
</ul>
</li>
</ol>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><ol>
<li><p><a href="https://www.mpi-inf.mpg.de/de/abteilungen/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild-mpiigaze/" target="_blank" rel="noopener">[MPIIGaze]</a>: fine-grained annotation</p>
</li>
<li><p><a href="https://www.idiap.ch/dataset/eyediap" target="_blank" rel="noopener">[Eyediap]</a>: RGB-D </p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>gaze estimation</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN</title>
    <url>/2022/06/16/image_video_synthesis/GAN/</url>
    <content><![CDATA[<h3 id="Training-tricks"><a href="#Training-tricks" class="headerlink" title="Training tricks:"></a>Training tricks:</h3><p>17 tricks for training GAN: <a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">https://github.com/soumith/ganhacks</a></p>
<ul>
<li><p>soft label: replace 1 with 0.9 and 0 with 0.3</p>
</li>
<li><p>train discriminator more times (e.g., 2X) than generator</p>
</li>
<li><p>use labels: auxiliary tasks</p>
</li>
<li><p>normalize inputs to [-1, 1]</p>
</li>
<li><p>use tanh before output</p>
</li>
<li><p>use batchnorm (not for the first and last layer)</p>
</li>
<li><p>use spherical distribution instead of uniform distribution</p>
</li>
<li><p>leaky relu</p>
</li>
<li><p>stability tricks from RL</p>
</li>
</ul>
<p>Tricks from the BigGAN <a href="https://arxiv.org/pdf/1809.11096.pdf" target="_blank" rel="noopener">[1]</a> </p>
<ul>
<li><p>class-conditional BatchNorm</p>
</li>
<li><p>Spectral normalization</p>
</li>
<li><p>orthogonal initialization</p>
</li>
<li><p>truncated prior (truncation trick to seek the trade-off between fidelity and variety)</p>
</li>
<li><p>enforce orthogonality on weights to improve the model smoothness</p>
</li>
</ul>
<p>More tricks</p>
<ul>
<li><p>gradient penalty <a href="https://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.pdf" target="_blank" rel="noopener">[8]</a></p>
</li>
<li><p>unrolling <a href="https://arxiv.org/pdf/1611.02163.pdf" target="_blank" rel="noopener">[9]</a> and packing <a href="https://papers.nips.cc/paper/7423-pacgan-the-power-of-two-samples-in-generative-adversarial-networks.pdf" target="_blank" rel="noopener">[10]</a></p>
</li>
</ul>
<h3 id="Famous-GANs"><a href="#Famous-GANs" class="headerlink" title="Famous GANs:"></a>Famous GANs:</h3><ul>
<li><p>LSGAN: replace cross-entropy loss with least square loss</p>
</li>
<li><p>Wasserstein GAN: replace discriminator with a critic function</p>
</li>
<li><p>LAPGAN: coarse-to-fine using laplacian pyramid</p>
</li>
<li><p>seqGAN: generate discrete sequences</p>
</li>
<li><p>E-GAN <a href="https://arxiv.org/pdf/1803.00657.pdf" target="_blank" rel="noopener">[2]</a>: place GAN under the framework of genetic evolution</p>
</li>
<li><p>Dissection GAN <a href="https://arxiv.org/pdf/1811.10597.pdf" target="_blank" rel="noopener">[3]</a>: use intervention for causality</p>
</li>
<li><p>CoGAN <a href="https://papers.nips.cc/paper/6544-coupled-generative-adversarial-networks.pdf" target="_blank" rel="noopener">[4]</a>: two generators and discriminators softly share parameters</p>
</li>
<li><p>DCGAN <a href="https://arxiv.org/pdf/1511.06434.pdf%C3%AF%C2%BC%E2%80%B0" target="_blank" rel="noopener">[5]</a></p>
</li>
<li><p>Progressive GAN  <a href="https://arxiv.org/pdf/1710.10196.pdf?__hstc=200028081.1bb630f9cde2cb5f07430159d50a3c91.1524009600081.1524009600082.1524009600083.1&amp;__hssc=200028081.1.1524009600084&amp;__hsfp=1773666937" target="_blank" rel="noopener">[6]</a></p>
</li>
<li><p>Style-based GAN <a href="https://arxiv.org/pdf/1812.04948.pdf?utm_campaign=nathan.ai%20newsletter&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank" rel="noopener">[7]</a></p>
</li>
<li><p>stack GAN <a href="https://arxiv.org/pdf/1612.03242.pdf" target="_blank" rel="noopener">[17]</a></p>
</li>
<li><p>self-attention GAN <a href="https://arxiv.org/pdf/1805.08318.pdf" target="_blank" rel="noopener">[18]</a></p>
</li>
<li><p>BigGAN <a href="https://arxiv.org/pdf/1809.11096.pdf?utm_campaign=Dynamically%20Typed&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank" rel="noopener">[20]</a></p>
</li>
<li><p>LoGAN <a href="https://arxiv.org/pdf/1912.00953.pdf" target="_blank" rel="noopener">[19]</a></p>
</li>
<li><p>Conditioned on label vector: conditional GAN <a href="https://arxiv.org/pdf/1411.1784.pdf" target="_blank" rel="noopener">[14]</a>, CVAE-GAN <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[16]</a></p>
</li>
<li><p>Conditioned on a single image: pix2pix <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[11]</a>; high-resolution pix2pix <a href="https://arxiv.org/pdf/1711.11585.pdf" target="_blank" rel="noopener">[12]</a> (add coarse-to-fine strategy); BicycleGAN <a href="https://papers.nips.cc/paper/6650-toward-multimodal-image-to-image-translation.pdf" target="_blank" rel="noopener">[13]</a> (combination of cVAE-GAN and cLR-GAN); DAGAN <a href="https://arxiv.org/pdf/1711.04340" target="_blank" rel="noopener">[15]</a></p>
</li>
<li><p>StyleGAN-XL <a href="https://dl.acm.org/doi/pdf/10.1145/3528233.3530738" target="_blank" rel="noopener">[23]</a></p>
</li>
<li><p>StyleGAN-T <a href="https://arxiv.org/pdf/2301.09515.pdf" target="_blank" rel="noopener">[22]</a></p>
</li>
<li><p>GigaGAN  <a href="https://arxiv.org/pdf/2303.05511.pdf" target="_blank" rel="noopener">[21]</a></p>
</li>
</ul>
<h3 id="Measurement："><a href="#Measurement：" class="headerlink" title="Measurement："></a>Measurement：</h3><p><u>Results</u>: Besides qualitative results, there are some quantitative metric like Inception score and Frechet Inception Distance.</p>
<p><u>Stability</u>: for the stability of generator and discriminator, refer to <a href="https://arxiv.org/pdf/1809.11096.pdf" target="_blank" rel="noopener">[1]</a>.</p>
<h3 id="Tutorial-and-Survey"><a href="#Tutorial-and-Survey" class="headerlink" title="Tutorial and Survey:"></a>Tutorial and Survey:</h3><ul>
<li><p>The GAN zoo: <a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="noopener">https://github.com/hindupuravinash/the-gan-zoo</a></p>
</li>
<li><p>A good tutorial: <a href="https://github.com/mingyuliutw/cvpr2017_gan_tutorial/blob/master/gan_tutorial.pdf" target="_blank" rel="noopener">https://github.com/mingyuliutw/cvpr2017\_gan\_tutorial/blob/master/gan_tutorial.pdf</a></p>
</li>
<li><p><a href="https://arxiv.org/ftp/arxiv/papers/2005/2005.09165.pdf" target="_blank" rel="noopener">Regularization Methods for Generative Adversarial Networks: An Overview of Recent Studies</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1906.01529.pdf" target="_blank" rel="noopener">Generative adversarial networks in computer vision: A survey and taxonomy</a> <a href="https://github.com/sheqi/GAN_Review" target="_blank" rel="noopener">[code]</a></p>
</li>
</ul>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Brock A, Donahue J, Simonyan K. Large scale gan training for high fidelity natural image synthesis[J]. arXiv preprint arXiv:1809.11096, 2018.</p>
<p>[2] Wang C, Xu C, Yao X, et al. Evolutionary Generative Adversarial Networks[J]. arXiv preprint arXiv:1803.00657, 2018.</p>
<p>[3] Bau D, Zhu J Y, Strobelt H, et al. GAN Dissection: Visualizing and Understanding Generative Adversarial Networks[J]. arXiv preprint arXiv:1811.10597, 2018.</p>
<p>[4] Liu M Y, Tuzel O. Coupled generative adversarial networks[C]//Advances in neural information processing systems. 2016: 469-477.</p>
<p>[5] Radford, Alec, Luke Metz, and Soumith Chintala. “Unsupervised representation learning with deep convolutional generative adversarial networks.” arXiv preprint arXiv:1511.06434 (2015).</p>
<p>[6] Karras, Tero, et al. “Progressive growing of gans for improved quality, stability, and variation.” arXiv preprint arXiv:1710.10196 (2017).</p>
<p>[7] Karras, Tero, Samuli Laine, and Timo Aila. “A Style-Based Generator Architecture for Generative Adversarial Networks.” arXiv preprint arXiv:1812.04948 (2018).</p>
<p>[8] Gulrajani, Ishaan, et al. “Improved training of wasserstein gans.” Advances in Neural Information Processing Systems. 2017.</p>
<p>[9] Metz, Luke, et al. “Unrolled generative adversarial networks.” arXiv preprint arXiv:1611.02163 (2016).</p>
<p>[10] Lin, Zinan, et al. “PacGAN: The power of two samples in generative adversarial networks.” Advances in Neural Information Processing Systems. 2018.</p>
<p>[11] Isola, Phillip, et al. “Image-to-image translation with conditional adversarial networks.” CVPR, 2017</p>
<p>[12] Wang, Ting-Chun, et al. “High-resolution image synthesis and semantic manipulation with conditional gans.” CVPR, 2018.</p>
<p>[13] Zhu, Jun-Yan, et al. “Toward multimodal image-to-image translation.” NIPS, 2017.</p>
<p>[14] Mirza, Mehdi, and Simon Osindero. “Conditional generative adversarial nets.” arXiv preprint arXiv:1411.1784 (2014).</p>
<p>[15] Antoniou, Antreas, Amos Storkey, and Harrison Edwards. “Data augmentation generative adversarial networks.” arXiv preprint arXiv:1711.04340 (2017).</p>
<p>[16] Bao, Jianmin, et al. “CVAE-GAN: fine-grained image generation through asymmetric training.” ICCV, 2017.</p>
<p>[17] Han Zhang, Tao Xu, Hongsheng Li, “StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks”, ICCV 2017</p>
<p>[18] Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, Augustus Odena, “Self-Attention Generative Adversarial Networks”. CoRR abs/1805.08318 (2018)</p>
<p>[19] Wu, Yan, et al. “LOGAN: Latent Optimisation for Generative Adversarial Networks.” arXiv preprint arXiv:1912.00953 (2019).</p>
<p>[20] Brock, Andrew, Jeff Donahue, and Karen Simonyan. “Large scale gan training for high fidelity natural image synthesis.” arXiv preprint arXiv:1809.11096 (2018).</p>
<p>[21] Kang, Minguk, et al. “Scaling up GANs for Text-to-Image Synthesis.” arXiv preprint arXiv:2303.05511 (2023).</p>
<p>[22] Sauer, Axel, et al. “Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis.” arXiv preprint arXiv:2301.09515 (2023).</p>
<p>[24] Sauer, Axel, Katja Schwarz, and Andreas Geiger. “Stylegan-xl: Scaling stylegan to large diverse datasets.” ACM SIGGRAPH 2022 conference proceedings. 2022.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>From Weak to Strong Supervision</title>
    <url>/2022/06/16/deep_learning/From%20Weak%20to%20Strong%20Supervision/</url>
    <content><![CDATA[<p><strong>Object Detection:</strong></p>
<ol>
<li><p>image label: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bilen_Weakly_Supervised_Deep_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[WSDDN]</a></p>
</li>
<li><p>points that indicate the location of the object</p>
</li>
<li><p>bounding boxes</p>
</li>
</ol>
<p><strong>Segmentation:</strong></p>
<ol>
<li><p>image label: <a href="https://arxiv.org/pdf/1603.06098.pdf]" target="_blank" rel="noopener">[SEC]</a></p>
</li>
<li><p>points that indicate the location of the object</p>
</li>
<li><p>scribbles that imply the extent of the object</p>
</li>
<li><p>bounding boxes</p>
</li>
<li><p>segmentation masks</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>weakly-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title>From Anchor to ROI</title>
    <url>/2022/06/16/classification_detection_segmentation/From%20Anchor%20to%20ROI/</url>
    <content><![CDATA[<h2 id="layer-area"><a href="#layer-area" class="headerlink" title="layer area"></a>layer area</h2><p>From layer i to layer i+1, assume the parameters on layer i are $s_i$ (stride), $p_i$ (patch), $k_i$ (kernel filter size), the width or height of layer i are $r_i$. Then, based on common sense, </p>
<script type="math/tex; mode=display">r_{i+1} = (r_i+2p_i-k_i)/s_i+1.</script><p>In the reverse process, $r_i = s_i r_{i+1}-s_i-2p_i+k_i$ or $r_i = s_i r_{i+1}-s_i+k_i$ if counting in padding area.</p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/5oklFvo.png" alt=""></p>
<h2 id="coordinate-map"><a href="#coordinate-map" class="headerlink" title="coordinate map"></a>coordinate map</h2><p>Now consider mapping the point $x_i$ on the ROI to the point $x_{i+1}$ on the feature map, which can be transformed to the layer area problem above. In particular, the receptive field formed by left-up corner and $x_i$ on the ROI can be mapped to the region formed by left-up corner and $x_{i+1}$ on the feature map. Based on the similar formula for the layer area problem above (note the only difference is that we only include left padding and up padding, and subtract the radius of kernel filter $(k_i-1)/2$,</p>
<script type="math/tex; mode=display">x_i=s_i x_{i+1}-s_i-p_i+k_i-(k_i-1)/2.</script><p>The above coordinate system starts from 1. When the coordinate system starts from 0, </p>
<script type="math/tex; mode=display">x_i+1=s_i (x_{i+1}+1)-s_i-p_i+k_i-(k_i-1)/2,</script><p>which can be simplified as</p>
<script type="math/tex; mode=display">x_i=s_i x_{i+1}+(\frac{k_i-1}{2}-p_i).</script><p>when $p_i=floor(k_i/2)$, $x_i=s_i x_{i+1}$ approximately, which is the simplest case.</p>
<p>By applying $x_i=s_i x_{i+1}+(\frac{k_i-1}{2}-p_i)$ recursively, we can achieve a general solution</p>
<script type="math/tex; mode=display">x_1 = \alpha_L x_{L}+\beta_L,</script><p>in which $\alpha_L = \prod_{l=1}^{L-1} s_l$ and $\beta_L=\sum_{l=1}^{L-1} (\prod_{n=1}^{l-1} s_n)(\frac{k_l-1}{2}-p_l) $ </p>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/acLbSez.png" alt=""></p>
<h2 id="anchor-box-to-ROI"><a href="#anchor-box-to-ROI" class="headerlink" title="anchor box to ROI"></a>anchor box to ROI</h2><p>Given two corner points of an anchor box on the feature map, we can find their corresponding points on the original image, which determine the ROI.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Frequency Domain</title>
    <url>/2022/06/16/deep_learning/Frequency%20Domain/</url>
    <content><![CDATA[<ol>
<li><p>Distinguish generated fake images and real images in the freqency domain. <a href="https://arxiv.org/pdf/1912.11035.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>Use frequency map as network input or output <a href="https://arxiv.org/pdf/2002.12416.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/2012.01832.pdf" target="_blank" rel="noopener">[5]</a> <a href="https://arxiv.org/pdf/2011.09876.pdf" target="_blank" rel="noopener">[6]</a></p>
</li>
<li><p>Use intermediate frequency features <a href="https://arxiv.org/pdf/2109.07161.pdf" target="_blank" rel="noopener">[7]</a> <a href="https://proceedings.neurips.cc/paper/2020/file/a23156abfd4a114c35b930b836064e8b-Paper.pdf" target="_blank" rel="noopener">[9]</a></p>
</li>
<li><p>An image can be composed of or decomposed into low-frequency part and high-frequency part <a href="https://openreview.net/pdf?id=Syhr6pxCW" target="_blank" rel="noopener">[3]</a> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_WaveFill_A_Wavelet-Based_Generation_Network_for_Image_Inpainting_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[8]</a> <a href="https://arxiv.org/pdf/2004.05498.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Cai_Frequency_Domain_Image_Translation_More_Photo-Realistic_Better_Identity-Preserving_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[10]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><p>Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang Chen, Fengbo Ren, “Learning in the Frequency Domain”, CVPR, 2020.</p>
</li>
<li><p>Wang, Sheng-Yu, et al. “CNN-generated images are surprisingly easy to spot… for now.” arXiv preprint arXiv:1912.11035 (2019).</p>
</li>
<li><p>ayush Bansal, Yaser Sheikh, Deva Ramanan, “PixelNN: Example-based Image Synthesis”, ICLR 2018.</p>
</li>
<li><p>Yanchao Yang, Stefano Soatto, “FDA: Fourier Domain Adaptation for Semantic Segmentation”, CVPR 2020.</p>
</li>
<li><p>Roy, Hiya, et al. “Image inpainting using frequency domain priors.” arXiv preprint arXiv:2012.01832 (2020).</p>
</li>
<li><p>Shen, Xing, et al. “DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation.” arXiv preprint arXiv:2011.09876 (2020).</p>
</li>
<li><p>Suvorov, Roman, et al. “Resolution-robust Large Mask Inpainting with Fourier Convolutions.” WACV (2021).</p>
</li>
<li><p>Yu, Yingchen, et al. “WaveFill: A Wavelet-based Generation Network for Image Inpainting.” ICCV, 2021.</p>
</li>
<li><p>Mardani, Morteza, et al. “Neural ffts for universal texture image synthesis.” NeurIPS (2020).</p>
</li>
<li><p>Cai, Mu, et al. “Frequency domain image translation: More photo-realistic, better identity-preserving.” ICCV, 2021.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Forecast Future based on One Still Image</title>
    <url>/2022/06/16/deep_learning/Forecast%20Future%20based%20on%20One%20Still%20Image/</url>
    <content><![CDATA[<ol>
<li><p>Predict visual feature of one future frame <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Vondrick_Anticipating_Visual_Representations_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>Predict optical flow of one future frame <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Im2Flow_Motion_Hallucination_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>Predict one future frame <a href="http://papers.nips.cc/paper/6552-visual-dynamics-probabilistic-future-frame-synthesis-via-cross-convolutional-networks.pdf" target="_blank" rel="noopener">[4]</a> (a special case of video prediction)</p>
</li>
<li><p>Predict future trajectories <a href="https://arxiv.org/pdf/1606.07873.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
<li><p>Predict optical flows of future frames, and then obtain future frames <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yijun_Li_Flow-Grounded_Spatial-Temporal_Video_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><p>Vondrick, Carl, Hamed Pirsiavash, and Antonio Torralba. “Anticipating visual representations from unlabeled video.” CVPR, 2016.</p>
</li>
<li><p>Gao, Ruohan, Bo Xiong, and Kristen Grauman. “Im2flow: Motion hallucination from static images for action recognition.” CVPR, 2018.</p>
</li>
<li><p>Li, Yijun, et al. “Flow-grounded spatial-temporal video prediction from still images.” ECCV, 2018.</p>
</li>
<li><p>Xue, Tianfan, et al. “Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks.” NIPS, 2016.</p>
</li>
<li><p>Walker, Jacob, et al. “An uncertain future: Forecasting from static images using variational autoencoders.” ECCV, 2016.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Fine-grained Dataset</title>
    <url>/2022/06/16/classification_detection_segmentation/Fine-grained%20Classification/</url>
    <content><![CDATA[<h2 id="Surveys"><a href="#Surveys" class="headerlink" title="Surveys:"></a>Surveys:</h2><ol>
<li><p>Deep learning for fine-grained image analysis: A survey <a href="https://arxiv.org/pdf/1907.03069.pdf" target="_blank" rel="noopener">[1]</a>: include few-shot classification, few-shot retrieval, and few-shot generation</p>
</li>
<li><p>A survey on deep learningbased fine-grained object classification and semantic segmentation <a href="https://www.semanticscholar.org/paper/A-survey-on-deep-learning-based-fine-grained-object-Zhao-Feng/a65d672715e2c58a01d693b48b9f14b68d4916bf" target="_blank" rel="noopener">[2]</a></p>
</li>
</ol>
<p>[1] Xiu-Shen Wei, Jianxin Wu, Quan Cui. “Deep learning for fine-grained image analysis: A survey.” arXiv preprint arXiv:1907.03069 (2019).</p>
<p>[2] Zhao, Bo, et al. “A survey on deep learning-based fine-grained object classification and semantic segmentation.” International Journal of Automation and Computing 14.2 (2017): 119-135.</p>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets:"></a>Datasets:</h2><ol>
<li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Xiao_Learning_From_Massive_2015_CVPR_paper.pdf" target="_blank" rel="noopener">clothing dataset</a></li>
<li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhuang_Attend_in_Groups_CVPR_2017_paper.pdf" target="_blank" rel="noopener">car dataset</a></li>
<li><a href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html" target="_blank" rel="noopener">CUB</a>, <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Berg_Birdsnap_Large-scale_Fine-grained_2014_CVPR_paper.pdf" target="_blank" rel="noopener">Birdsnap</a></li>
<li><a href="http://cs.brown.edu/~gmpatter/sunattributes.html" target="_blank" rel="noopener">scene dataset</a></li>
<li><a href="http://vision.stanford.edu/aditya86/ImageNetDogs/" target="_blank" rel="noopener">dog dataset</a></li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/data/flowers/102/" target="_blank" rel="noopener">flower dataset</a></li>
<li><a href="https://arxiv.org/pdf/1306.5151.pdf&amp;sa=U&amp;ei=tJ9PU__qHs-yyAT4j4HQBw&amp;ved=0CCcQFjAC&amp;usg=AFQjCNGvOvTh3EkWKn3TQ0r2492_22apbw" target="_blank" rel="noopener">aircraft dataset</a></li>
<li><a href="https://www.vision.ee.ethz.ch/datasets_extra/food-101/" target="_blank" rel="noopener">Food-101 dataset</a></li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>fine-grained</tag>
      </tags>
  </entry>
  <entry>
    <title>Few-Shot Object Detection</title>
    <url>/2022/06/16/classification_detection_segmentation/Few-Shot%20Object%20Detection/</url>
    <content><![CDATA[<ul>
<li><p>Feature generation for novel categories: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Hallucination_Improves_Few-Shot_Object_Detection_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9354609&amp;tag=1" target="_blank" rel="noopener">[3]</a> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Universal-Prototype_Enhancing_for_Few-Shot_Object_Detection_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[4]</a> </p>
</li>
<li><p>Model emsembling: <a href="https://arxiv.org/pdf/2011.10142.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Zhang, Weilin, and Yu-Xiong Wang. “Hallucination Improves Few-Shot Object Detection.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.</p>
<p>[2] Zhang, Weilin, Yu-Xiong Wang, and David A. Forsyth. “Cooperating RPN’s Improve Few-Shot Object Detection.” arXiv preprint arXiv:2011.10142 (2020).</p>
<p>[3] Xu, Honghui, et al. “Few-Shot Object Detection via Sample Processing.” IEEE Access 9 (2021): 29207-29221.</p>
<p>[4] Wu, Aming, et al. “Universal-prototype enhancing for few-shot object detection.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>few-shot</tag>
      </tags>
  </entry>
  <entry>
    <title>Few-Shot Image Generation From Large Dataset to Small Dataset</title>
    <url>/2022/06/16/image_video_synthesis/Few-Shot%20Image%20Generation%20From%20Large%20Dataset%20to%20Small%20Dataset/</url>
    <content><![CDATA[<ol>
<li><p>Transfer a large proportion of parameters and only update a few parameters <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Noguchi_Image_Generation_From_Small_Datasets_via_Batch_Statistics_Adaptation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[1]</a> updates scaling and shifting parameters. <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_MineGAN_Effective_Knowledge_Transfer_From_GANs_to_Target_Domains_With_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[2]</a> updates the miner before generator. <a href="https://arxiv.org/pdf/2012.02780.pdf" target="_blank" rel="noopener">[3]</a> uses Fisher information to select the parameters to be updates. Empirically, the last layers are prone to be frozen. Similarly, in <a href="http://proceedings.mlr.press/v119/zhao20a/zhao20a.pdf" target="_blank" rel="noopener">[6]</a>, the last layers are frozen and scaling/shifting parameters are predicted.</p>
</li>
<li><p>Transfer structure similarity from large dataset to small dataset: <a href="https://arxiv.org/pdf/2104.06820.pdf" target="_blank" rel="noopener">[4]</a> </p>
</li>
<li><p>Transfer parameter basis: <a href="https://arxiv.org/pdf/2010.11943.pdf" target="_blank" rel="noopener">[5]</a> adapts the singular values of the pre-trained weights while freezing the corresponding singular vectors. </p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><p>Atsuhiro Noguchi, Tatsuya Harada: “Image generation from small datasets via batch statistics adaptation.” ICCV (2019)</p>
</li>
<li><p>Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, Joost van de Weijer: “MineGAN: effective knowledge transfer from GANs to target domains with few images.” CVPR (2020).</p>
</li>
<li><p>Yijun Li, Richard Zhang, Jingwan Lu, Eli Shechtman: “Few-shot Image Generation with Elastic Weight Consolidation.” NeurIPS (2020).</p>
</li>
<li><p>Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A. Efros, Yong Jae Lee, Eli Shechtman, Richard Zhang: “Few-shot Image Generation via Cross-domain Correspondence.” CVPR (2021). </p>
</li>
<li><p>Esther Robb, Wen-Sheng Chu, Abhishek Kumar, Jia-Bin Huang: “Few-Shot Adaptation of Generative Adversarial Networks.” arXiv (2020).</p>
</li>
<li><p>Miaoyun Zhao, Yulai Cong, Lawrence Carin: “On Leveraging Pretrained GANs for Generation with Limited Data.” ICML (2020).</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>few-shot</tag>
      </tags>
  </entry>
  <entry>
    <title>Few-Shot Image Generation From Base Categories to Novel Categories</title>
    <url>/2022/06/16/image_video_synthesis/Few-Shot%20Image%20Generation%20From%20Base%20Categories%20to%20Novel%20Categories/</url>
    <content><![CDATA[<ol>
<li><p>Fusion-based method: Generative Matching Network (GMN) <a href="http://proceedings.mlr.press/v84/bartunov18a/bartunov18a.pdf" target="_blank" rel="noopener">[1]</a> (VAE with matching network for generator and recognizer). MatchingGAN <a href="https://arxiv.org/pdf/2003.03497.pdf" target="_blank" rel="noopener">[3]</a> learns reasonable interpolation coefficients. F2GAN <a href="https://arxiv.org/pdf/2008.01999.pdf" target="_blank" rel="noopener">[5]</a> first fuses high-level features and then fills in low-level details. </p>
</li>
<li><p>Optimization-based method: FIGR <a href="https://arxiv.org/pdf/1901.02199v1.pdf" target="_blank" rel="noopener">[2]</a> is based on Reptile. DAWSON <a href="https://arxiv.org/pdf/2001.00576.pdf" target="_blank" rel="noopener">[4]</a> is based on MAML. </p>
</li>
<li><p>Transformation-based method: DAGAN <a href="https://arxiv.org/pdf/1711.04340.pdf" target="_blank" rel="noopener">[6]</a> samples random vectors to generate new images. DeltaGAN <a href="https://arxiv.org/pdf/2009.08753.pdf" target="_blank" rel="noopener">[7]</a> learns sample-specific delta. </p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Bartunov, Sergey, and Dmitry Vetrov. “Few-shot generative modelling with generative matching networks.” , 2018.</p>
<p>[2] Clouâtre, Louis, and Marc Demers. “FIGR: Few-shot ImASTATISage Generation with Reptile.” arXiv preprint arXiv:1901.02199 (2019).</p>
<p>[3] Yan Hong, Li Niu, Jianfu Zhang, Liqing Zhang, “MatchingGAN: Matching-based Few-shot Image Generation”, ICME, 2020</p>
<p>[4] Weixin Liang, Zixuan Liu, Can Liu: “DAWSON: A Domain Adaptive Few Shot Generation Framework.” CoRR abs/2001.00576 (2020)</p>
<p>[5] Yan Hong, Li Niu, Jianfu Zhang, Weijie Zhao, Chen Fu, Liqing Zhang: “F2GAN: Fusing-and-Filling GAN for Few-shot Image Generation.” ACM MM (2020)</p>
<p>[6] Antreas Antoniou, Amos J. Storkey, Harrison Edwards: “Data Augmentation Generative Adversarial Networks.” stat (2018)</p>
<p>[7] Yan Hong, Li Niu, Jianfu Zhang, Jing Liang, Liqing Zhang: “DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta.” CoRR abs/2009.08753 (2020)</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>few-shot</tag>
      </tags>
  </entry>
  <entry>
    <title>Few-Shot Feature Generation</title>
    <url>/2022/06/16/deep_learning/Few-Shot%20Feature%20Generation/</url>
    <content><![CDATA[<h2 id="Few-shot-Feature-Generation"><a href="#Few-shot-Feature-Generation" class="headerlink" title="Few-shot Feature Generation"></a>Few-shot Feature Generation</h2><ol>
<li><p>Meta-learning method: <a href="https://www.cse.ust.hk/~yqsong/papers/2018-NIPS-MetaGAN-long.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>Delta-based: delta between each pair of samples <a href="https://arxiv.org/pdf/1806.04734.pdf" target="_blank" rel="noopener">[2]</a>; delta between each sample and class center <a href="https://arxiv.org/pdf/2002.10826.pdf" target="_blank" rel="noopener">[3]</a> <a href="https://arxiv.org/pdf/1803.09014.pdf" target="_blank" rel="noopener">[4]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Zhang, Ruixiang, et al. “Metagan: An adversarial approach to few-shot learning.” NIPS, 2018.</p>
<p>[2] Schwartz, Eli, et al. “Delta-encoder: an effective sample synthesis method for few-shot object recognition.” Advances in Neural Information Processing Systems. 2018.</p>
<p>[3] Liu, Jialun, et al. “Deep Representation Learning on Long-tailed Data: A Learnable Embedding Augmentation Perspective.” arXiv preprint arXiv:2002.10826 (2020).</p>
<p>[4] Yin, Xi, et al. “Feature transfer learning for face recognition with under-represented data.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>few-shot</tag>
      </tags>
  </entry>
  <entry>
    <title>Few-Shot Classification</title>
    <url>/2022/06/16/classification_detection_segmentation/Few-Shot%20Classification/</url>
    <content><![CDATA[<h2 id="One-shot-few-shot-learning"><a href="#One-shot-few-shot-learning" class="headerlink" title="One-shot/few-shot learning"></a>One-shot/few-shot learning</h2><p>The first one-shot learning paper dates back to <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1597116" target="_blank" rel="noopener">2006</a>, but becomes more popular recently.</p>
<h3 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h3><p><u>training/validation/test categories</u>: Training categories and test categories have no overlap</p>
<p><u>support(sample)/query(batch) set</u>: In the testing stage, for each test category, we preserve some instances to form the support set and sample from the remaining instances to form the query set</p>
<p><u>C-way K-shot</u>: The test set has C categories. For each test category, we preserve K instances as the support set</p>
<p><u>episode</u>: Episode-based strategy used in the training stage to match the inference in the testing stage. First sample some categories and then sample the suppport/query set for each category</p>
<h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><ul>
<li><p>Metric based:</p>
<ul>
<li><p><a href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" target="_blank" rel="noopener">Siamese network</a>: the earliest and simplest metric-learning based few-shot learning, standard verification problem.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1606.04080" target="_blank" rel="noopener">Matching network</a>: map a support set to a classification function p(y|x,S) (KNN or LSTM). For the LSTM version, there is another similar <a href="https://arxiv.org/pdf/1605.06065.pdf" target="_blank" rel="noopener">work</a> using memory module. </p>
</li>
<li><p><a href="https://arxiv.org/pdf/1711.06025.pdf" target="_blank" rel="noopener">Relation network</a>: calculate the relation score for 1-shot, calculate the average of relation scores for k-shot</p>
</li>
<li><p><a href="http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf" target="_blank" rel="noopener">Prototypical network</a>: compare with the prototype representations of each class. Each class can have more than one prototype representation. There are some other prototype-based methods <a href="https://link.springer.com/chapter/10.1007/978-3-642-33709-3_35" target="_blank" rel="noopener">[1]</a> <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Linchao_Zhu_Compound_Memory_Networks_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[2]</a>.</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Optimization (gradient) based:</p>
<ul>
<li><p>[MAML] (<a href="https://arxiv.org/pdf/1703.03400.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.03400.pdf</a>)</p>
</li>
<li><p><a href="(https://arxiv.org/abs/1803.02999">REPTILE</a>) (an approximation of MAML)</p>
</li>
<li><p><a href="https://openreview.net/pdf?id=rJY0-Kcll" target="_blank" rel="noopener">Meta-Learner LSTM</a></p>
</li>
</ul>
</li>
<li><p>Model based:</p>
<ul>
<li><p><a href="http://papers.nips.cc/paper/6068-learning-feed-forward-one-shot-learners.pdf" target="_blank" rel="noopener">[learnet]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[2]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Qiao_Few-Shot_Image_Recognition_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[3]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Memory_Matching_Networks_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[4]</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-01267-0_2" target="_blank" rel="noopener">[5]</a>: predict the parameters of classifiers for novel categories. </p>
</li>
<li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Memory_Matching_Networks_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[1]</a>: predict the parameters of CNN feature extractor by virtue of memory module.</p>
</li>
</ul>
</li>
<li><p>Generation based: generate more features for novel categories <a href="https://www.cse.ust.hk/~yqsong/papers/2018-NIPS-MetaGAN-long.pdf" target="_blank" rel="noopener">[1]</a>, <a href="https://arxiv.org/pdf/1806.04734.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>Pretraind and fine-tune: use the whole meta-training set to learn feature extractor <a href="https://arxiv.org/pdf/2003.11539.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1909.02729.pdf" target="_blank" rel="noopener">[2]</a> pretrain+MatchingNet <a href="https://arxiv.org/pdf/2003.04390.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
</ul>
<h3 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h3><ol>
<li><p><a href="https://arxiv.org/pdf/1904.05046.pdf" target="_blank" rel="noopener">Generalizing from a Few Examples: A Survey on Few-Shot Learning</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2007.15484.pdf" target="_blank" rel="noopener">Learning from Few Samples: A Survey</a></p>
</li>
</ol>
<h3 id="Datsets"><a href="#Datsets" class="headerlink" title="Datsets"></a>Datsets</h3><ol>
<li><a href="https://github.com/google-research/meta-dataset" target="_blank" rel="noopener">Meta-Dataset</a></li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>few-shot</tag>
      </tags>
  </entry>
  <entry>
    <title>Face Verification and Recognition</title>
    <url>/2022/06/16/classification_detection_segmentation/Face%20Verification%20and%20Recognition/</url>
    <content><![CDATA[<h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework:"></a>Framework:</h3><p>The similarity between two faces Ia and Ib can be unified in the following formulation:</p>
<p><strong>M[W(F(S(Ia))), W(F(S(Ib)))]</strong></p>
<p>in which <strong>S</strong> is synthesis operation (<em>e.g.</em>, face alignment, frontalization), <strong>F</strong> is robust feature extraction, <strong>W</strong> is transformation subspace learning, <strong>M</strong> means face matching algorithm (<em>e.g.</em>, NN, SVM, metric learning).</p>
<h3 id="Paper"><a href="#Paper" class="headerlink" title="Paper:"></a>Paper:</h3><ul>
<li><p>DeepID 1,2,3: Deep learning face representation from predicting 10,000 classes</p>
</li>
<li><p>FaceNet: A Unified Embedding for Face Recognition and Clustering<br>code: <a href="https://cmusatyalab.github.io/openface/" target="_blank" rel="noopener">https://cmusatyalab.github.io/openface/</a> (triplet loss)</p>
</li>
<li><p>DeepFace: Closing the Gap to Human-Level Performance in Face Verification (3D face alignment)</p>
</li>
<li><p>A Discriminative Feature Learning Approach for Deep Face Recognition<br>code: <a href="https://github.com/ydwen/caffe-face" target="_blank" rel="noopener">https://github.com/ydwen/caffe-face</a></p>
</li>
<li><p>Unconstrained Face Verification using Deep CNN Features (Joint Bayesian Metric Learning)<br>code: <a href="https://github.com/happynear/FaceVerification" target="_blank" rel="noopener">https://github.com/happynear/FaceVerification</a></p>
</li>
<li><p>A Light CNN for Deep Face Representation with Noisy Label<br>code: <a href="https://github.com/AlfredXiangWu/face_verification_experiment" target="_blank" rel="noopener">https://github.com/AlfredXiangWu/face_verification_experiment</a></p>
</li>
</ul>
<h3 id="Survey"><a href="#Survey" class="headerlink" title="Survey:"></a>Survey:</h3><ul>
<li><a href="https://arxiv.org/pdf/1811.00116.pdf" target="_blank" rel="noopener">Face Recognition: From Traditional to Deep Learning Methods</a></li>
</ul>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset:"></a>Dataset:</h3><p>LFW: <a href="http://vis-www.cs.umass.edu/lfw/" target="_blank" rel="noopener">http://vis-www.cs.umass.edu/lfw/</a></p>
<p>IJB-A: (free upon request) <a href="https://www.nist.gov/itl/iad/image-group/ijba-dataset-request-form" target="_blank" rel="noopener">https://www.nist.gov/itl/iad/image-group/ijba-dataset-request-form</a></p>
<p>FERET: (free upon request) <a href="https://www.nist.gov/itl/iad/image-group/color-feret-database" target="_blank" rel="noopener">https://www.nist.gov/itl/iad/image-group/color-feret-database</a></p>
<p>CMU Multi-Pie: (not free) <a href="http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html" target="_blank" rel="noopener">http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html</a></p>
<p>CASIA WebFace Database: (free upon request) <a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html" target="_blank" rel="noopener">http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html</a></p>
<p>MS-Celeb-1M: <a href="https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/</a></p>
<p>MegaFace: (free upon request) <a href="http://megaface.cs.washington.edu/dataset/download_training.html" target="_blank" rel="noopener">http://megaface.cs.washington.edu/dataset/download_training.html</a></p>
<p>Cross-Age Celebrity Dataset: <a href="http://bcsiriuschen.github.io/CARC/" target="_blank" rel="noopener">http://bcsiriuschen.github.io/CARC/</a></p>
<p>VGG face: <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~vgg/data/vgg_face/</a></p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>face verification</tag>
        <tag>face recognition</tag>
      </tags>
  </entry>
  <entry>
    <title>Energy Efficient Deep Learning</title>
    <url>/2022/06/16/deep_learning/Energy%20Efficient%20Deep%20Learning/</url>
    <content><![CDATA[<ol>
<li><p>Light-weighted network structure</p>
<ul>
<li><a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception</a>: strictly speaking, not light-weighted CNN</li>
<li><a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="noopener">SqueezeNet</a></li>
<li><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNet</a></li>
<li><a href="https://arxiv.org/abs/1707.01083" target="_blank" rel="noopener">ShuffleNet</a></li>
<li><a href="https://arxiv.org/pdf/2011.12289.pdf" target="_blank" rel="noopener">MicroNet</a></li>
</ul>
<p>SqueezeNet, MobileNet, and ShuffleNet share the same idea: decouple the temporal convolution and spatial convolution to reduce the nummber of parameters, sharing the similar spirit with <a href="https://arxiv.org/pdf/1711.10305.pdf" target="_blank" rel="noopener">Pseudo-3D Residual Networks</a>. SqueezeNet is serial while MobileNet and ShuffleNet are parrallel. MobileNet is a special case of ShuffleNet when using only one group.</p>
<p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhang_Efficient_and_Accurate_2015_CVPR_paper.pdf" target="_blank" rel="noopener">Low-rank approximation</a> ($k\times k \times c\times d = k\times k\times c\times d’ + 1\times 1\times d’\times d$) also falls into the above scope. The difference between MobileNet and Low-rank approximation is layerwise convolution or not.</p>
</li>
<li><p>Tweak network structure</p>
<ul>
<li>prune nodes based on certain criteria (e.g., <a href="https://arxiv.org/pdf/1510.00149.pdf" target="_blank" rel="noopener">response value</a>, <a href="https://arxiv.org/pdf/1801.05787.pdf" target="_blank" rel="noopener">Fisher information</a>): require special implementation and take up more space than expected due to irregular network structure.</li>
</ul>
</li>
<li><p>Compress weights</p>
<ul>
<li>Quantization (fixed bit number): learn codebook and encode weights. Fine-tune codebook after quantizatizing weights, which averages the gradient of weights belonging to the same cluster. Extreme cases are <a href="https://arxiv.org/pdf/1510.03009.pdf" target="_blank" rel="noopener">binary net</a> and <a href="https://arxiv.org/pdf/1612.01064.pdf" target="_blank" rel="noopener">ternary net</a>. Binary (resp, ternary) net are quantized to [-1, 1] (resp, [-1, 0, 1]), with different weights $\alpha$ for different layers.</li>
<li>Huffman Coding (flexible bit number): applied after quantization for further compression.</li>
</ul>
</li>
<li><p>Computation</p>
<ul>
<li>spatial domain to frequency domain: convert convolution to pointwise multiplication by using FFT</li>
</ul>
</li>
<li><p>Sparsity regularization</p>
<ul>
<li><a href="https://arxiv.org/pdf/1712.01312.pdf" target="_blank" rel="noopener">L0 norm</a></li>
</ul>
</li>
<li><p>Efficient Inference</p>
<ul>
<li>cascade of networks, early exit network (predict whether to exit or not after each layer) <a href="https://arxiv.org/pdf/1702.07811.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://papers.nips.cc/paper/7058-adaptive-classification-for-prediction-under-a-budget.pdf" target="_blank" rel="noopener">[2]</a></li>
</ul>
</li>
</ol>
<p>Good introduction slides: <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture15.pdf" target="_blank" rel="noopener">http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture15.pdf</a></p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>network structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Edge Detection</title>
    <url>/2022/06/16/deep_learning/Edge%20Detection/</url>
    <content><![CDATA[<p>Multi-scale fusion: HED <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[1]</a>, RCF <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Richer_Convolutional_Features_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><p>Xie, Saining, and Zhuowen Tu. “Holistically-nested edge detection.” ICCV, 2015.</p>
</li>
<li><p>Liu, Yun, et al. “Richer convolutional features for edge detection.” CVPR, 2017.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>edge detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Domain Translation</title>
    <url>/2022/06/16/image_video_synthesis/Domain%20Translation/</url>
    <content><![CDATA[<div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>supervised</th>
<th>multi-domain</th>
<th>multi-modal </th>
</tr>
</thead>
<tbody>
<tr>
<td>pix2pix <a href="https://arxiv.org/pdf/1611.07004.pdf" target="_blank" rel="noopener">[1]</a></td>
<td>yes</td>
<td>no</td>
<td>no </td>
</tr>
<tr>
<td>BicycleGAN <a href="https://arxiv.org/pdf/1711.11586.pdf" target="_blank" rel="noopener">[6]</a>, <a href="https://arxiv.org/pdf/1805.09730.pdf" target="_blank" rel="noopener">[7]</a></td>
<td>yes</td>
<td>no</td>
<td>yes</td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/1704.04886.pdf" target="_blank" rel="noopener">[10]</a></td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr>
<td>cycleGAN <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[2]</a>, UNIT <a href="http://papers.nips.cc/paper/6672-unsupervised-image-to-image-translation-networks.pdf" target="_blank" rel="noopener">[3]</a></td>
<td>no</td>
<td>no</td>
<td>no </td>
</tr>
<tr>
<td>MUNIT <a href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Xun_Huang_Multimodal_Unsupervised_Image-to-image_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[4]</a>, AugCGAN <a href="https://arxiv.org/pdf/1802.10151.pdf" target="_blank" rel="noopener">[5]</a></td>
<td>no</td>
<td>no</td>
<td>yes </td>
</tr>
<tr>
<td>starGAN <a href="https://arxiv.org/pdf/1805.09731.pdf" target="_blank" rel="noopener">[8]</a>, <a href="https://arxiv.org/pdf/1712.02050.pdf" target="_blank" rel="noopener">[9]</a>, <a href="https://arxiv.org/pdf/1811.07483.pdf" target="_blank" rel="noopener">[11]</a>, <a href="https://arxiv.org/pdf/1901.04604.pdf" target="_blank" rel="noopener">[12]</a>, ComboGAN <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w13/Anoosheh_ComboGAN_Unrestrained_Scalability_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[13]</a>, <a href="http://papers.nips.cc/paper/7525-a-unified-feature-disentangler-for-multi-domain-image-translation-and-manipulation.pdf" target="_blank" rel="noopener">[14]</a></td>
<td>no</td>
<td>yes</td>
<td>no</td>
</tr>
<tr>
<td>SMIT<a href="https://arxiv.org/pdf/1905.01270.pdf" target="_blank" rel="noopener">[15]</a>, DRIT++<a href="https://arxiv.org/pdf/1905.01270.pdf" target="_blank" rel="noopener">[16]</a>, starGANv2<a href="https://arxiv.org/pdf/1912.01865.pdf" target="_blank" rel="noopener">[19]</a></td>
<td>no</td>
<td>yes</td>
<td>yes</td>
</tr>
</tbody>
</table>
</div>
<p><img src="http://bcmi.sjtu.edu.cn/~niuli/github_images/sqrtSjK.jpg" width="70%" border="0"></p>
<p><strong>Exemplar-guided domain translation:</strong> use an exemplar to define the target domain <a href="https://arxiv.org/pdf/2004.05571.pdf" target="_blank" rel="noopener">[17]</a> <a href="https://arxiv.org/pdf/2003.08791.pdf" target="_blank" rel="noopener">[18]</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Image-to-Image Translation with Conditional Adversarial Networks</p>
<p>[2] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</p>
<p>[3] Unsupervised image-to-image translation networks</p>
<p>[4] Multimodal Unsupervised Image-to-Image Translation</p>
<p>[5] Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data</p>
<p>[6] Toward Multimodal Image-to-Image Translation</p>
<p>[7] Image-to-image translation for cross-domain disentanglement</p>
<p>[8] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</p>
<p>[9] Unsupervised Multi-Domain Image Translation with Domain-specific Encoders/Decoders</p>
<p>[10] Multi-view image Generation from a single-view</p>
<p>[11] Show, Attend and Translate- Unpaired Multi-Domain Image-to-Image Translation with Visual Attention</p>
<p>[12] Dual Generator Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</p>
<p>[13] ComboGAN: Unrestrained Scalability for Image Domain Translation</p>
<p>[14] A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation</p>
<p>[15] SMIT: Stochastic Multi-Label Image-to-Image Translation</p>
<p>[16] DRIT++: Diverse Image-to-Image Translation via Disentangled Representations</p>
<p>[17] Cross-domain Correspondence Learning for Exemplar-based Image Translation</p>
<p>[18] High-Resolution Daytime Translation Without Domain Labels</p>
<p>[19] StarGAN v2: Diverse Image Synthesis for Multiple Domains</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>domain translation</tag>
      </tags>
  </entry>
  <entry>
    <title>Domain Generalization</title>
    <url>/2022/06/16/deep_learning/Domain%20Generalization/</url>
    <content><![CDATA[<p>a) When the domain labels are known: </p>
<ul>
<li><p>reduce the distance between different domains: MMD <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2932.pdf" target="_blank" rel="noopener">[1]</a><a href="http://proceedings.mlr.press/v28/muandet13.pdf" target="_blank" rel="noopener">[2]</a>, mutual information</p>
</li>
<li><p>domain-invariant and domain-specific components: <a href="">[1]</a><a href="https://arxiv.org/pdf/1710.03077.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
</ul>
<p>b) When the domain labels are unkown:</p>
<ul>
<li>first discover multiple latent domains: cluster <a href="https://people.eecs.berkeley.edu/~jhoffman/papers/Hoffman_ECCV2012.pdf" target="_blank" rel="noopener">[1]</a><a href="http://www.vision.ee.ethz.ch/~liwenw/papers/Xu_ECCV2014.pdf" target="_blank" rel="noopener">[2]</a>, max margin separation <a href="http://papers.nips.cc/paper/5210-reshaping-visual-datasets-for-domain-adaptation.pdf" target="_blank" rel="noopener">[1]</a></li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>domain adaptation</tag>
      </tags>
  </entry>
  <entry>
    <title>Domain Adaptative Segmentation</title>
    <url>/2022/06/16/classification_detection_segmentation/Domain%20Adaptive%20Segmentation/</url>
    <content><![CDATA[<h3 id="Domain-adaptation"><a href="#Domain-adaptation" class="headerlink" title="Domain adaptation:"></a>Domain adaptation:</h3><ul>
<li><p>Align source feature map and target feature map: reduce H-divergence of regional feature map <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_ROAD_Reality_Oriented_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[4]</a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_Conditional_Generative_Adversarial_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[8]</a>, cycle consistency <a href="https://papers.nips.cc/paper/2020/file/243be2818a23c980ad664f30f48e5d19-Paper.pdf" target="_blank" rel="noopener">[11]</a></p>
</li>
<li><p>Translation source domain image to target domain image: <a href="https://arxiv.org/pdf/1711.03213.pdf" target="_blank" rel="noopener">[5]</a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Learning_From_Synthetic_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[6]</a><a href="https://arxiv.org/pdf/1904.10620.pdf" target="_blank" rel="noopener">[10]</a> (combine with target domain pseudo labels).</p>
</li>
<li><p>Training with pseudo labels on the target domain: Curriculumn learning <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[1]</a> (global image label distribution and landmark superpixel label distribution); self-training <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yang_Zou_Unsupervised_Domain_Adaptation_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[7]</a>; use multiple models to vote for pseudo labels <a href="https://arxiv.org/pdf/1711.03694.pdf" target="_blank" rel="noopener">[9]</a> </p>
</li>
</ul>
<h3 id="Domain-adaptation-with-privileged-information"><a href="#Domain-adaptation-with-privileged-information" class="headerlink" title="Domain adaptation with privileged information:"></a>Domain adaptation with privileged information:</h3><ul>
<li>Domain adaptation with privileged information like depth: SPIGAN <a href="https://arxiv.org/pdf/1810.03756.pdf" target="_blank" rel="noopener">[2]</a> (enforce synthetic image and generated image to predict the same depth), <a href="https://arxiv.org/pdf/1904.01886.pdf" target="_blank" rel="noopener">[3]</a> (adversarial learning on depth)</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Zhang, Yang, Philip David, and Boqing Gong. “Curriculum domain adaptation for semantic segmentation of urban scenes.” ICCV, 2017.</p>
<p>[2] Lee, Kuan-Hui, et al. “SPIGAN: Privileged Adversarial Learning from Simulation.” ICLR, 2019.</p>
<p>[3] Vu, Tuan-Hung, et al. “DADA: Depth-aware Domain Adaptation in Semantic Segmentation.” arXiv preprint arXiv:1904.01886 (2019).</p>
<p>[4] Chen, Yuhua, Wen Li, and Luc Van Gool. “Road: Reality oriented adaptation for semantic segmentation of urban scenes.” CVPR, 2018.</p>
<p>[5] Hoffman, Judy, et al. “Cycada: Cycle-consistent adversarial domain adaptation.” arXiv preprint arXiv:1711.03213 (2017).</p>
<p>[6] Sankaranarayanan, Swami, et al. “Learning from synthetic data: Addressing domain shift for semantic segmentation.” CVPR, 2018.</p>
<p>[7] Zou, Yang, et al. “Unsupervised domain adaptation for semantic segmentation via class-balanced self-training.”, ECCV, 2018.</p>
<p>[8] Hong, Weixiang, et al. “Conditional generative adversarial network for structured domain adaptation.” CVPR, 2018.</p>
<p>[9] Zhang, Junting, Chen Liang, and C-C. Jay Kuo. “A fully convolutional tri-branch network (FCTN) for domain adaptation.” ICASSP, 2018.</p>
<p>[10] Li, Yunsheng, Lu Yuan, and Nuno Vasconcelos. “Bidirectional Learning for Domain Adaptation of Semantic Segmentation.” arXiv preprint arXiv:1904.10620 (2019).</p>
<p>[11] Kang, Guoliang, et al. “Pixel-Level Cycle Association: A New Perspective for Domain Adaptive Semantic Segmentation.” Advances in Neural Information Processing Systems 33 (2020).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>segmentation</tag>
        <tag>domain adaptation</tag>
      </tags>
  </entry>
  <entry>
    <title>Domain Adaptation</title>
    <url>/2022/06/16/deep_learning/Domain%20Adaptation/</url>
    <content><![CDATA[<p><strong>Methods</strong></p>
<ol>
<li><p>learn projection matrix: F(PXs, QXt)</p>
<ul>
<li>project to common subspace<ul>
<li>TCA <a href="http://www.aaai.org/ocs/index.php/IJCAI/IJCAI-09/paper/download/294/962" target="_blank" rel="noopener">[pdf]</a></li>
<li>SA <a href="https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Fernando_Unsupervised_Visual_Domain_2013_ICCV_paper.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>LSSA: extension of SA <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Aljundi_Landmarks-Based_Kernelized_Subspace_2015_CVPR_paper.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>DIP <a href="https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Baktashmotlagh_Unsupervised_Domain_Adaptation_2013_ICCV_paper.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>CORAL <a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12443/11842" target="_blank" rel="noopener">[pdf]</a></li>
<li>deep CORAL <a href="https://arxiv.org/pdf/1607.01719.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>other deep feature-based methods <a href="http://proceedings.mlr.press/v37/long15.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1409.7495.pdf" target="_blank" rel="noopener">[2]</a></li>
</ul>
</li>
<li>interpolation on the manifold<ul>
<li>SGF <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.300.3323&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>GFK <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.373.2677&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[pdf]</a></li>
</ul>
</li>
</ul>
</li>
<li><p>sample selection: learn sample weights</p>
<ul>
<li>KMM <a href="http://papers.nips.cc/paper/3075-correcting-sample-selection-bias-by-unlabeled-data.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>STM <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chu_Selective_Transfer_Machine_2013_CVPR_paper.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>DASVM <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803844" target="_blank" rel="noopener">[pdf]</a></li>
<li>weighted adversarial network <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Importance_Weighted_Adversarial_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[1]</a><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1224.pdf" target="_blank" rel="noopener">[2]</a></li>
</ul>
</li>
<li><p>domain-invariant and domain-specific components</p>
<ul>
<li>SDDL <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Shekhar_Generalized_Domain-Adaptive_Dictionaries_2013_CVPR_paper.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>Domain Separation Network <a href="http://papers.nips.cc/paper/6254-domain-separation-networks.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>low-rank DL <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8053784" target="_blank" rel="noopener">[pdf]</a></li>
</ul>
</li>
<li><p>low-rank reconstruction</p>
<ul>
<li>LTSL <a href="https://search.proquest.com/docview/1531888211?pq-origsite=gscholar" target="_blank" rel="noopener">[pdf]</a></li>
<li>RDALR <a href="http://www.ee.columbia.edu/ln/dvmm/publications/12/DomainAdaptation.pdf" target="_blank" rel="noopener">[pdf]</a></li>
</ul>
</li>
<li><p>pixel-level image to image translation</p>
<ul>
<li>paired input: conditional GAN <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>unpaired input: cycling GAN <a href="https://arxiv.org/pdf/1703.10593.pdf" target="_blank" rel="noopener">[pdf]</a>, GAN with content-similarity loss <a href="https://arxiv.org/pdf/1612.05424.pdf" target="_blank" rel="noopener">[pdf]</a>, UNIT <a href="http://papers.nips.cc/paper/6672-unsupervised-image-to-image-translation-networks.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>combine with feature-based method: GraspGAN <a href="https://arxiv.org/pdf/1709.07857.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>A unified framework <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Murez_Image_to_Image_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[pdf]</a></li>
</ul>
</li>
<li><p>adversarial network <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[1]</a>: classification and domain confusion. The domain separation and confusion problem, which is a min-max problem, can be solved like GAN or using <a href="https://arxiv.org/pdf/1409.7495.pdf" target="_blank" rel="noopener">reverse gradient (RevGrad) algorithm</a>.  </p>
</li>
<li><p>meta-learning</p>
<ul>
<li>gradients on two domains should be consistent <a href="https://arxiv.org/pdf/1710.03463.pdf" target="_blank" rel="noopener">[pdf]</a></li>
</ul>
</li>
<li><p>domain alignment layer (batch normalization): <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Carlucci_AutoDIAL_Automatic_DomaIn_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/abs/1603.04779" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>guided learning: tutor guides students and get feedback from students. <a href="http://www.leizhang.tk/publications%20and%20codes.html" target="_blank" rel="noopener">ACM-MM18 paper</a></p>
</li>
<li><p>ensemble transfer learning: aggregate multiple transfer learning approaches <a href="https://arxiv.org/pdf/1708.05629.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
</ol>
<p><strong>Settings</strong></p>
<ol>
<li><p>open-set domain adaptation or partial transfer learning: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Busto_Open_Set_Domain_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[1]</a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Partial_Transfer_Learning_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[2]</a><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
<li><p>distant domain adaptation (two domains are too distant, so the transfer between them relies on transition domains): <a href="https://dl.acm.org/ft_gateway.cfm?id=2783295&amp;ftid=1613048&amp;dwn=1&amp;CFID=49856439&amp;CFTOKEN=825d8eea3c707e8-BF658D58-A3BB-2ACB-19DC13C6B3291336" target="_blank" rel="noopener">Transitive transfer learning</a>, <a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14446/14451" target="_blank" rel="noopener">distant domain transfer learning</a>  </p>
</li>
<li><p>open compound domain adaptation <a href="https://arxiv.org/pdf/1909.03403.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
</ol>
<p><strong>Domain adaptation for diverse applications</strong></p>
<ol>
<li><p>pose estimation <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8245864" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>person re-identification <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018030" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>objection detection <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Domain_Adaptive_Faster_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>segmentation <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>VQA <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chao_Cross-Dataset_Adaptation_for_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
</ol>
<p><strong>Domain difference metric</strong>: To measure data distribution mismatch, the most commonly used metric is MMD and its extensions such as <a href="https://arxiv.org/pdf/1405.2664.pdf" target="_blank" rel="noopener">fast MMD</a>, conditional MMD <a href="https://arxiv.org/pdf/1606.04218.pdf" target="_blank" rel="noopener">[1]</a><a href="https://arxiv.org/pdf/1503.00591.pdf" target="_blank" rel="noopener">[2]</a> and <a href="https://arxiv.org/pdf/1605.06636.pdf" target="_blank" rel="noopener">joint MMD</a>. There are also some other metrics like KL divergence, HSIC criterion, Bregman divergence, manifold criterion, and second-order statistic.</p>
<p><strong>Theories</strong>: <a href="http://bcmi.sjtu.edu.cn/home/niuli/download/Transfer_Learning_Theories_and_Algorithms.pdf" target="_blank" rel="noopener">A summary of related theories</a></p>
<p><strong>Survey</strong>: </p>
<ol>
<li>An old survey of transfer learning <a href="http://www.cs.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>Recent advance on domain adaptation <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7078994" target="_blank" rel="noopener">[pdf]</a></li>
<li>My survey of old deep learning domain adaptation methods <a href="http://bcmi.sjtu.edu.cn/home/niuli/download/deep_learning_for_domain_adaptation.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>A Chinese version of transfer learning tutorial <a href="http://jd92.wang/assets/files/transfer_learning_tutorial_wjd.pdf" target="_blank" rel="noopener">[pdf]</a></li>
<li>Datasets and code: <a href="https://github.com/jindongwang/transferlearning" target="_blank" rel="noopener">[1]</a></li>
<li>A Comprehensive Survey on Transfer Learning <a href="https://arxiv.org/pdf/1911.02685.pdf" target="_blank" rel="noopener">[pdf]</a></li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>domain adaptation</tag>
      </tags>
  </entry>
  <entry>
    <title>Disentangled Representation</title>
    <url>/2022/06/16/deep_learning/Disentangled%20Representation/</url>
    <content><![CDATA[<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods:"></a>Methods:</h2><p>The goal of Disentangled Representation [4] is to extract explanatory factors of the data in the input distribution and generate a more meaningful representation. disentangle codes/encodings/representations/latent factors/latent variables. single-dimension attribute encoding or multi-dimension attribute encoding.</p>
<p>A math definition of disentangled representation <a href="https://arxiv.org/pdf/1812.02230.pdf" target="_blank" rel="noopener">[11]</a></p>
<p>A survey on disentangled representation learning <a href="https://arxiv.org/pdf/2211.11695.pdf" target="_blank" rel="noopener">[19]</a></p>
<ul>
<li><p>Unsupervised disentanglement</p>
<p>  Recently, InfoGAN [5] utilizes GAN framework and maximizes the mutual information between a subset of the latent variables to learn disentangled representations in an unsupervised manner. Different latent variables are enforced to be independent based on the independence assumption [6].  </p>
</li>
<li><p>Supervised disentanglement</p>
<p>  Swapping attribute representation with the supervision of attribute annotation such as Dual Swap GAN [7] (semi-supervised) and DNA-GAN [8].</p>
</li>
<li><p>Disentangle representation for domain adaptation, disentangle representation into Class/domain-invariant and class/domain-specific: [9][10]<a href="http://128.84.21.203/pdf/1906.11796" target="_blank" rel="noopener">[12]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hadad_A_Two-Step_Disentanglement_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[13]</a></p>
</li>
<li><p>instance-level disentangle<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Shen_Towards_Instance-Level_Image-To-Image_Translation_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[14]</a> <a href="https://arxiv.org/pdf/1812.10889.pdf" target="_blank" rel="noopener">[15]</a> FUNIT<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Few-Shot_Unsupervised_Image-to-Image_Translation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[16]</a> COCO-FUNIT[<a href="http://xxx.itp.ac.cn/pdf/2007.07431v1" target="_blank" rel="noopener">17</a></p>
</li>
<li><p>close-form disentanglement <a href="https://arxiv.org/pdf/2007.06600.pdf" target="_blank" rel="noopener">[18]</a>: after the model is trained, perform eigen decomposition to obtain orthogonal directions. </p>
</li>
</ul>
<h2 id="Disentanglement-metric"><a href="#Disentanglement-metric" class="headerlink" title="Disentanglement metric:"></a>Disentanglement metric:</h2><ul>
<li>disentangement metric score <a href="(https://pdfs.semanticscholar.org/a902/26c41b79f8b06007609f39f82757073641e2.pdf">[1]</a>)</li>
<li>perceptual path length,  linear separabilit <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[2]</a></li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1]  Higgins, Irina, et al. “beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.” ICLR 2.5 (2017): 6. </p>
<p>[2] Karras, Tero, Samuli Laine, and Timo Aila. “A style-based generator architecture for generative adversarial networks.” CVPR, 2019.</p>
<p>[4] <a href="https://arxiv.org/pdf/1206.5538.pdf" target="_blank" rel="noopener">Representation learning: A review and new perspectives</a></p>
<p>[5] <a href="http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf" target="_blank" rel="noopener">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</a></p>
<p>[6] <a href="https://arxiv.org/pdf/1710.05050.pdf" target="_blank" rel="noopener">Learning Independent Features with adversarial Nets for Non-linear ICA</a></p>
<p>[7] <a href="http://papers.nips.cc/paper/7830-dual-swap-disentangling.pdf" target="_blank" rel="noopener">Dual Swap Disentangling</a></p>
<p>[8] <a href="https://arxiv.org/pdf/1711.05415.pdf" target="_blank" rel="noopener">DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images</a></p>
<p>[9] <a href="https://arxiv.org/pdf/1805.09730.pdf" target="_blank" rel="noopener">Image-to-image translation for cross-domain disentanglement</a></p>
<p>[10] <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Hsin-Ying_Lee_Diverse_Image-to-Image_Translation_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Diverse Image-to-Image Translation via Disentangled Representations</a></p>
<p>[11] Higgins, Irina, et al. “Towards a definition of disentangled representations.” arXiv preprint arXiv:1812.02230 (2018).</p>
<p>[12] Gabbay, Aviv, and Yedid Hoshen. “Demystifying Inter-Class Disentanglement.” arXiv preprint arXiv:1906.11796 (2019).</p>
<p>[13] Hadad, Naama, Lior Wolf, and Moni Shahar. “A two-step disentanglement method.” CVPR, 2018.</p>
<p>[14] Shen, Zhiqiang, et al. “Towards instance-level image-to-image translation.” CVPR, 2019.</p>
<p>[15] Sangwoo Mo, Minsu Cho, Jinwoo Shin:<br>InstaGAN: Instance-aware Image-to-Image Translation. ICLR, 2019.</p>
<p>[16] Liu, Ming-Yu, et al. “Few-shot unsupervised image-to-image translation.” ICCV, 2019.</p>
<p>[17] Saito, Kuniaki, Kate Saenko, and Ming-Yu Liu. “COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content Conditioned Style Encoder.” arXiv preprint arXiv:2007.07431 (2020).</p>
<p>[18] Shen, Yujun, and Bolei Zhou. “Closed-Form Factorization of Latent Semantics in GANs.” arXiv preprint arXiv:2007.06600 (2020).</p>
<p>[19] Xin Wang, Hong Chen, Siao Tang, Zihao Wu, and Wenwu Zhu. “Disentangled Representation Learning.”</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>disentangle</tag>
        <tag>representation learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Disentangle Datasets</title>
    <url>/2022/06/16/deep_learning/Disentangle%20Datasets/</url>
    <content><![CDATA[<ul>
<li>CelebA <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Liu_Deep_Learning_Face_ICCV_2015_paper.pdf" target="_blank" rel="noopener">[14]</a> (dataset for human faces): [12, 2, 11, 17, 13, 8, 13, 18]</li>
<li>MNIST <a href="http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf" target="_blank" rel="noopener">[10]</a>, MNIST-M <a href="http://www.jmlr.org/papers/volume17/15-239/15-239.pdf" target="_blank" rel="noopener">[4]</a> (digits): [16, 15, 12, 5, 2, 11, 9, 17, 6, 8, 13, 3]</li>
<li>Yosemite <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[19]</a> (summer and winter scenes): [11]</li>
<li>Artworks <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[19]</a> (Monet and Van Gogh): [11]</li>
<li><a href="http://lpc.opengameart.org/" target="_blank" rel="noopener">2D Sprites</a> (game characters): [15, 9, 6, 8, 3]</li>
<li>LineMod <a href="https://pdfs.semanticscholar.org/5142/39d0323bbcc36f6fa806c3a35a00f697c7aa.pdf" target="_blank" rel="noopener">[7]</a> (3D object): [9]</li>
<li>11k Hands <a href="https://arxiv.org/pdf/1711.04322.pdf" target="_blank" rel="noopener">[1]</a> (hand gestures): [17]</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] M. Afifi. Gender recognition and biometric identification using a large dataset of hand images. arXiv preprint arXiv:1711.04322, 2017.</p>
<p>[2] E. Dupont. Learning disentangled joint continuous and discrete representations. In S. Bengio, H.Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 708–718. Curran Associates, Inc., 2018.</p>
<p>[3] Z. Feng, X. Wang, C. Ke, A.-X. Zeng, D. Tao, and M. Song. Dual swap disentangling. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 5898–5908. Curran Associates, Inc., 2018.</p>
<p>[4] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016.</p>
<p>[5] A. Gonzalez-Garcia, J. van de Weijer, and Y. Bengio. Image-to-image translation for cross-domain disentanglement. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 1294–1305. Curran Associates, Inc., 2018.</p>
<p>[6] N. Hadad, L. Wolf, and M. Shahar. A two-step disentanglement method. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.</p>
<p>[7] S. Hinterstoisser, V. Lepetit, S. Ilic, S. Holzer, G. Bradski, K. Konolige, and N. Navab. Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes. In Asian conference on computer vision, pages 548–562. Springer, 2012.</p>
<p>[8] Q. Hu, A. Szab, T. Portenier, P. Favaro, and M. Zwicker. Disentangling factors of variation by mixing them. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.</p>
<p>[9] A. H. Jha, S. Anand, M. Singh, and V. Veeravasarapu. Disentangling factors of variation with cycle-consistent variational autoencoders. In The European Conference on Computer Vision (ECCV), September 2018.</p>
<p>[10] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.</p>
<p>[11] H.-Y. Lee, H.-Y. Tseng, J.-B. Huang, M. Singh, and M.-H. Yang. Diverse image-to-image translation via disentangled representations. In The European Conference on Computer Vision (ECCV), September 2018.</p>
<p>[12] A. H. Liu, Y.-C. Liu, Y.-Y. Yeh, and Y.-C. F. Wang. A unified feature disentangler for multi-domain image translation and manipulation. In S. Bengio, H.Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 2595–2604. Curran Associates, Inc., 2018.</p>
<p>[13] Y. Liu, F. Wei, J. Shao, L. Sheng, J. Yan, and X. Wang. Exploring disentangled feature representation beyond face identification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.</p>
<p>[14] Z. Liu, P. Luo, X.Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pages 3730–3738, 2015.</p>
<p>[15] M. F. Mathieu, J. J. Zhao, J. Zhao, A. Ramesh, P. Sprechmann, and Y. LeCun. Disentangling factors of variation in deep representation using adversarial training. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 5040–5048. Curran Associates, Inc., 2016.</p>
<p>[16] S. Narayanaswamy, T. B. Paige, J.-W. van de Meent, A. Desmaison, N. Goodman, P. Kohli, F. Wood, and P. Torr. Learning disentangled representations with semi-supervised deep generative models. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5925–5935. Curran Associates, Inc., 2017.</p>
<p>[17] Z. Shu, M. Sahasrabudhe, R. Alp Guler, D. Samaras, N. Paragios, and I. Kokkinos. Deforming autoencoders: Unsupervised disentangling of shape and appearance. In The European Conference on Computer Vision (ECCV), September 2018.</p>
<p>[18] Z. Shu, E. Yumer, S. Hadap, K. Sunkavalli, E. Shechtman, and D. Samaras. Neural face editing with intrinsic image disentangling. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.</p>
<p>[19] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint, 2017.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>disentangle</tag>
      </tags>
  </entry>
  <entry>
    <title>Differential Programming</title>
    <url>/2022/06/16/deep_learning/Differential%20Programming/</url>
    <content><![CDATA[<p>The first work of differential programming is using network to approximate sparse coding. <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=9252E34D6078997E19677B5397A89074?doi=10.1.1.184.7256&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[pdf]</a></p>
<p>Introduction tutorials: <a href="http://bcmi.sjtu.edu.cn/home/niuli/download/intro_deep_unrolling.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://bcmi.sjtu.edu.cn/home/niuli/download/intro_differential_programming.pdf" target="_blank" rel="noopener">[2]</a></p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Different Ways of Injecting Latent Code</title>
    <url>/2022/06/16/deep_learning/Different%20Ways%20of%20Injecting%20Latent%20Code/</url>
    <content><![CDATA[<h4 id="Inject-a-latent-vector-into-network"><a href="#Inject-a-latent-vector-into-network" class="headerlink" title="Inject a latent vector into network:"></a>Inject a latent vector into network:</h4><ol>
<li><p>Concatenate or add it to the input image <a href="https://arxiv.org/pdf/1805.09731.pdf" target="_blank" rel="noopener">[1]</a> <a href="https://arxiv.org/pdf/1905.01164.pdf" target="_blank" rel="noopener">[7]</a> </p>
</li>
<li><p>Concatenate or add it to encoder layer <a href="https://arxiv.org/pdf/1711.11586.pdf" target="_blank" rel="noopener">[2]</a> </p>
</li>
<li><p>Concatenate or add it to the bottleneck <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Pluralistic_Image_Completion_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[3]</a> </p>
</li>
<li><p>Concatenate or add it to decoder layer <a href="https://arxiv.org/pdf/1711.04340.pdf" target="_blank" rel="noopener">[6]</a> <a href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Hsin-Ying_Lee_Diverse_Image-to-Image_Translation_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[8]</a></p>
</li>
<li><p>Add to decoder layers using AdaIn <a href="https://arxiv.org/pdf/1812.04948.pdf" target="_blank" rel="noopener">[4]</a> (without skip connection), <a href="https://arxiv.org/pdf/2003.08791.pdf" target="_blank" rel="noopener">[5]</a> (with skip connection)</p>
</li>
</ol>
<h4 id="Inject-a-latent-map-into-network"><a href="#Inject-a-latent-map-into-network" class="headerlink" title="Inject a latent map into network:"></a>Inject a latent map into network:</h4><ol>
<li><p>For concatenation or addition, spatially stack latent codes <a href="https://arxiv.org/pdf/2004.12411.pdf" target="_blank" rel="noopener">[9]</a></p>
</li>
<li><p>Spatially adaptive AdaIn: SPADE <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[10]</a>, OASIS <a href="https://arxiv.org/pdf/2012.04781.pdf" target="_blank" rel="noopener">[11]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</p>
<p>[2] Toward Multimodal Image-to-Image Translation</p>
<p>[3] Zheng, Chuanxia, Tat-Jen Cham, and Jianfei Cai. “Pluralistic image completion.” CVPR, 2019.</p>
<p>[4] Karras, Tero, Samuli Laine, and Timo Aila. “A style-based generator architecture for generative adversarial networks.” CVPR, 2019.</p>
<p>[5] High-Resolution Daytime Translation Without Domain Labels</p>
<p>[6] Antoniou, Antreas, Amos Storkey, and Harrison Edwards. “Data augmentation generative adversarial networks.” arXiv preprint arXiv:1711.04340 (2017).</p>
<p>[7] Tamar Rott Shaham, Tali Dekel, Tomer Michaeli, “SinGAN: Learning a Generative Model from a Single Natural Image”, ICCV2019</p>
<p>[8] Lee, Hsin-Ying, et al. “Diverse image-to-image translation via disentangled representations.” ECCV, 2018.</p>
<p>[9] Yazeed Alharbi, Peter Wonka: Disentangled Image Generation Through Structured Noise Injection. CVPR, 2020.</p>
<p>[10] Park, Taesung, et al. “Semantic image synthesis with spatially-adaptive normalization.” CVPR, 2019.</p>
<p>[11] Sushko, Vadim, et al. “You only need adversarial supervision for semantic image synthesis.” ICLR, 2021</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>domain translation</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning Platform</title>
    <url>/2022/06/16/deep_learning/Deep%20Learning%20Platform/</url>
    <content><![CDATA[<ol>
<li><p>Ready-made DevBox:</p>
<ul>
<li><a href="https://www.dell.com/en-us/gaming/alienware-desktops?cid=312465046&amp;st=alienware&amp;lid=59673390274&amp;ptaid=kwd-24191410&amp;VEN1=suyunKW6A%2C282265229036%2C901pdb6671%2Cc%2CM68G1rWA%2C%2C56626657187%2Ckwd-24191410&amp;VEN2=e%2Calienware&amp;pgrid=56626657187&amp;dgc=st&amp;dgseg=dhs&amp;acd=1230923830920560&amp;VEN3=113804489962527442" target="_blank" rel="noopener">Dell Alienware</a>: at most 2 GPUs</li>
<li><a href="https://www.newegg.com/Product/Product.aspx?Item=9SIAB946K29620" target="_blank" rel="noopener">newegg</a>: 4 GPUs</li>
<li><a href="https://lambdal.com/raw-configurator?product=quad-amd" target="_blank" rel="noopener">Lambda Labs</a>: 4 GPUs</li>
</ul>
</li>
<li><p>Assemble: cheap, but no warranty</p>
<ul>
<li><a href="https://sites.google.com/site/ustcnewly/download/newegg_list.pdf?attredirects=0" target="_blank" rel="noopener">part list</a>: most things are out of date. <a href="https://www.tomshardware.com/" target="_blank" rel="noopener">Tom’s hardware</a> is a good website for comparison. </li>
</ul>
</li>
<li><p>Nvidia</p>
<ul>
<li><p>microarchitecture: maxwell-&gt;pascal-&gt;volta</p>
</li>
<li><p><a href="https://www.nvidia.com/en-us/data-center/" target="_blank" rel="noopener">DGX-systems</a></p>
</li>
</ul>
</li>
<li><p>GPU cloud</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Deep Feature Invariance</title>
    <url>/2022/06/16/deep_learning/Deep%20Feature%20Invariance/</url>
    <content><![CDATA[<p>Some related papers: <a href="https://arxiv.org/pdf/1811.00252.pdf" target="_blank" rel="noopener">[1]</a><a href="https://arxiv.org/pdf/1811.01122.pdf" target="_blank" rel="noopener">[2]</a><a href="https://arxiv.org/pdf/1810.03234.pdf" target="_blank" rel="noopener">[3]</a><a href="https://arxiv.org/pdf/1812.11832.pdf" target="_blank" rel="noopener">[4]</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><p>Pun, Chi Seng, Kelin Xia, and Si Xian Lee. “Persistent-Homology-based Machine Learning and its Applications—A Survey.” arXiv preprint arXiv:1811.00252 (2018).</p>
</li>
<li><p>Carlsson, Gunnar, and Rickard Brüel Gabrielsson. “Topological approaches to deep learning.” arXiv preprint arXiv:1811.01122 (2018).</p>
</li>
<li><p>Gabrielsson, Rickard Brüel, and Gunnar Carlsson. “Exposition and interpretation of the topology of neural networks.” 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA). IEEE, 2019.</p>
</li>
<li><p>Bergomi, Mattia G., et al. “Towards a topological–geometrical theory of group equivariant non-expansive operators for data analysis and machine learning.” Nature Machine Intelligence 1.9 (2019): 423-433.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Deep EM</title>
    <url>/2022/06/16/deep_learning/Deep%20EM/</url>
    <content><![CDATA[<ol>
<li><p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Xiao_Learning_From_Massive_2015_CVPR_paper.pdf" target="_blank" rel="noopener">Learning from Massive Noisy Labeled Data for Image Classification</a>: hidden variable is the label noise type</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1907.13426.pdf" target="_blank" rel="noopener">Expectation-Maximization Attention Networks for Semantic Segmentation</a>: hidden variable is dictionary basis</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>EM</tag>
      </tags>
  </entry>
  <entry>
    <title>Cut and Paste</title>
    <url>/2022/06/16/image_video_synthesis/Cut%20and%20Paste/</url>
    <content><![CDATA[<ol>
<li><p>Do segmentation, image enhancemnet, and inpainting simultaneously <a href="https://arxiv.org/pdf/1811.07630.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>Learning to Segment via Cut-and-Paste <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Tal_Remez_Learning_to_Segment_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Ostyakov, Pavel, et al. “SEIGAN: Towards Compositional Image Generation by Simultaneously Learning to Segment, Enhance, and Inpaint.” arXiv preprint arXiv:1811.07630 (2018).</p>
<p>[2] Remez, Tal, Jonathan Huang, and Matthew Brown. “Learning to segment via cut-and-paste.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>semantic segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>Conditional GAN</title>
    <url>/2022/06/16/image_video_synthesis/Conditional%20GAN/</url>
    <content><![CDATA[<ol>
<li><p>Conditioned on label vector: conditional GAN <a href="https://arxiv.org/pdf/1411.1784.pdf" target="_blank" rel="noopener">[4]</a>, CVAE-GAN <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[6]</a></p>
</li>
<li><p>Conditioned on a single image</p>
<ul>
<li>pix2pix <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[1]</a>; high-resolution pix2pix <a href="https://arxiv.org/pdf/1711.11585.pdf" target="_blank" rel="noopener">[2]</a> (add coarse-to-fine strategy); BicycleGAN <a href="https://papers.nips.cc/paper/6650-toward-multimodal-image-to-image-translation.pdf" target="_blank" rel="noopener">[3]</a> (combination of cVAE-GAN and cLR-GAN)</li>
<li>DAGAN <a href="https://arxiv.org/pdf/1711.04340" target="_blank" rel="noopener">[5]</a></li>
</ul>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Isola, Phillip, et al. “Image-to-image translation with conditional adversarial networks.” CVPR, 2017</p>
<p>[2] Wang, Ting-Chun, et al. “High-resolution image synthesis and semantic manipulation with conditional gans.” CVPR, 2018.</p>
<p>[3] Zhu, Jun-Yan, et al. “Toward multimodal image-to-image translation.” NIPS, 2017.</p>
<p>[4] Mirza, Mehdi, and Simon Osindero. “Conditional generative adversarial nets.” arXiv preprint arXiv:1411.1784 (2014).</p>
<p>[5] Antoniou, Antreas, Amos Storkey, and Harrison Edwards. “Data augmentation generative adversarial networks.” arXiv preprint arXiv:1711.04340 (2017).</p>
<p>[6] Bao, Jianmin, et al. “CVAE-GAN: fine-grained image generation through asymmetric training.” ICCV, 2017.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>Color Mapping</title>
    <url>/2022/06/16/deep_learning/Color%20Mapping/</url>
    <content><![CDATA[<h4 id="Global-color-mapping"><a href="#Global-color-mapping" class="headerlink" title="Global color mapping:"></a>Global color mapping:</h4><ul>
<li>3D LUT: <a href="https://www4.comp.polyu.edu.hk/~cslzhang/paper/PAMI_LUT.pdf" target="_blank" rel="noopener">[1]</a>, <a href="https://arxiv.org/pdf/2204.13983.pdf" target="_blank" rel="noopener">[2]</a> non-uniform LUT</li>
<li>curve function: <a href="https://arxiv.org/pdf/2109.05750.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>linear transformation: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Afifi_What_Else_Can_Fool_Deep_Learning_Addressing_Color_Constancy_Errors_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[1]</a></li>
</ul>
<h4 id="Local-color-mapping"><a href="#Local-color-mapping" class="headerlink" title="Local color mapping:"></a>Local color mapping:</h4><ul>
<li>3D LUT: <a href="https://arxiv.org/pdf/2108.08697.pdf" target="_blank" rel="noopener">[1]</a></li>
<li>curve function: <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Song_StarEnhancer_Learning_Real-Time_and_Style-Aware_Image_Enhancement_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[1]</a>, DCE<a href="https://arxiv.org/pdf/2001.06826.pdf" target="_blank" rel="noopener">[2]</a></li>
<li>linear transformation: HDRNet<a href="https://arxiv.org/pdf/1707.02880" target="_blank" rel="noopener">[1]</a></li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>CLIP</title>
    <url>/2022/06/16/deep_learning/CLIP/</url>
    <content><![CDATA[<ul>
<li><p>image classication: CLIP <a href="https://arxiv.org/pdf/2103.00020.pdf" target="_blank" rel="noopener">[1]</a>, learnable prompt <a href="https://arxiv.org/pdf/2109.01134.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>video classification: ActionCLIP <a href="https://arxiv.org/pdf/2109.08472.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
<li><p>object detection: ViLD <a href="https://arxiv.org/pdf/2104.13921.pdf" target="_blank" rel="noopener">[4]</a>, ZSD-YOLO <a href="https://arxiv.org/pdf/2109.12066.pdf" target="_blank" rel="noopener">[6]</a></p>
</li>
<li><p>segmentation: <a href="https://arxiv.org/pdf/2112.14757.pdf" target="_blank" rel="noopener">[8]</a> <a href="https://arxiv.org/pdf/2112.10003.pdf" target="_blank" rel="noopener">[9]</a></p>
</li>
<li><p>visual grounding: CPT <a href="https://arxiv.org/pdf/2109.11797.pdf" target="_blank" rel="noopener">[5]</a> </p>
</li>
<li><p>image translation: StyleClip <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Patashnik_StyleCLIP_Text-Driven_Manipulation_of_StyleGAN_Imagery_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[7]</a> </p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Radford, Alec, et al. “Learning transferable visual models from natural language supervision.” arXiv preprint arXiv:2103.00020 (2021).</p>
<p>[2] Zhou, Kaiyang, et al. “Learning to Prompt for Vision-Language Models.” arXiv preprint arXiv:2109.01134 (2021).</p>
<p>[3] Wang, Mengmeng, Jiazheng Xing, and Yong Liu. “ActionCLIP: A New Paradigm for Video Action Recognition.” arXiv preprint arXiv:2109.08472 (2021).</p>
<p>[4] Gu, Xiuye, et al. “Zero-Shot Detection via Vision and Language Knowledge Distillation.” arXiv preprint arXiv:2104.13921 (2021).</p>
<p>[5] Yao, Yuan, et al. “CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models.” arXiv preprint arXiv:2109.11797 (2021).</p>
<p>[6] Xie, Johnathan, and Shuai Zheng. “ZSD-YOLO: Zero-Shot YOLO Detection using Vision-Language KnowledgeDistillation.” arXiv preprint arXiv:2109.12066 (2021).</p>
<p>[7] Patashnik, Or, et al. “Styleclip: Text-driven manipulation of stylegan imagery.” ICCV, 2021.</p>
<p>[8] Xu, Mengde, et al. “A simple baseline for zero-shot semantic segmentation with pre-trained vision-language model.” arXiv preprint arXiv:2112.14757 (2021).</p>
<p>[9] Lüddecke, Timo, and Alexander Ecker. “Image Segmentation Using Text and Image Prompts.” CVPR, 2022.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Capsule Network</title>
    <url>/2022/06/16/deep_learning/Capsule%20Network/</url>
    <content><![CDATA[<ul>
<li><p>Typical works: CapsNet <a href="https://arxiv.org/pdf/1710.09829.pdf" target="_blank" rel="noopener">[1]</a>, CapProNet <a href="https://arxiv.org/pdf/1805.07621.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://github.com/maple-research-lab" target="_blank" rel="noopener">[code]</a></p>
</li>
<li><p>The robustness of Capsule network: <a href="https://arxiv.org/pdf/2103.15459.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Sabour S, Frosst N, Hinton G E. Dynamic routing between capsules[C]//Advances in Neural Information Processing Systems. 2017: 3856-3866.</p>
<p>[2] Zhang L, Edraki M, Qi G J. CapProNet: Deep feature learning via orthogonal projections onto capsule subspaces[J]. arXiv preprint arXiv:1805.07621, 2018.</p>
<p>[3] Jindong Gu, Volker Tresp, Han Hu, “Capsule Network is Not More Robust than Convolutional Network”, CVPR 2021.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>network structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Camouflaged Object Detection and Segmentation</title>
    <url>/2022/06/16/classification_detection_segmentation/Camouflaged%20Object%20Detection%20and%20Segmentation/</url>
    <content><![CDATA[<ul>
<li><p>Camouflaged Object Detection: <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_Camouflaged_Object_Detection_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>Camouflaged Object Segmentation <a href="https://arxiv.org/pdf/2007.12881.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><p>Fan, Deng-Ping, et al. “Camouflaged object detection.” CVPR, 2020.</p>
</li>
<li><p>Yan, Jinnan, et al. “MirrorNet: Bio-Inspired Adversarial Attack for Camouflaged Object Segmentation.” arXiv preprint arXiv:2007.12881 (2020).</p>
</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>Boundary-guided Semantic Segmentation</title>
    <url>/2022/06/16/classification_detection_segmentation/Boundary-guided%20Semantic%20Segmentation/</url>
    <content><![CDATA[<ol>
<li><p>propagate information within each non-boundary region <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_Boundary-Aware_Feature_Propagation_for_Scene_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[1]</a> </p>
</li>
<li><p>focus on unconfident boundary regions <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Marin_Efficient_Segmentation_Learning_Downsampling_Near_Semantic_Boundaries_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
<li><p>fuse boundary feature and image feature <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Takikawa_Gated-SCNN_Gated_Shape_CNNs_for_Semantic_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[3]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Ding, Henghui, et al. “Boundary-aware feature propagation for scene segmentation.” ICCV, 2019.</p>
<p>[2] Marin, Dmitrii, et al. “Efficient segmentation: Learning downsampling near semantic boundaries.” ICCV, 2019.</p>
<p>[3] Takikawa, Towaki, et al. “Gated-scnn: Gated shape cnns for semantic segmentation.” ICCV, 2019.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>boundary</tag>
        <tag>semantic segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>Bio-inspired Network</title>
    <url>/2022/06/16/deep_learning/Bio-inspired%20Network/</url>
    <content><![CDATA[<ol>
<li><p>Use the first few layers to simulate neuron activation in human brain <a href="https://www.biorxiv.org/content/biorxiv/early/2020/10/22/2020.06.16.154542.full.pdf" target="_blank" rel="noopener">[1]</a> </p>
</li>
<li><p>Use the attention learnt by network to mimick human attention <a href="https://arxiv.org/pdf/1805.08819.pdf" target="_blank" rel="noopener">[2]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Dapello, Joel, et al. “Simulating a primary visual cortex at the front of CNNs improves robustness to image perturbations.” BioRxiv (2020).</p>
<p>[2] Linsley, Drew, et al. “Learning what and where to attend.” arXiv preprint arXiv:1805.08819 (2018).</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Attention Mechanism</title>
    <url>/2022/06/16/deep_learning/Attention%20Mechanism/</url>
    <content><![CDATA[<h2 id="Attention-in-CNN"><a href="#Attention-in-CNN" class="headerlink" title="Attention in CNN:"></a>Attention in CNN:</h2><p>According to <a href="https://arxiv.org/pdf/1805.08819.pdf" target="_blank" rel="noopener">[4]</a>, attention can be categorized into bottom-up attention (visual saliency, unsupervised) and top-down attention (task-driven, supervised).</p>
<p>According to <a href="https://arxiv.org/pdf/1804.02391.pdf" target="_blank" rel="noopener">[5]</a>, attention can be categorized into forward attention, post-hoc attention, and query-based attention.</p>
<ul>
<li><p>forward attention: spatial attention <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_An_Empirical_Study_of_Spatial_Attention_Mechanisms_in_Deep_Networks_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[16]</a>, channel attention <a href="https://arxiv.org/pdf/1802.08122.pdf" target="_blank" rel="noopener">[10]</a> <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_ECA-Net_Efficient_Channel_Attention_for_Deep_Convolutional_Neural_Networks_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[17]</a> <a href="https://arxiv.org/pdf/2012.11879.pdf" target="_blank" rel="noopener">[18]</a>,  full attention <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Cheng_Wang_Mancs_A_Multi-task_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[11]</a>, Deformable CNN v1 <a href="https://arxiv.org/pdf/1703.06211.pdf" target="_blank" rel="noopener">[8]</a> v2 <a href="https://arxiv.org/pdf/1811.11168.pdf" target="_blank" rel="noopener">[9]</a>, </p>
</li>
<li><p>post-hoc attention: CAM <a href="https://arxiv.org/pdf/1512.04150.pdf" target="_blank" rel="noopener">[6]</a>, GradCAM <a href="https://arxiv.org/pdf/1610.02391.pdf" target="_blank" rel="noopener">[7]</a>, scoreCAM <a href="https://arxiv.org/pdf/1910.01279" target="_blank" rel="noopener">[14]</a>, trainable CAM <a href="https://arxiv.org/pdf/2101.11253.pdf" target="_blank" rel="noopener">[20]</a><a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Araslanov_Single-Stage_Semantic_Segmentation_From_Image_Labels_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[21]</a></p>
</li>
<li><p>query-based attention: <a href="https://arxiv.org/pdf/1804.02391.pdf" target="_blank" rel="noopener">[5]</a></p>
</li>
<li><p>erase attention: <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[19]</a> <a href="https://arxiv.org/pdf/1702.04595.pdf" target="_blank" rel="noopener">[12]</a> <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Fong_Interpretable_Explanations_of_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[13]</a></p>
</li>
<li><p>high-order attention <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Mixed_High-Order_Attention_Network_for_Person_Re-Identification_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[15]</a></p>
</li>
</ul>
<h2 id="Attention-in-RNN"><a href="#Attention-in-RNN" class="headerlink" title="Attention in RNN:"></a>Attention in RNN:</h2><p>survey paper: survey on the attention based RNN model and its applications in computer vision [<a href="https://arxiv.org/pdf/1601.06823.pdf" target="_blank" rel="noopener">1</a>]</p>
<ul>
<li><p>soft/hard attention: binary weight or soft weight </p>
</li>
<li><p>item-wise/location-wise attention: location-wise attention is to convert an image to a sequence of local regions, which is essentially item-wise.</p>
</li>
</ul>
<p>Earliest papers [<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">2</a>] [<a href="https://arxiv.org/pdf/1412.7449.pdf" target="_blank" rel="noopener">3</a>] are basically the same except design specs of RNN unit.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Wang, Feng, and David MJ Tax. “Survey on the attention based RNN model and its applications in computer vision.” arXiv preprint arXiv:1601.06823 (2016).</p>
<p>[2] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. “Neural machine translation by jointly learning to align and translate.” arXiv preprint arXiv:1409.0473 (2014).</p>
<p>[3] Vinyals, Oriol, et al. “Grammar as a foreign language.” NIPS, 2015.</p>
<p>[4] Drew Linsley, Dan Shiebler, Sven Eberhardt, Thomas Serre: Learning what and where to attend. ICLR, 2019.</p>
<p>[5] Saumya Jetley, Nicholas A. Lord, Namhoon Lee, Philip H. S. Torr: Learn to Pay Attention. ICLR, 2018.</p>
<p>[6] Bolei Zhou, Aditya Khosla, Àgata Lapedriza, Aude Oliva, Antonio Torralba: Learning Deep Features for Discriminative Localization. CVPR 2016.</p>
<p>[7] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra:<br>Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. ICCV 2017.</p>
<p>[8] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei:<br>Deformable Convolutional Networks. ICCV 2017.</p>
<p>[9] Xizhou Zhu, Han Hu, Stephen Lin, Jifeng Dai: Deformable ConvNets v2: More Deformable, Better Results. CoRR abs/1811.11168 (2018)</p>
<p>[10] Wei Li, Xiatian Zhu, Shaogang Gong: Harmonious Attention Network for Person Re-Identification. CVPR 2018.</p>
<p>[11] Cheng Wang, Qian Zhang, Chang Huang, Wenyu Liu, Xinggang Wang: Mancs: A Multi-task Attentional Network with Curriculum Sampling for Person Re-Identification. ECCV (4) 2018.</p>
<p>[12] Zintgraf, Luisa M., et al. “Visualizing deep neural network decisions: Prediction difference analysis.” arXiv preprint arXiv:1702.04595 (2017).</p>
<p>[13] Fong, Ruth C., and Andrea Vedaldi. “Interpretable explanations of black boxes by meaningful perturbation.” ICCV, 2017.</p>
<p>[14] Wang, Haofan, et al. “Score-CAM: Improved Visual Explanations Via Score-Weighted Class Activation Mapping.” arXiv preprint arXiv:1910.01279 (2019).</p>
<p>[15] Chen, Binghui, Weihong Deng, and Jiani Hu. “Mixed high-order attention network for person re-identification.” Proceedings of the IEEE International Conference on Computer Vision. 2019.</p>
<p>[16] Zhu, Xizhou, et al. “An empirical study of spatial attention mechanisms in deep networks.” ICCV, 2019.</p>
<p>[17] Wang, Qilong, et al. “ECA-net: Efficient channel attention for deep convolutional neural networks.” CVPR, 2020.</p>
<p>[18] Qin, Zequn, et al. “FcaNet: Frequency Channel Attention Networks.” arXiv preprint arXiv:2012.11879 (2020).</p>
<p>[19] Zhang, Xiaolin, et al. “Adversarial complementary learning for weakly supervised object localization.” CVPR, 2018.</p>
<p>[20] Jo, Sanhyun, and In-Jae Yu. “Puzzle-CAM: Improved localization via matching partial and full features.” arXiv preprint arXiv:2101.11253 (2021).</p>
<p>[21] Araslanov, Nikita, and Stefan Roth. “Single-stage semantic segmentation from image labels.” CVPR, 2020.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Aesthetic Evaluation and Cropping</title>
    <url>/2022/06/16/deep_learning/Aesthetic%20Evaluation%20and%20Cropping/</url>
    <content><![CDATA[<h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><ul>
<li><a href="https://github.com/bcmi/Awesome-Aesthetic-Evaluation-and-Cropping" target="_blank" rel="noopener">Awesome-Aesthetic-Evaluation-and-Cropping</a></li>
</ul>
<p>Focus on certain aspects:</p>
<ul>
<li>image composition: <a href="https://github.com/bcmi/Image-Composition-Assessment-Dataset-CADB" target="_blank" rel="noopener">CADB</a></li>
<li>color: <a href="https://github.com/woshidandan/Image-Color-Aesthetics-Assessment" target="_blank" rel="noopener">ICAA17K</a></li>
</ul>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>aesthetic cropping</tag>
      </tags>
  </entry>
  <entry>
    <title>Adversarial Attack</title>
    <url>/2022/06/16/deep_learning/Adversarial%20Attack/</url>
    <content><![CDATA[<p>A comprehensive survey can be found <a href="https://arxiv.org/pdf/1801.00553.pdf" target="_blank" rel="noopener">here</a>.</p>
<h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology:"></a>Terminology:</h2><ul>
<li><p>black-box/white-box attack: the adversarial example is generated with or without knowing the prior knowledge of the target model.</p>
</li>
<li><p>targeted/non-targeted attack: whether predicting a specific label for the adversarial example.</p>
</li>
<li><p>universal perturbation: fool a given model on any image with high probability. </p>
</li>
</ul>
<h2 id="Attack"><a href="#Attack" class="headerlink" title="Attack"></a>Attack</h2><ol>
<li><p>Backward Update</p>
<ul>
<li><p>add imperceptible distortion and increase the classification loss</p>
</li>
<li><p>universal adversarial perturbation: learn a residual perturbation that works on most clean images</p>
</li>
</ul>
</li>
<li><p>Forward Update</p>
<ul>
<li><p><a href="https://arxiv.org/pdf/1710.08864.pdf" target="_blank" rel="noopener">one-pixel attack</a>: use differential evolution algorithm</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1703.09387.pdf" target="_blank" rel="noopener">Adversarial Transformation Networks</a>: learn a network to translate clean image to adversarial example.</p>
</li>
</ul>
</li>
</ol>
<h2 id="Defense"><a href="#Defense" class="headerlink" title="Defense"></a>Defense</h2><ol>
<li><p>Use modified training samples during training or modified test samples during testing</p>
</li>
<li><p>Modify network: model parameters regularization, add a layer/module</p>
</li>
<li><p>Adversarial example detector: classify an example as adversarial or clean based on certain statistics</p>
</li>
</ol>
<h2 id="New-perspective"><a href="#New-perspective" class="headerlink" title="New perspective"></a>New perspective</h2><p><a href="https://arxiv.org/pdf/1905.02175" target="_blank" rel="noopener">Adversarial examples are not bugs, they are features.</a></p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>adversarial attack</tag>
      </tags>
  </entry>
  <entry>
    <title>3D Photography</title>
    <url>/2022/06/16/deep_learning/3D%20Photography/</url>
    <content><![CDATA[<ol>
<li><p>Layered Depth Image (LDI): <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shih_3D_Photography_Using_Context-Aware_Layered_Depth_Inpainting_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[1]</a></p>
</li>
<li><p>MultiPlane Image (MPI): <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Tucker_Single-View_View_Synthesis_With_Multiplane_Images_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[2]</a> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Li_MINE_Towards_Continuous_Depth_MPI_With_NeRF_for_Novel_View_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[3]</a> </p>
</li>
<li><p>Point cloud: <a href="https://arxiv.org/pdf/1909.05483.pdf" target="_blank" rel="noopener">[4]</a></p>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Shih, Meng-Li, et al. “3d photography using context-aware layered depth inpainting.” CVPR, 2020.</p>
<p>[2] Tucker, Richard, and Noah Snavely. “Single-view view synthesis with multiplane images.” CVPR, 2020.</p>
<p>[3] Li, Jiaxin, et al. “Mine: Towards continuous depth mpi with nerf for novel view synthesis.” ICCV, 2021.</p>
<p>[4] Niklaus, Simon, et al. “3d ken burns effect from a single image.” ACM Transactions on Graphics (TOG) 38.6 (2019): 1-15.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>3D</tag>
      </tags>
  </entry>
  <entry>
    <title>Makefile</title>
    <url>/2022/06/16/compiler/Makefile/</url>
    <content><![CDATA[<hr>
<h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><ol>
<li><p>use nemiver to debug. </p>
<ul>
<li>gcc/g++ <strong>-g</strong> hello.c -o hello.o <font color="green">#-g for debug</font></li>
<li>nemiver hello <font color="green">#bin file</font></li>
</ul>
</li>
<li><p>The comment character # does not introduce a make comment in the text of commands.</p>
</li>
<li><p><strong>Wildcards</strong>: <em>.</em> expands to all the files containing a period. A question mark represents any single character, and […] represents a character class.</p>
</li>
<li><p>.PHONY: clean</p>
</li>
<li><p><strong>Automatic Variables</strong>:</p>
<ul>
<li>$@ The name of the current target.</li>
<li>$% The filename element of an archive member specification.</li>
<li>$&lt; The name of the first prerequisite.</li>
<li>$? The names of all prerequisites that are newer than the target, separated by spaces.</li>
<li>$^ The names of all the prerequisites, separated by spaces. This list has duplicate names removed since for most uses, such as compiling, copying, etc., duplicates are not wanted.</li>
<li>$+ The names of all the prerequisites separated by spaces, including duplicates. This variable was created for specific situations such as arguments to linkers where duplicate values have meaning.</li>
<li>$* The stem of the target filename. A stem is typically a filename without its suffix. Its use outside of pattern rules is discouraged.</li>
</ul>
</li>
<li><p>run makefile with —just-print option to view the execution process</p>
</li>
</ol>
<h2 id="How-to-write-Makefile"><a href="#How-to-write-Makefile" class="headerlink" title="How to write Makefile"></a>How to write Makefile</h2><ol>
<li><p>single C-file</p>
 <figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">hello: hello.c</span></span><br><span class="line">	gcc -g hello.c -o hello&lt;/code&gt;&lt;/pre&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>multiple C-files</p>
 <figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">count_words: count_words.o lexer.o -lfl</span></span><br><span class="line">	gcc count_words.o lexer.o -lfl -ocount_words</span><br><span class="line"><span class="section">count_words.o: count_words.c</span></span><br><span class="line">	gcc -g -c count_words.c</span><br><span class="line"><span class="section">lexer.o: lexer.c</span></span><br><span class="line">	gcc -g -c lexer.c</span><br><span class="line"><span class="section">lexer.c: lexer.l</span></span><br><span class="line">	flex -t lexer.l &gt; lexer.c</span><br></pre></td></tr></table></figure>
</li>
<li><p>set <strong>VPATH</strong> and <strong>CPPFLAGS</strong> in implicit rules</p>
 <figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">VPATH    = src <span class="keyword">include</span></span><br><span class="line">CPPFLAGS = -I <span class="keyword">include</span></span><br><span class="line"></span><br><span class="line"><span class="section">count_words: counter.o lexer.o -lfl</span></span><br><span class="line"><span class="section">count_words.o: counter.h</span></span><br><span class="line"><span class="section">counter.o: counter.h lexer.h</span></span><br><span class="line"><span class="section">lexer.o: lexer.h</span></span><br></pre></td></tr></table></figure>
<p>VPATH can be used in a more advanced fashion as follows,</p>
 <figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">vpath</span> %.c src</span><br><span class="line"><span class="keyword">vpath</span> %.l src</span><br><span class="line"><span class="keyword">vpath</span> %.h <span class="keyword">include</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Use library <strong>.a</strong>. pack <strong>.o</strong> files into <strong>.a</strong>, similar as <strong>.lib</strong> in Windows.</p>
 <figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">libcounter.a: libcounter.a(lexer.o) libcounter.a(counter.o)</span></span><br><span class="line"><span class="section">libcounter.a(lexer.o): lexer.o</span></span><br><span class="line">	<span class="variable">$(AR)</span> <span class="variable">$(ARFLAGS)</span> <span class="variable">$@</span> <span class="variable">$&lt;</span></span><br><span class="line"><span class="section">libcounter.a(counter.o): counter.o</span></span><br><span class="line">	<span class="variable">$(AR)</span> <span class="variable">$(ARFLAGS)</span> <span class="variable">$@</span> <span class="variable">$&lt;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>compiler</category>
      </categories>
      <tags>
        <tag>Makefile</tag>
      </tags>
  </entry>
  <entry>
    <title>CMake</title>
    <url>/2022/06/16/compiler/CMake/</url>
    <content><![CDATA[<ul>
<li>in-source make</li>
<li>out-of-source make</li>
</ul>
<h3 id="Write-Basic-CMakeLists-txt"><a href="#Write-Basic-CMakeLists-txt" class="headerlink" title="Write Basic CMakeLists.txt"></a>Write Basic CMakeLists.txt</h3><ul>
<li><p>common head</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">PROJECT</span>(projectname [CXX] [C] [Java]) <span class="comment"># project name</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">2.8</span>.<span class="number">12</span>) <span class="comment"># minimum cmake version</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>variable assignment</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span>(VAR [VALUE] [CACHE TYPE DOCSTRING [FORCE]])</span><br></pre></td></tr></table></figure>
<p>variable should be used by  ${VAR} except for IF condition.<br>commonly used path variables are listed as follows:</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">&lt;projectname&gt;_BINARY_DIR = PROJECT_BINARY_DIR=CMAKE_BINARY_DIR</span><br><span class="line">&lt;projectname&gt;_SOURCE_DIR = PROJECT__SOURCE_DIR=CMAKE_SOURCE_DIR</span><br><span class="line"><span class="keyword">SET</span>(EXECUTABLE_OUTPUT_PATH <span class="variable">$&#123;PROJECT_BINARY_DIR&#125;</span>/bin)</span><br><span class="line"><span class="keyword">SET</span>(LIBRARY_OUTPUT_PATH <span class="variable">$&#123;PROJECT_BINARY_DIR&#125;</span>/lib)</span><br></pre></td></tr></table></figure>
</li>
<li><p>display message</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">MESSAGE</span>([SEND_ERROR | STATUS | FATAL_ERROR] <span class="string">"message to display"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>generate output binary/library</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ADD_EXECUTABLE</span>(hello <span class="variable">$&#123;SRC_LIST&#125;</span>)</span><br><span class="line"><span class="keyword">ADD_LIBRARY</span>(libname [SHARED|STATIC|MODULE] [EXCLUDE_FROM_ALL] source1 source2 ... sourceN)</span><br><span class="line"><span class="keyword">TARGET_LINK_LIBRARIES</span>(target library1 &lt;debug | optimized&gt; library2 ...) /<span class="comment">#target-specific</span></span><br><span class="line"></span><br><span class="line">// change the name, version, <span class="keyword">and</span> so <span class="keyword">on</span> of the output (*e.g.*, library, binary)</span><br><span class="line"><span class="keyword">SET_TARGET_PROPERTIES</span>(target1 target2 ...</span><br><span class="line">	PROPERTIES prop1 value1</span><br><span class="line">	prop2 value2 ...)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Finding"><a href="#Finding" class="headerlink" title="Finding"></a>Finding</h3><ul>
<li><p>search source</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INCLUDE_DIRECTORIES</span>([AFTER|BEFORE] [SYSTEM] dir1 dir2 ...) <span class="comment">#include files</span></span><br><span class="line"><span class="keyword">LINK_DIRECTORIES</span>(directory1 directory2 ...)</span><br></pre></td></tr></table></figure>
<p><code>CMAKE_INCLUDE_PATH</code>, <code>CMAKE_LIBRARY_PATH</code>, <code>CMAKE_MODULE_PATH</code>  are environment variables instead of CMake variables. when using <code>FIND_***</code>, <code>CMAKE_INCLUDE_PATH</code>, the above paths will be searched.</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FIND_PATH</span>(myHeader hello.h)</span><br><span class="line"><span class="keyword">IF</span>(myHeader)</span><br><span class="line"><span class="keyword">INCLUDE_DIRECTORIES</span>(<span class="variable">$&#123;myHeader&#125;</span>)</span><br><span class="line"><span class="keyword">ENDIF</span>(myHeader)</span><br></pre></td></tr></table></figure>
</li>
<li><p>search commands</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FIND_FILE</span>(&lt;VAR&gt; name1 path1 path2 ...)</span><br><span class="line"><span class="keyword">FIND_LIBRARY</span>(&lt;VAR&gt; name1 path1 path2 ...)</span><br><span class="line"><span class="keyword">FIND_PATH</span>(&lt;VAR&gt; name1 path1 path2 ...)</span><br><span class="line"><span class="keyword">FIND_PROGRAM</span>(&lt;VAR&gt; name1 path1 path2 ...)</span><br><span class="line"><span class="keyword">FIND_PACKAGE</span>(&lt;name&gt; [major.minor] [QUIET] [NO_MODULE] [[REQUIRED|COMPONENTS] [componets...]]) <span class="comment"># find \*.cmake</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Compactness"><a href="#Compactness" class="headerlink" title="Compactness"></a>Compactness</h3><ul>
<li><p>include the content of other files</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">include</span>(FILE) <span class="comment"># load the content of FILE</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>hierarchical binary tree</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ADD_SUBDIRECTORY</span>(source_dir [binary_dir][EXCLUDE_FROM_ALL])</span><br></pre></td></tr></table></figure>
</li>
<li><p>macro definition</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">MACRO</span>(add_example name)</span><br><span class="line">	<span class="keyword">ADD_EXECUTABLE</span>(<span class="variable">$&#123;name&#125;</span> <span class="variable">$&#123;name&#125;</span>.cpp)</span><br><span class="line">	<span class="keyword">TARGET_LINK_LIBRARIES</span>(<span class="variable">$&#123;name&#125;</span> dlib::dlib )</span><br><span class="line"><span class="keyword">ENDMACRO</span>()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="CMake-Module"><a href="#CMake-Module" class="headerlink" title="CMake Module"></a>CMake Module</h3><ul>
<li>define FindHELLO.cmake module  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FIND_PATH</span>(HELLO_INCLUDE_DIR hello.h /usr/<span class="keyword">include</span>/hello /usr/local/<span class="keyword">include</span>/hello)</span><br><span class="line"><span class="keyword">FIND_LIBRARY</span>(HELLO_LIBRARY NAMES hello PATH /usr/lib/usr/local/lib)</span><br><span class="line"><span class="keyword">IF</span> (HELLO_INCLUDE_DIR <span class="keyword">AND</span> HELLO_LIBRARY)</span><br><span class="line"><span class="keyword">SET</span>(HELLO_FOUND <span class="keyword">TRUE</span>)</span><br><span class="line"><span class="keyword">ENDIF</span> (HELLO_INCLUDE_DIR <span class="keyword">AND</span> HELLO_LIBRARY)</span><br><span class="line"><span class="keyword">IF</span> (HELLO_FOUND)</span><br><span class="line"><span class="keyword">IF</span> (NOT HELLO_FIND_QUIETLY)</span><br><span class="line"><span class="keyword">MESSAGE</span>(STATUS <span class="string">"Found Hello: $&#123;HELLO_LIBRARY&#125;"</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h3><p><code>cmake -DCMAKE_INSTALL_PREFIX=/usr  # the default install target is /usr/local</code></p>
<ul>
<li><p>install library and binary</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSTALL</span>(TARGETS targets...</span><br><span class="line">	[[ARCHIVE|LIBRARY|RUNTIME]</span><br><span class="line">		[DESTINATION &lt;dir&gt;]</span><br><span class="line">		[PERMISSIONS permissions...]</span><br><span class="line">		[CONFIGURATIONS</span><br><span class="line">	[Debug|Release|...]]</span><br><span class="line">		[COMPONENT &lt;component&gt;]</span><br><span class="line">		[OPTIONAL]</span><br><span class="line">		] [...])</span><br></pre></td></tr></table></figure>
<p>ARCHIVE is static library *.a; LIBRARY is dynamic library *.so; RUNTIME is executable binary</p>
</li>
<li><p>install regular file</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSTALL</span>(FILES files... DESTINATION &lt;dir&gt;</span><br><span class="line">	[PERMISSIONS permissions...]</span><br><span class="line">	[CONFIGURATIONS [Debug|Release|...]]</span><br><span class="line">	[COMPONENT &lt;component&gt;]</span><br><span class="line">	[RENAME &lt;name&gt;] [OPTIONAL])</span><br></pre></td></tr></table></figure>
</li>
<li><p>install script file (<em>e.g.</em>, *.sh), almost the same with installing files except for permission</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSTALL</span>(PROGRAMS files... DESTINATION &lt;dir&gt;</span><br><span class="line">	[PERMISSIONS permissions...]</span><br><span class="line">	[CONFIGURATIONS [Debug|Release|...]]</span><br><span class="line">	[COMPONENT &lt;component&gt;]</span><br><span class="line">	[RENAME &lt;name&gt;] [OPTIONAL])</span><br></pre></td></tr></table></figure>
</li>
<li><p>install folders</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSTALL</span>(DIRECTORY dirs... DESTINATION &lt;dir&gt;</span><br><span class="line">	[FILE_PERMISSIONS permissions...]</span><br><span class="line">	[DIRECTORY_PERMISSIONS permissions...]</span><br><span class="line">	[USE_SOURCE_PERMISSIONS]</span><br><span class="line">	[CONFIGURATIONS [Debug|Release|...]]</span><br><span class="line">	[COMPONENT &lt;component&gt;]</span><br><span class="line">	[[PATTERN &lt;pattern&gt; | REGEX &lt;regex&gt;]</span><br><span class="line">	[EXCLUDE] [PERMISSIONS permissions...]] [...])</span><br></pre></td></tr></table></figure>
</li>
<li><p>install *.cmake</p>
  <figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSTALL</span>([[SCRIPT &lt;file&gt;] [CODE &lt;code&gt;]] [...])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h3><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ADD_TEST</span>(mytest <span class="variable">$&#123;PROJECT_BINARY_DIR&#125;</span>/bin/main)</span><br><span class="line"><span class="keyword">ENABLE_TESTING</span>()</span><br></pre></td></tr></table></figure>
<p>After generating <code>Makefile</code>, run <code>make test</code></p>
]]></content>
      <categories>
        <category>compiler</category>
      </categories>
      <tags>
        <tag>CMake</tag>
      </tags>
  </entry>
  <entry>
    <title>CMake with Eclipse</title>
    <url>/2022/06/16/compiler/CMake%20with%20Eclipse/</url>
    <content><![CDATA[<p>Cmake supports CDT4 and higher versions.<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cmake -help  # check the supported generator</span><br></pre></td></tr></table></figure></p>
<ol>
<li><p>Install CDT to Eclipse: <a href="http://www.eclipse.org/cdt/downloads.php" target="_blank" rel="noopener">http://www.eclipse.org/cdt/downloads.php</a></p>
</li>
<li><p>The eclipse build directory should be sibling directory of the source directory.</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir eclipse</span><br><span class="line">cd eclipse</span><br><span class="line">cmake -G "Eclipse CDT4 - Unix Makefiles" -D     CMAKE_BUILD_TYPE=Debug ../src_folder # note that CMAKE_BUILD_TYPE can be set as Debug or Release</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><code>cmake --build . --config Release</code> is equivalent to <code>make</code></p>
<p><code>cmake --build . --target install --config Release</code> is equivalent to <code>make install</code></p>
]]></content>
      <categories>
        <category>compiler</category>
      </categories>
      <tags>
        <tag>CMake</tag>
        <tag>Eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title>White Balance</title>
    <url>/2022/06/16/color/white_balance/</url>
    <content><![CDATA[<p><strong>White balance</strong>: White balance is the process of removing unrealistic color casts, so that objects which appear white in person eyes are shown white in the image and this is more relevant to the settings of digital cameras (auto white balance). Since human eyes are very good at judging what is white under different light sources, if an<br>white object is captured in a wrong or mismatched color temperature, the realism could be significantly degraded.</p>
<p>datasets: <a href="https://cvil.eecs.yorku.ca/projects/public_html/sRGB_WB_correction/dataset.html" target="_blank" rel="noopener">https://cvil.eecs.yorku.ca/projects/public_html/sRGB_WB_correction/dataset.html</a></p>
<p><strong>Color constancy</strong>: Color constancy is the ability to perceive color of objects, invariant to the color of the light source and it’s quite related to the human visual system. Existing computational color constancy methods<br>address this problem by first estimating the color of the light source and then correcting the input images pixel<br>to pixel to make it as taken under a white light source. </p>
<p>datasets: <a href="http://colorconstancy.com/evaluation/datasets/index.html" target="_blank" rel="noopener">http://colorconstancy.com/evaluation/datasets/index.html</a></p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
</search>
